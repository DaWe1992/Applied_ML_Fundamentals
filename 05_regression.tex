\input{preamble_theme}

% ====================================================
% ====================================================
% PRESENTATION DATA
% ====================================================
% ====================================================

\title[Machine Learning Introduction]{*** Applied Machine Learning Fundamentals *** Regression}
\institute{SAP\,SE}
\author{Daniel Wehner}
\date{\today}
\prefix{REG}

% ====================================================
% ====================================================
% BEGIN OF DOCUMENT
% ====================================================
% ====================================================

\begin{document}

% Title frame
%______________________________________________________________________
\maketitlepage


% Agenda
%______________________________________________________________________
\begin{frame}{Agenda \today}
	\begin{multicols}{2}
		\tableofcontents
	\end{multicols}
\end{frame}


% Section: Introduction to Regression
%______________________________________________________________________
\section{Introduction to Regression}
\makedivider{Introduction to Regression}

% Subsection: What is Regression?
% --------------------------------------------------------------------------------------------------------
\subsection{What is Regression?}

% Regression
\begin{frame}{Regression}{}
	\vspace*{-6mm}
	\begin{tabbing}
		\hspace*{7cm}\= \kill
		\highlight{Type of target variable}			\> Continuous 		\\[1mm]
		\highlight{Type of training information} 	\> Supervised 		\\[1mm]
		\highlight{Example Availability}			\> Batch learning		\\[1mm]
	\end{tabbing}
	
	\vspace*{-7mm}
	\footnotesize
	\begin{boxBlue}
		\textbf{Algorithm sketch:} Given the training data $\mathcal{D}$ the algorithm derives
		a function of the type
	
		\vspace*{-6mm}
		\begin{equation}
			h_{\bm{\theta}}(\bm{x}) = \theta_0 + \theta_1 x_1 + \dots + \theta_{m+1} x_{m}
				\qquad \bm{x} \in \mathbb{R}^{m}, \bm{\theta} \in \mathbb{R}^{m+1}
		\end{equation}

		from the data. $\bm{\theta}$ is the parameter vector containing the coefficients to be estimated by the regression
 		algorithm. Once $\bm{\theta}$ is learned it can be used for prediction. 
	\end{boxBlue}
\end{frame}


% Example Data Set: Revenues
\begin{frame}{Example Data Set: Revenues}{}
	\divideTwo{0.44}{
		\input{05_regression/01_tikz/data_regression_marketing_expenses}
	}{0.55}{
		\begin{itemize}
			\item Find a linear function:
			\begin{equation*}
				h_{\bm{\theta}}(\bm{x}) = \theta_0 + \theta_1 x_1 + \dots + \theta_{m+1} x_{m}
			\end{equation*}
			\item Usually: $x_0 = 1$:
		\end{itemize}
		\begin{align*}
			\bm{\widehat{x}} \in \mathbb{R}^{m+1}
				&= [1\ \bm{x}]^{\intercal} \\
			h_{\bm{\theta}}(\bm{\widehat{x}}) = \sum_{j=0}^{m+1} \theta_j x_j
				&= \bm{\theta}^{\intercal} \bm{\widehat{x}}
		\end{align*}
	}
\end{frame}


% Subsection: Least Squares Error Function
% --------------------------------------------------------------------------------------------------------
\subsection{Least Squares Error Function}

% Error Function for Regression
\begin{frame}{Error Function for Regression}{}
	\begin{itemize}
		\item In order to know how good the function fits we need an error function $\mathcal{J}(\bm{\theta})$:
		\begin{equation}
			\mathcal{J}(\bm{\theta}) = \frac{1}{2n} \sum_{i=1}^n (h_{\bm{\theta}}(\bm{\widehat{x}}^{(i)}) - y^{(i)})^2
		\end{equation}
		\item We want to minimize $\mathcal{J}(\bm{\theta})$:
		\begin{equation*}
			\min_{\bm{\theta}} \frac{1}{2n} \sum_{i=1}^n (h_{\bm{\theta}}(\bm{\widehat{x}}^{(i)}) - y^{(i)})^2
		\end{equation*}
		\item This is \highlight{ordinary least squares (OLS)}
	\end{itemize}
\end{frame}


% Error Function Intuition
\begin{frame}{Error Function Intuition}{}
	% tikz image showing least squares
\end{frame}


% Section: Solutions to Regression
%______________________________________________________________________
\section{Solutions to Regression}
\makedivider{Solutions to Regression}

% Subsection: Closed-Form Solutions and Normal Equation
% --------------------------------------------------------------------------------------------------------
\subsection{Closed-Form Solutions and Normal Equation}

% Closed-Form Solutions
\begin{frame}{Closed-Form Solutions}{}
	\bubble{0.5}{5.5}{
		\scriptsize sample mean $\overline x$
	}
	\begin{itemize}
		\item Usual approach (for two unknowns): Calculate $\theta_0$ and $\theta_1$ according to
		\begin{equation}
			\theta_0 = \overline{y} - \theta_2 \overline{x} \qquad\qquad
			\theta_1 = \frac{\sum_{i=1}^n (x^{(i)} - \overline{x}) \cdot
				(y^{(i)} - \overline{y})}{\sum_{i=1}^n (x^{(i)} - \overline{x})^2}
		\end{equation}
		\item \highlight{`Normal equation'} (scales to arbitrary dimensions):
		\begin{equation}
			\bm{\theta} = \underbracket{
				(\bm{\widehat{X}}^{\intercal} \bm{\widehat{X}})^{-1} \bm{\widehat{X}}^{\intercal}
			}_{\substack{\text{Moore-Penrose} \\ \text{pseudo-inverse}}} \bm{y}
		\end{equation}
		$\bm{\widehat{X}}$ is called `design matrix' or `regressor matrix'
	\end{itemize}
\end{frame}


% Design Matrix / Regressor Matrix
\begin{frame}{Design Matrix / Regressor Matrix}{}
	\bubble{12}{4.5}{
		\scriptsize In the following \\
		\scriptsize $\bm{\widehat{X}} \equiv \bm{X}$
	}
	\begin{itemize}
		\item The design matrix $\bm{\widehat{X}} \in \mathbb{R}^{n \times (m + 1)}$ looks as follows:
		\footnotesize
		\begin{equation}
			\bm{\widehat{X}} =
			\begin{pmatrix}
  				1 & x_1^{(1)} & x_2^{(1)} & \cdots & x_m^{(1)} \\
				1 & x_1^{(2)} & x_2^{(2)} & \cdots & x_m^{(2)} \\
				1 & x_1^{(3)} & x_2^{(3)} & \cdots & x_m^{(3)} \\
				\vdots & \vdots & \vdots & \ddots & \vdots \\
				1 & x_1^{(n)} & x_2^{(n)} & \cdots & x_m^{(n)} \\
 			\end{pmatrix}
		\end{equation}
		\normalsize
		\item And the $n \times 1$ label vector:
		\footnotesize
		\begin{equation*}
			 \bm{y} =
			 \begin{pmatrix}
				y^{(1)}, y^{(2)}, y^{(3)}, \dots, y^{(n)}
			\end{pmatrix}^{\intercal}
		\end{equation*}
	\end{itemize}
\end{frame}


% Derivation of the Normal Equation
\begin{frame}{Derivation of the Normal Equation}{}\optional
	\begin{itemize}
		\item The derivation involves a bit of linear algebra
		\item Step \ding{182}: Rewrite $\mathcal{J}(\bm{\theta})$ in matrix-vector notation:
		\begin{align*}
			\mathcal{J}(\bm{\theta})
				&= \frac{1}{2}(\bm{X} \bm{\theta} - \bm{y})^{\intercal} (\bm{X} \bm{\theta} - \bm{y}) \\
				&= ((\bm{X} \bm{\theta})^{\intercal} - \bm{y}^{\intercal}) (\bm{X} \bm{\theta} - \bm{y}) \\
				&= (\bm{X} \bm{\theta})^{\intercal} \bm{X} \bm{\theta} - (\bm{X} \bm{\theta})^{\intercal} \bm{y}
					- \bm{y}^{\intercal} (\bm{X} \bm{\theta}) + \bm{y}^{\intercal} \bm{y} \\
				&= \bm{\theta}^{\intercal} \bm{X}^{\intercal} \bm{X} \bm{\theta}
					- 2 (\bm{X} \bm{\theta})^{\intercal} \bm{y} + \bm{y}^{\intercal} \bm{y}
		\end{align*}
		\item To be continued...
	\end{itemize}
\end{frame}


% Derivation of the Normal Equation (Ctd.)
\begin{frame}{Derivation of the Normal Equation (Ctd.)}{}\optional
	\begin{itemize}
		\item Step \ding{183}: Calculate the derivative of $\mathcal{J}(\bm{\theta})$ and set it to zero:
		\begin{align*}
			\nabla_{\bm{\theta}}\mathcal{J}(\bm{\theta})
				&= 2 \bm{X}^{\intercal} \bm{X} \bm{\theta} - 2 \bm{X}^{\intercal} \bm{y} \overset{!}{=} \bm{0} \\	
				&\Leftrightarrow \bm{X}^{\intercal} \bm{X} \bm{\theta} = \bm{X}^{\intercal} \bm{y}
		\end{align*}
		\item If $\bm{X}^{\intercal} \bm{X}$ is invertible, we can multiply both sides by
			$(\bm{X}^{\intercal} \bm{X})^{-1}$:
		\vspace*{3mm}
		\begin{boxBlueNoFrame}
			\highlight{Normal equation:}
			\begin{equation*}
				\bm{\theta} = (\bm{X}^{\intercal} \bm{X})^{-1} \bm{X}^{\intercal} \bm{y}
			\end{equation*}
		\end{boxBlueNoFrame}
	\end{itemize}
\end{frame}


% Problems with Matrix Inversion?
\begin{frame}{Problems with Matrix Inversion?}{}
	\begin{itemize}
		\item What if $(\bm{X}^{\intercal} \bm{X})^{-1}$ does not exist?
		\item Problems and solutions:
		\begin{enumerate}
			\item Linearly dependent (redundant) features or design matrix does not have full rank? 
				(E.\,g. size in m$^2$ and size in feet$^2$) \\
				\highlight{$\Rightarrow$ Delete correlated features}
			\item Too many features ($m > n$)? \\
				\highlight{$\Rightarrow$ Delete features (e.\,g. using PCA) / add training examples}
			\item Other numerical instabilities? \\
				\highlight{$\Rightarrow$ Add a regularization term} (later)
			\item Computationally too expensive? \\
				\highlight{$\Rightarrow$ Use gradient descent}
		\end{enumerate}
	\end{itemize}
\end{frame}


% Subsection: Gradient Descent
% --------------------------------------------------------------------------------------------------------
\subsection{Gradient Descent}

% Gradient Descent
\begin{frame}{Gradient Descent}{}\important
	\begin{itemize}
		\item We want to minimize a smooth function $\mathcal{J} : \mathbb{R}^{m+1} \rightarrow \mathbb{R}$:
		\begin{equation*}
			\min_{\bm{\theta} \in \mathbb{R}^{m+1}} \mathcal{J}(\bm{\theta})
		\end{equation*}
		\item Update the parameters iteratively:
		\begin{equation}
			\bm{\theta}^{(t+1)} \longleftarrow \bm{\theta}^{(t)} -
				\alpha \nabla_{\bm{\theta}} \mathcal{J}(\bm{\theta}^{(t)})
		\end{equation}
		\item where $\alpha > 0$ (\highlight{learning rate}) and $\nabla_{\bm{\theta}} \mathcal{J}(\bm{\theta})$
			is the gradient of $\mathcal{J}(\bm{\theta})$ w.\,r.\,t. $\bm{\theta}$:
		\begin{equation*}
			\nabla_{\bm{\theta}} \mathcal{J}(\bm{\theta}) = 
			\begin{pmatrix}
				\frac{\partial \mathcal{J}(\bm{\theta})}{\partial \theta_0},
				\frac{\partial \mathcal{J}(\bm{\theta})}{\partial \theta_1}, 
				\dots,
				\frac{\partial \mathcal{J}(\bm{\theta})}{\partial \theta_{m+1}}
			\end{pmatrix}^{\intercal}
		\end{equation*}
	\end{itemize}
\end{frame}


% Data Input Space vs. Hypothesis Space
\begin{frame}{Data Input Space vs. Hypothesis Space}{}
	\divideTwoTop{0.49}{
		\highlight{Data input space}
		\input{05_regression/01_tikz/data_input_space}
	}{0.49}{
		\highlight{Hypothesis space $\mathcal{H}$}
		\input{05_regression/01_tikz/hypothesis_space}
	}
\end{frame}


% Data Input Space vs. Hypothesis Space (Ctd.)
\begin{frame}{Data Input Space vs. Hypothesis Space (Ctd.)}{}
	\begin{itemize}
		\item \textbf{Data input space}
		\begin{itemize}
			\item Determined by the $m$ \textbf{attributes} of the data set $x_1, x_2, \dots, x_m$
			\item Often high-dimensional
		\end{itemize}
		\item \textbf{Hypothesis space $\mathcal{H}$}
		\begin{itemize}
			\item Determined by the \textbf{number of parameters} of the model
			\item Each point in the hypothesis space corresponds to a \textbf{specific assignment of model parameters}
			\item The error function gives information about how good this assignment is
			\item \highlight{Gradient descent is applied in the hypothesis space $\mathcal{H}$}
		\end{itemize}
	\end{itemize}
\end{frame}


% Visualization of Gradient Descent in 3 Dimensions
\begin{frame}{Visualization of Gradient Descent in 3 Dimensions}{}
	\input{05_regression/01_tikz/hypothesis_space_gradient_descent}
\end{frame}


% Versions of Gradient Descent
\begin{frame}{Versions of Gradient Descent}{}
	\begin{itemize}
		\item Assume some training data $\mathcal{D}$: $\{ \bm{x}^{(i)}, y^{(i)} \}_{i=1}^{n}$
		\item Squared error for a \textbf{single} example: $\ell(y_{pred}, y_{true}) = (y_{pred} - y_{true})^2$
		\item Our objective is to minimize the \textbf{total} error:
		\begin{equation*}
			\min_{\bm{\theta} \in \mathbb{R}^{m+1}} \mathcal{J}(\bm{\theta}) =
				\min_{\bm{\theta} \in \mathbb{R}^{m+1}} \sum_{i=1}^n \ell(h_{\bm{\theta}}(\bm{x}^{(i)}), y^{(i)})
		\end{equation*}
		\item Three versions of gradient descent:
		\begin{enumerate}
			\item Batch gradient descent
			\item Stochastic gradient descent
			\item Mini-batch gradient descent
		\end{enumerate}
	\end{itemize}
\end{frame}


% Versions of Gradient Descent (Ctd.)
\begin{frame}{Versions of Gradient Descent (Ctd.)}{}
	\begin{itemize}
		\item \highlight{Batch gradient descent}: Compute gradient based on \textbf{\underline{ALL}} data points
		\begin{equation}
			\bm{\theta}^{(t+1)} \longleftarrow \bm{\theta}^{(t)} - \alpha
				\textcolor{myblue1}{\bm{\sum_{i=1}^{n}}} \nabla \ell(h_{\bm{\theta}^{(t)}}(\bm{x}^{(i)}), y^{(i)})
		\end{equation}
		
		\item \highlight{Stochastic gradient descent}: Compute gradient based on a \textbf{\underline{SINGLE}} data point
			\Highlight{(pick training example randomly and not sequentially!)}
		\item \texttt{For} $i \in \{1, \dots, n\}$ \texttt{do:}
		\vspace*{-3mm}
		\begin{equation}
			\bm{\theta}^{(t+1)} \longleftarrow \bm{\theta}^{(t)} - \alpha \nabla \ell(h_{\bm{\theta}^{(t)}}(\bm{x}^{(i)}), y^{(i)})
		\end{equation}
	\end{itemize}
\end{frame}


% Solving linear Regression using Gradient Descent
\begin{frame}{Solving linear Regression using Gradient Descent}{}
	\begin{itemize}
		\item Randomly initialize $\bm{\theta}$
		\item To minimize the error, keep changing $\bm{\theta}$ according to:
		\begin{equation}
			\bm{\theta}^{(t+1)} \longleftarrow \bm{\theta}^{(t)}
				- \alpha \nabla_{\bm{\theta}}\mathcal{J}(\bm{\theta}^{(t)})
		\end{equation}
		\item We need to calculate $\nabla_{\theta_j}\mathcal{J}(\bm{\theta}^{(t)})$: 
			{\footnotesize (based on a single example)}
		{\footnotesize
		\begin{align*}
			\nabla_{\theta_j}\mathcal{J}(\bm{\theta}^{(t)})
				&= \frac{\partial}{\partial \theta_j} \frac{1}{2}(h_{\bm{\theta}}(\bm{x}) - y)^2
					= 2 \cdot \frac{1}{2} (h_{\bm{\theta}}(\bm{x}) - y) \cdot \frac{\partial}{\partial \theta_j}
						(h_{\bm{\theta}}(\bm{x}) - y) \\
				&= (h_{\bm{\theta}}(\bm{x}) - y) \cdot \frac{\partial}{\partial \theta_j}
					(\theta_0 x_0 + \dots + \theta_{m+1} x_{m+1} - y)
					= \boxed{(h_{\bm{\theta}}(\bm{x}) - y) x_j}
		\end{align*}}
	\end{itemize}
\end{frame}


% Solving the introductory Example
\begin{frame}{Solving the introductory Example}{}
	\divideTwo{0.49}{
		\vspace*{4mm}
		\input{05_regression/01_tikz/data_regression_marketing_expenses_with_curve}
	}{0.49}{
		\begin{itemize}
			\item $\theta_0 \approx 7.4218$
			\item $\theta_1 \approx 2.9827$
			\item $\mathcal{J}(\bm{\theta}) \approx 446.9584$
			\item $h_{\bm{\theta}}(\bm{x}) = 7.4218 + 2.9827 \cdot x_1$
			\item $R = h_{\bm{\theta}}(2.7) = \bm{\underline{\underline{15.4750}}}$
		\end{itemize}
	}
\end{frame}


% Disadvantage of Gradient Descent
\begin{frame}{Disadvantage of Gradient Descent}{}
	\input{05_regression/01_tikz/local_optima}
\end{frame}


% Section: Probabilistic Regression
%______________________________________________________________________
\section{Probabilistic Regression}
\makedivider{Probabilistic Regression}

% Probabilistic Regression
\begin{frame}{Probabilistic Regression}{}
	\begin{itemize}
		\item \textbf{Assumption 1:} The target function values are generated by adding noise the true function's estimate:
		\begin{equation}
			y = f(\bm{x}, \bm{\theta}) + \epsilon
		\end{equation}
		\item \textbf{Assumption 2:} The noise is a Gaussian random variable:
		\begin{align}
			\epsilon &\thicksim \mathcal{N}(0, \beta^{-1}) \\
			p(y \vert \bm{x}, \bm{\theta}, \beta) &= \mathcal{N}(y \vert f(\bm{x}, \bm{\theta}), \beta^{-1})
		\end{align}
		\item \highlight{$y$ is now a random variable!}
	\end{itemize}
\end{frame}


% Probabilistic Regression (Ctd.)
\begin{frame}{Probabilistic Regression (Ctd.)}{}
	\begin{figure}
		\includegraphics[scale=0.35]{05_regression/02_img/probabilistic_regression}
	\end{figure}
\end{frame}


% Maximum Likelihood Regression
\begin{frame}{Maximum Likelihood Regression}{}
	\begin{itemize}
		\item \textbf{Given:} A labeled set of training data points $\{ (\bm{x}^{(i)}, y^{(i)}) \}_{i=1}^n$
		\item \textbf{Conditional likelihood} (assuming the data is i.\,i.\,d.):
		\begin{align}
			p(\bm{y} \vert \bm{X}, \bm{\theta}, \beta)
				&= \prod_{i=1}^n \mathcal{N}(y^{(i)} \vert f(\bm{x}^{(i)}), \beta^{-1}) \\
				&= \prod_{i=1}^n \mathcal{N}(y^{(i)} \vert \bm{\theta}^{\intercal}\bm{x}^{(i)}, \beta^{-1})
		\end{align}
		\item Maximize the likelihood w.\,r.\,t. $\bm{\theta}$ and $\beta$
	\end{itemize}
\end{frame}


% Maximum Likelihood Regression (Ctd.)
\begin{frame}{Maximum Likelihood Regression (Ctd.)}{}
	Simplify using the \textbf{log}-likelihood:
	\begin{align}
		\log p(\bm{y} \vert \bm{X}, \bm{\theta}, \beta)
			&= \sum_{i=1}^n \log\mathcal{N}(y^{(i)} \vert \bm{\theta}^{\intercal}\bm{x}^{(i)}, \beta^{-1}) \\
			&= \sum_{i=1}^n \left[ \log\left( \frac{\sqrt{\beta}}{\sqrt{2\pi}} \right) - 
				\frac{\beta}{2}(y^{(i)} - \bm{\theta}^{\intercal}\bm{x}^{(i)})^2 \right] \\
			&= \frac{n}{2} \log\beta - \frac{n}{2}\log(2\pi) - \frac{\beta}{2}\sum_{i=1}^n(y^{(i)} -
				\bm{\theta}^{\intercal}\bm{x}^{(i)})^2
	\end{align}
\end{frame}


% Section: Basis Function Regression
%______________________________________________________________________
\section{Basis Function Regression}
\makedivider{Basis Function Regression}

\begin{frame}{}{}

\end{frame}


% Section: Wrap-Up
%______________________________________________________________________
\section{Wrap-Up}

% Subsection: Summary
% --------------------------------------------------------------------------------------------------------
\subsection{Summary}

% Summary
\begin{frame}{Summary}{}

\end{frame}

% Meme of the day
\begin{frame}{Summary}{Meme of the day}
	\begin{figure}
		\includegraphics[scale=0.25]{05_regression/02_img/gauss_meme.png}
	\end{figure}
\end{frame}


% Subsection: Lecture Overview
% --------------------------------------------------------------------------------------------------------
\subsection{Lecture Overview}

\makeoverview{3}


% Subsection: Self-Test Questions
% --------------------------------------------------------------------------------------------------------
\subsection{Self-Test Questions}

% Self-Test Questions
\begin{frame}{Self-Test Questions}{}

\end{frame}

% Subsection: Recommended Literature and further Reading
% --------------------------------------------------------------------------------------------------------
\subsection{Recommended Literature and further Reading}

% Recommended Literature and further Reading
\begin{frame}{Recommended Literature and further Reading}{}
	\begin{itemize}
	\item "Pattern Recognition and Machine Learning", Christopher M. Bishop - Chapter 3.1 Linear Models for Regression
	\item Stanford CS229 course notes, Andrew Ng, \url{http://cs229.stanford.edu/summer2019/cs229-notes1.pdf}
	\item "Machine Learning: A Probabilistic Perspective", Kevin P. Murphy, Chapters 1.4.5 Linear Regression and 1.4.7 Overfitting
	\end{itemize}
\end{frame}

% Literature
%______________________________________________________________________
\begin{frame}{Recommended Literature and further Reading}{}
	\footnotesize
	\begin{thebibliography}{2}

	\end{thebibliography}
\end{frame}


% Thank you
%______________________________________________________________________
\makethanks

\end{document}