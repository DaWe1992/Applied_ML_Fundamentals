\input{preamble_theme}

% ====================================================
% ====================================================
% PRESENTATION DATA
% ====================================================
% ====================================================

\title[Regression]{*** Applied Machine Learning Fundamentals *** Regression}
\institute{SAP\,SE}
\author{Daniel Wehner}
\date{\today}
\prefix{REG}

% ====================================================
% ====================================================
% BEGIN OF DOCUMENT
% ====================================================
% ====================================================

\begin{document}

% Title frame
%______________________________________________________________________
\maketitlepage


% Agenda
%______________________________________________________________________
\begin{frame}{Agenda \today}
	\begin{multicols}{2}
		\tableofcontents
	\end{multicols}
\end{frame}


% Section: Introduction to Regression
%______________________________________________________________________
\section{Introduction to Regression}
\makedivider{Introduction to Regression}

% Subsection: What is Regression?
% --------------------------------------------------------------------------------------------------------
\subsection{What is Regression?}

% Regression
\begin{frame}{Regression}{}
	\vspace*{-6mm}
	\begin{tabbing}
		\hspace*{7cm}\= \kill
		\highlight{Type of target variable}			\> Continuous 		\\[1mm]
		\highlight{Type of training information} 	\> Supervised 		\\[1mm]
		\highlight{Example Availability}			\> Batch learning		\\[1mm]
	\end{tabbing}
	
	\vspace*{-7mm}
	\footnotesize
	\begin{boxBlue}
		\textbf{Algorithm sketch:} Given the training data $\mathcal{D}$ the algorithm derives
		a function of the type
	
		\vspace*{-6mm}
		\begin{equation}
			h_{\bm{\theta}}(\bm{x}) = \theta_0 + \theta_1 x_1 + \dots + \theta_{m+1} x_{m}
				\qquad \bm{x} \in \mathbb{R}^{m}, \bm{\theta} \in \mathbb{R}^{m+1}
		\end{equation}

		from the data. $\bm{\theta}$ is the parameter vector containing the coefficients to be estimated by the regression
 		algorithm. Once $\bm{\theta}$ is learned it can be used for prediction. 
	\end{boxBlue}
\end{frame}


% Example Data Set: Revenues
\begin{frame}{Example Data Set: Revenues}{}
	\divideTwo{0.44}{
		\input{05_regression/01_tikz/data_regression_marketing_expenses}
	}{0.55}{
		\begin{itemize}
			\item Find a linear function:
			\begin{equation*}
				h_{\bm{\theta}}(\bm{x}) = \theta_0 + \theta_1 x_1 + \dots + \theta_{m+1} x_{m}
			\end{equation*}
			\item Usually: $x_0 = 1$:
		\end{itemize}
		\begin{align*}
			\bm{\widehat{x}} \in \mathbb{R}^{m+1}
				&= [1\ \bm{x}]^{\intercal} \\
			h_{\bm{\theta}}(\bm{\widehat{x}}) = \sum_{j=0}^{m+1} \theta_j x_j
				&= \bm{\theta}^{\intercal} \bm{\widehat{x}}
		\end{align*}
	}
\end{frame}


% Subsection: Least Squares Error Function
% --------------------------------------------------------------------------------------------------------
\subsection{Least Squares Error Function}

% Error Function for Regression
\begin{frame}{Error Function for Regression}{}
	\begin{itemize}
		\item In order to know how good the function fits we need an error function $\mathcal{J}(\bm{\theta})$:
		\begin{equation}
			\mathcal{J}(\bm{\theta}) = \frac{1}{2n} \sum_{i=1}^n (h_{\bm{\theta}}(\bm{\widehat{x}}^{(i)}) - y^{(i)})^2
		\end{equation}
		\item We want to minimize $\mathcal{J}(\bm{\theta})$:
		\begin{equation*}
			\min_{\bm{\theta}} \frac{1}{2n} \sum_{i=1}^n (h_{\bm{\theta}}(\bm{\widehat{x}}^{(i)}) - y^{(i)})^2
		\end{equation*}
		\item This is \highlight{ordinary least squares (OLS)}
	\end{itemize}
\end{frame}


% Error Function Intuition
\begin{frame}{Error Function Intuition}{}
	\bubble{12}{9}{
		\scriptsize Why the \textbf{square} \\[-2mm]
		\scriptsize in the error function?
	}
	\input{05_regression/01_tikz/error_function_intuition}
\end{frame}


% Section: Solutions to Regression
%______________________________________________________________________
\section{Solutions to Regression}
\makedivider{Solutions to Regression}

% Subsection: Closed-Form Solutions and Normal Equation
% --------------------------------------------------------------------------------------------------------
\subsection{Closed-Form Solutions and Normal Equation}

% Closed-Form Solutions
\begin{frame}{Closed-Form Solutions}{}
	\bubble{0.5}{5.5}{
		\scriptsize sample mean $\overline x$
	}
	\begin{itemize}
		\item Usual approach (for two unknowns): Calculate $\theta_0$ and $\theta_1$ according to
		\begin{equation}
			\theta_0 = \overline{y} - \theta_1 \overline{x} \qquad\qquad
			\theta_1 = \frac{\sum_{i=1}^n (x^{(i)} - \overline{x}) \cdot
				(y^{(i)} - \overline{y})}{\sum_{i=1}^n (x^{(i)} - \overline{x})^2}
		\end{equation}
		\item \highlight{`Normal equation'} (scales to arbitrary dimensions):
		\begin{equation}
			\bm{\theta} = \underbracket{
				(\bm{\widehat{X}}^{\intercal} \bm{\widehat{X}})^{-1} \bm{\widehat{X}}^{\intercal}
			}_{\substack{\text{Moore-Penrose} \\ \text{pseudo-inverse}}} \bm{y}
		\end{equation}
		$\bm{\widehat{X}}$ is called `design matrix' or `regressor matrix'
	\end{itemize}
\end{frame}


% Design Matrix / Regressor Matrix
\begin{frame}{Design Matrix / Regressor Matrix}{}
	\bubble{12}{4.5}{
		\scriptsize In the following \\
		\scriptsize $\bm{\widehat{X}} \equiv \bm{X}$
	}
	\begin{itemize}
		\item The design matrix $\bm{\widehat{X}} \in \mathbb{R}^{n \times (m + 1)}$ looks as follows:
		\footnotesize
		\begin{equation}
			\bm{\widehat{X}} =
			\begin{pmatrix}
  				1 & x_1^{(1)} & x_2^{(1)} & \cdots & x_m^{(1)} \\
				1 & x_1^{(2)} & x_2^{(2)} & \cdots & x_m^{(2)} \\
				1 & x_1^{(3)} & x_2^{(3)} & \cdots & x_m^{(3)} \\
				\vdots & \vdots & \vdots & \ddots & \vdots \\
				1 & x_1^{(n)} & x_2^{(n)} & \cdots & x_m^{(n)} \\
 			\end{pmatrix}
		\end{equation}
		\normalsize
		\item And the $n \times 1$ label vector:
		\footnotesize
		\begin{equation*}
			 \bm{y} =
			 \begin{pmatrix}
				y^{(1)}, y^{(2)}, y^{(3)}, \dots, y^{(n)}
			\end{pmatrix}^{\intercal}
		\end{equation*}
	\end{itemize}
\end{frame}


% Derivation of the Normal Equation
\begin{frame}{Derivation of the Normal Equation}{}\optional
	\begin{itemize}
		\item The derivation involves a bit of linear algebra
		\item Step \ding{182}: Rewrite $\mathcal{J}(\bm{\theta})$ in matrix-vector notation:
		\begin{align*}
			\mathcal{J}(\bm{\theta})
				&= \frac{1}{2}(\bm{X} \bm{\theta} - \bm{y})^{\intercal} (\bm{X} \bm{\theta} - \bm{y}) \\
				&= ((\bm{X} \bm{\theta})^{\intercal} - \bm{y}^{\intercal}) (\bm{X} \bm{\theta} - \bm{y}) \\
				&= (\bm{X} \bm{\theta})^{\intercal} \bm{X} \bm{\theta} - (\bm{X} \bm{\theta})^{\intercal} \bm{y}
					- \bm{y}^{\intercal} (\bm{X} \bm{\theta}) + \bm{y}^{\intercal} \bm{y} \\
				&= \bm{\theta}^{\intercal} \bm{X}^{\intercal} \bm{X} \bm{\theta}
					- 2 (\bm{X} \bm{\theta})^{\intercal} \bm{y} + \bm{y}^{\intercal} \bm{y}
		\end{align*}
		\item To be continued...
	\end{itemize}
\end{frame}


% Derivation of the Normal Equation (Ctd.)
\begin{frame}{Derivation of the Normal Equation (Ctd.)}{}\optional
	\begin{itemize}
		\item Step \ding{183}: Calculate the derivative of $\mathcal{J}(\bm{\theta})$ and set it to zero:
		\begin{align*}
			\nabla_{\bm{\theta}}\mathcal{J}(\bm{\theta})
				&= 2 \bm{X}^{\intercal} \bm{X} \bm{\theta} - 2 \bm{X}^{\intercal} \bm{y} \overset{!}{=} \bm{0} \\	
				&\Leftrightarrow \bm{X}^{\intercal} \bm{X} \bm{\theta} = \bm{X}^{\intercal} \bm{y}
		\end{align*}
		\item If $\bm{X}^{\intercal} \bm{X}$ is invertible, we can multiply both sides by
			$(\bm{X}^{\intercal} \bm{X})^{-1}$:
		\vspace*{3mm}
		\begin{boxBlueNoFrame}
			\highlight{Normal equation:}
			\begin{equation*}
				\bm{\theta} = (\bm{X}^{\intercal} \bm{X})^{-1} \bm{X}^{\intercal} \bm{y}
			\end{equation*}
		\end{boxBlueNoFrame}
	\end{itemize}
\end{frame}


% Problems with Matrix Inversion?
\begin{frame}{Problems with Matrix Inversion?}{}
	\begin{itemize}
		\item What if $(\bm{X}^{\intercal} \bm{X})^{-1}$ does not exist?
		\item Problems and solutions:
		\begin{enumerate}
			\item Linearly dependent (redundant) features or design matrix does not have full rank? 
				(E.\,g. size in m$^2$ and size in feet$^2$) \\
				\highlight{$\Rightarrow$ Delete correlated features}
			\item Too many features ($m > n$)? \\
				\highlight{$\Rightarrow$ Delete features (e.\,g. using PCA) / add training examples}
			\item Other numerical instabilities? \\
				\highlight{$\Rightarrow$ Add a regularization term} (later)
			\item Computationally too expensive? \\
				\highlight{$\Rightarrow$ Use gradient descent}
		\end{enumerate}
	\end{itemize}
\end{frame}


% Subsection: Gradient Descent
% --------------------------------------------------------------------------------------------------------
\subsection{Gradient Descent}

% Gradient Descent
\begin{frame}{Gradient Descent}{}\important
	\begin{itemize}
		\item We want to minimize a smooth function $\mathcal{J} : \mathbb{R}^{m+1} \rightarrow \mathbb{R}$:
		\begin{equation*}
			\min_{\bm{\theta} \in \mathbb{R}^{m+1}} \mathcal{J}(\bm{\theta})
		\end{equation*}
		\item Update the parameters iteratively:
		\begin{equation}
			\bm{\theta}^{(t+1)} \longleftarrow \bm{\theta}^{(t)} -
				\alpha \nabla_{\bm{\theta}} \mathcal{J}(\bm{\theta}^{(t)})
		\end{equation}
		\item where $\alpha > 0$ (\highlight{learning rate}) and $\nabla_{\bm{\theta}} \mathcal{J}(\bm{\theta})$
			is the gradient of $\mathcal{J}(\bm{\theta})$ w.\,r.\,t. $\bm{\theta}$:
		\begin{equation*}
			\nabla_{\bm{\theta}} \mathcal{J}(\bm{\theta}) = 
			\begin{pmatrix}
				\frac{\partial \mathcal{J}(\bm{\theta})}{\partial \theta_0},
				\frac{\partial \mathcal{J}(\bm{\theta})}{\partial \theta_1}, 
				\dots,
				\frac{\partial \mathcal{J}(\bm{\theta})}{\partial \theta_{m+1}}
			\end{pmatrix}^{\intercal}
		\end{equation*}
	\end{itemize}
\end{frame}


% Data Input Space vs. Hypothesis Space
\begin{frame}{Data Input Space vs. Hypothesis Space}{}
	\divideTwoTop{0.49}{
		\highlight{Data input space}
		\input{05_regression/01_tikz/data_input_space}
	}{0.49}{
		\highlight{Hypothesis space $\mathcal{H}$}
		\input{05_regression/01_tikz/hypothesis_space}
	}
\end{frame}


% Data Input Space vs. Hypothesis Space (Ctd.)
\begin{frame}{Data Input Space vs. Hypothesis Space (Ctd.)}{}
	\begin{itemize}
		\item \textbf{Data input space}
		\begin{itemize}
			\item Determined by the $m$ \textbf{attributes} of the data set $x_1, x_2, \dots, x_m$
			\item Often high-dimensional
		\end{itemize}
		\item \textbf{Hypothesis space $\mathcal{H}$}
		\begin{itemize}
			\item Determined by the \textbf{number of parameters} of the model
			\item Each point in the hypothesis space corresponds to a \textbf{specific assignment of model parameters}
			\item The error function gives information about how good this assignment is
			\item \highlight{Gradient descent is applied in the hypothesis space $\mathcal{H}$}
		\end{itemize}
	\end{itemize}
\end{frame}


% Visualization of Gradient Descent in 3 Dimensions
\begin{frame}{Visualization of Gradient Descent in 3 Dimensions}{}
	\input{05_regression/01_tikz/hypothesis_space_gradient_descent}
\end{frame}


% Versions of Gradient Descent
\begin{frame}{Versions of Gradient Descent}{}
	\begin{itemize}
		\item Assume some training data $\mathcal{D}$: $\{ \bm{x}^{(i)}, y^{(i)} \}_{i=1}^{n}$
		\item Squared error for a \textbf{single} example: $\ell(y_{pred}, y_{true}) = (y_{pred} - y_{true})^2$
		\item Our objective is to minimize the \textbf{total} error:
		\begin{equation*}
			\min_{\bm{\theta} \in \mathbb{R}^{m+1}} \mathcal{J}(\bm{\theta}) =
				\min_{\bm{\theta} \in \mathbb{R}^{m+1}} \sum_{i=1}^n \ell(h_{\bm{\theta}}(\bm{x}^{(i)}), y^{(i)})
		\end{equation*}
		\item Three versions of gradient descent:
		\begin{enumerate}
			\item Batch gradient descent
			\item Stochastic gradient descent
			\item Mini-batch gradient descent
		\end{enumerate}
	\end{itemize}
\end{frame}


% Versions of Gradient Descent (Ctd.)
\begin{frame}{Versions of Gradient Descent (Ctd.)}{}
	\begin{itemize}
		\item \highlight{Batch gradient descent}: Compute gradient based on \textbf{\underline{ALL}} data points
		\begin{equation}
			\bm{\theta}^{(t+1)} \longleftarrow \bm{\theta}^{(t)} - \alpha
				\textcolor{myblue1}{\bm{\sum_{i=1}^{n}}} \nabla \ell(h_{\bm{\theta}^{(t)}}(\bm{x}^{(i)}), y^{(i)})
		\end{equation}
		
		\item \highlight{Stochastic gradient descent}: Compute gradient based on a \textbf{\underline{SINGLE}} data point
			\Highlight{(pick training example randomly and not sequentially!)}
		\item \texttt{For} $i \in \{1, \dots, n\}$ \texttt{do:}
		\vspace*{-3mm}
		\begin{equation}
			\bm{\theta}^{(t+1)} \longleftarrow \bm{\theta}^{(t)} - \alpha \nabla \ell(h_{\bm{\theta}^{(t)}}(\bm{x}^{(i)}), y^{(i)})
		\end{equation}
	\end{itemize}
\end{frame}


% Solving linear Regression using Gradient Descent
\begin{frame}{Solving linear Regression using Gradient Descent}{}
	\begin{itemize}
		\item Randomly initialize $\bm{\theta}$
		\item To minimize the error, keep changing $\bm{\theta}$ according to:
		\begin{equation}
			\bm{\theta}^{(t+1)} \longleftarrow \bm{\theta}^{(t)}
				- \alpha \nabla_{\bm{\theta}}\mathcal{J}(\bm{\theta}^{(t)})
		\end{equation}
		\item We need to calculate $\nabla_{\theta_j}\mathcal{J}(\bm{\theta}^{(t)})$: 
			{\footnotesize (based on a single example)}
		{\footnotesize
		\begin{align}
			\frac{\partial}{\partial \theta_j} \mathcal{J}(\bm{\theta})
				&= \frac{\partial}{\partial \theta_j} \frac{1}{2}(h_{\bm{\theta}}(\bm{x}) - y)^2
					= 2 \cdot \frac{1}{2} (h_{\bm{\theta}}(\bm{x}) - y) \cdot \frac{\partial}{\partial \theta_j}
						(h_{\bm{\theta}}(\bm{x}) - y) \\
				&= (h_{\bm{\theta}}(\bm{x}) - y) \cdot \frac{\partial}{\partial \theta_j}
					(\theta_0 x_0 + \dots + \theta_{m+1} x_{m+1} - y)
					= \boxed{(h_{\bm{\theta}}(\bm{x}) - y) x_j}
		\end{align}}
	\end{itemize}
\end{frame}


% Solving the introductory Example
\begin{frame}{Solving the introductory Example}{}
	\divideTwo{0.49}{
		\vspace*{4mm}
		\input{05_regression/01_tikz/data_regression_marketing_expenses_with_curve}
	}{0.49}{
		\begin{itemize}
			\item $\theta_0 \approx 7.4218$
			\item $\theta_1 \approx 2.9827$
			\item $\mathcal{J}(\bm{\theta}) \approx 446.9584$
			\item $h_{\bm{\theta}}(\bm{x}) = 7.4218 + 2.9827 \cdot x_1$
			\item $R = h_{\bm{\theta}}(2.7) = \bm{\underline{\underline{15.4750}}}$
		\end{itemize}
	}
\end{frame}


% Disadvantage of Gradient Descent
\begin{frame}{Disadvantage of Gradient Descent}{}
	\input{05_regression/01_tikz/local_optima}
\end{frame}


% Section: Probabilistic Regression
%______________________________________________________________________
\section{Probabilistic Regression}
\makedivider{Probabilistic Regression}

% Subsection: Underlying Assumptions
% --------------------------------------------------------------------------------------------------------
\subsection{Underlying Assumptions}

% Probabilistic Regression
\begin{frame}{Probabilistic Regression}{}
	\begin{itemize}
		\item \textbf{Assumption 1:} The target function values are generated by adding noise the true function's estimate:
		\begin{equation}
			y = f(\bm{x}, \bm{\theta}) + \epsilon
		\end{equation}
		\item \textbf{Assumption 2:} The noise is a Gaussian random variable:
		\begin{align}
			\epsilon &\thicksim \mathcal{N}(0, \beta^{-1}) \\
			p(y \vert \bm{x}, \bm{\theta}, \beta) &= \mathcal{N}(y \vert f(\bm{x}, \bm{\theta}), \beta^{-1})
		\end{align}
		\item \highlight{$y$ is now a random variable!}
	\end{itemize}
\end{frame}


% Probabilistic Regression (Ctd.)
\begin{frame}{Probabilistic Regression (Ctd.)}{}
	\begin{figure}
		\includegraphics[scale=0.35]{05_regression/02_img/probabilistic_regression}
	\end{figure}
\end{frame}


% Subsection: Gradient Descent
% --------------------------------------------------------------------------------------------------------
\subsection{Maximum Likelihood Solution}

% Maximum Likelihood Regression
\begin{frame}{Maximum Likelihood Regression}{}
	\begin{itemize}
		\item \textbf{Given:} A labeled set of training data points $\{ (\bm{x}^{(i)}, y^{(i)}) \}_{i=1}^n$
		\item \textbf{Conditional likelihood} (assuming the data is i.\,i.\,d.):
		\begin{align}
			p(\bm{y} \vert \bm{X}, \bm{\theta}, \beta)
				&= \prod_{i=1}^n \mathcal{N}(y^{(i)} \vert f(\bm{x}^{(i)}), \beta^{-1}) \\
				&= \prod_{i=1}^n \mathcal{N}(y^{(i)} \vert \bm{\theta}^{\intercal}\bm{x}^{(i)}, \beta^{-1})
		\end{align}
		\item Maximize the likelihood w.\,r.\,t. $\bm{\theta}$ and $\beta$
	\end{itemize}
\end{frame}


% Maximum Likelihood Regression (Ctd.)
\begin{frame}{Maximum Likelihood Regression (Ctd.)}{}
	\bubble{1}{10.5}{
		\scriptsize Remember log-rules?
	}
	Simplify using the \textbf{log}-likelihood:
	\begin{align}
		\log p(\bm{y} \vert \bm{X}, \bm{\theta}, \beta)
			&= \sum_{i=1}^n \log\mathcal{N}(y^{(i)} \vert \bm{\theta}^{\intercal}\bm{x}^{(i)}, \beta^{-1}) \\
			&= \sum_{i=1}^n \left[ \log\left( \frac{\sqrt{\beta}}{\sqrt{2\pi}} \right) - 
				\frac{\beta}{2}(y^{(i)} - \bm{\theta}^{\intercal}\bm{x}^{(i)})^2 \right] \\
			&= \frac{n}{2} \log\beta - \frac{n}{2}\log(2\pi) - \frac{\beta}{2}\sum_{i=1}^n(y^{(i)} -
				\bm{\theta}^{\intercal}\bm{x}^{(i)})^2
	\end{align}
\end{frame}


% Maximum Likelihood Regression (Ctd.)
\begin{frame}{Maximum Likelihood Regression (Ctd.)}{}
	\begin{itemize}
		\item Compute the gradient w.\,r.\,t. $\bm{\theta}$:
		\begin{align*}
			\nabla_{\bm{\theta}} \log p(\bm{y} \vert \bm{X}, \bm{\theta}, \beta) &= \bm{0} \\
			-\beta \sum_{i=1}^n (y^{(i)} - \bm{\theta}^{\intercal} \bm{x}^{(i)})\bm{x}^{(i)} &= \bm{0} \\
			\dots \\
			\bm{\theta}_{ml} &= (\bm{X}^{\intercal}\bm{X})^{-1} \bm{X}^{\intercal} \bm{y}
		\end{align*}
		\item \highlight{Same result as in least squares regression}
	\end{itemize}
\end{frame}


% We have derived the squared Error!
\begin{frame}{We have derived the squared Error!}{}\important
	\begin{boxBlue}
		Minimizing the squared error gives the maximum likelihood solution for the parameters $\bm{\theta}$ assuming
		Gaussian noise.
	\end{boxBlue}
	\begin{itemize}
		\item The maximum likelihood approach gives rise to the squared error
		\item But it is much more powerful than regular least squares
			\highlight{$\Rightarrow$ We can estimate the uncertainty $\beta$}
	\end{itemize}
	\begin{equation}
		\beta_{ML} = \left( \frac{1}{n} \sum_{i=1}^n
			(y^{(i)} - \bm{\theta}_{ML}^{\intercal} \bm{x}^{(i)})^2 \right)^{-1}
	\end{equation}
\end{frame}


% Section: Basis Function Regression
%______________________________________________________________________
\section{Basis Function Regression}
\makedivider{Basis Function Regression}

% Subsection: General Idea
% --------------------------------------------------------------------------------------------------------
\subsection{General Idea}

% What if...?
\begin{frame}{What if...?}{}
	\bubble{10}{7.5}{
		\scriptsize This best-fitting function is \\[-2mm]
		\scriptsize obviously \textbf{not a straight line}! \\

		\scriptsize \highlight{What would you do?}
	}
	\begin{itemize}
		\item So far we have fitted linear functions
		\item \Highlight{What if the data is not linear...?}
	\end{itemize}
	\includegraphics[scale=0.5]{05_regression/02_img/non_linear_data}
\end{frame}


% Basis Functions
\begin{frame}{Basis Functions}{}
	\bubble{2}{7.5}{
		\scriptsize We assume 1-D data
	}
	\begin{itemize}
		\item Remember: \highlight{`When stuck switch to a different perspective'}
		\item We can add \textbf{higher-order} features using \highlight{basis functions $\varphi$}:
		\begin{equation}
			h_{\bm{\theta}}(x) = \sum_{j=0}^P \theta_j\varphi_j(x)
		\end{equation}
		\item There exist several types of basis functions:
		\begin{itemize}
			\item \highlight{linear:} $\varphi_0(x) = 1$ and $\varphi_1(x) = x$
			\item \highlight{polynomial} $\Rightarrow$ see below
			\item \highlight{radial basis functions (RBFs)} $\Rightarrow$ see below
			\item \highlight{Fourier basis}
		\end{itemize}
	\end{itemize}
\end{frame}


% New Design Matrix
\begin{frame}{New Design Matrix}{}
	Applying the basis functions to $\bm{X}$ we get the new design matrix $\bm{\Phi}$:
	
	\footnotesize
	\begin{equation}
		\bm{\Phi} =
		\begin{pmatrix}
			\varphi_0(x^{(1)}) 	& \varphi_1(x^{(1)}) 	& \varphi_2(x^{(1)}) 	& \hdots & \varphi_P(x^{(1)}) 	\\
			\varphi_0(x^{(2)}) 	& \varphi_1(x^{(2)}) 	& \varphi_2(x^{(2)}) 	& \hdots & \varphi_P(x^{(2)}) 	\\
			\vdots 				& \vdots 			& \vdots 			& \ddots & \vdots 				\\
			\varphi_0(x^{(n)}) 	& \varphi_1(x^{(n)}) 	& \varphi_2(x^{(n)}) 	& \hdots & \varphi_P(x^{(n)}) 
		\end{pmatrix}
	\end{equation}
	
	\vspace*{2mm}
	\begin{boxBlue}
		\footnotesize
		\highlight{The model is still linear in the parameters, so we can still use the same algorithm as before.}
		\Highlight{This is still linear regression (!!!)}
	\end{boxBlue}
\end{frame}


% Subsection: Polynomial Basis Functions
% --------------------------------------------------------------------------------------------------------
\subsection{Polynomial Basis Functions}

% Polynomial Basis Functions
\begin{frame}{Polynomial Basis Functions}{}
	\bubble{8}{6.5}{
		\scriptsize For $N$-D data we would also \\[-2mm]
		\scriptsize include cross-terms!
	}
	\begin{itemize}
		\item A quite frequently used basis function: The \highlight{polynomial basis}
		\begin{align*}
			\varphi_0(x) 			&= 1 	\\
			\varphi_j(x) 			&= x^j	\\
			h_{\bm{\theta}}(x) 	&= \sum_{j=0}^P \theta_j \varphi_j(x) = \theta_0 + \theta_1 x +
									\theta_2 x^2 + \dots + \theta_P x^P
		\end{align*}
		\item Here, $P$ is the degree of the polynomial
		\item Here: $\varphi(x) = [1, x, x^2, x^3, \dots, x^P]$
	\end{itemize}
\end{frame}


% Subsection: Radial Basis Functions
% --------------------------------------------------------------------------------------------------------
\subsection{Radial Basis Functions}

% Radial Basis Functions
\begin{frame}{Basis Functions: Radial Basis Functions}{}
	\begin{itemize}
		\item Yet another possible choice of basis function: \highlight{Radial basis functions}
		\begin{align}
			\varphi_0(x) 	&= 1 \\
			\varphi_j(x) 	&= \exp\left\{ - \nicefrac{1}{2} \Vert x - z_j \Vert^2 / 2 \sigma^2 \right\}
		\end{align}
		\item $\{z_j\}$ are the centers of the radial basis functions
		\item $P$ denotes the number of centers / number of radial basis functions
		\item Often we take each data point as a center, so $P = N$
	\end{itemize}
\end{frame}


% Radial Basis Functions (Ctd.)
\begin{frame}{Radial Basis Functions (Ctd.)}{}
	\vspace{-3mm}
	\begin{figure}
		\includegraphics[scale=0.55]{05_regression/02_img/non_linear_data_rbf}
	\end{figure}
\end{frame}


% Subsection: Regularization Techniques
% --------------------------------------------------------------------------------------------------------
\subsection{Regularization Techniques}

% The Danger of too expressive Models...
\begin{frame}{The Danger of too expressive Models...}{}
	\divideTwo{0.49}{
		\footnotesize
		Polynomial of degree $P = 16$ \\
		\Highlight{($\skull$ severe overfitting $\skull$)}
		\vspace{-3mm}
		\begin{figure}
			\includegraphics[scale=0.45]{05_regression/02_img/non_linear_data_with_poly_fit}
		\end{figure}
	}{0.49}{
		\footnotesize
		RBF with $\sigma = 1.00, P = N$ \\
		(About right)
		\vspace{-3mm}
		\begin{figure}
			\includegraphics[scale=0.45]{05_regression/02_img/non_linear_data_with_rbf_fit}
		\end{figure}
	}
\end{frame}


% Overfitting vs. Underfitting
\begin{frame}{Overfitting vs. Underfitting}{}
	\begin{itemize}
		\item \textbf{Underfitting}
		\begin{itemize}
			\item The model is not complex enough to fit the data well $\Rightarrow$ \highlight{High bias}
			\item Make the model more complex; adding new examples \Highlight{does not help}
		\end{itemize}
		\item \textbf{Overfitting}
		\begin{itemize}
			\item The model predicts the training data perfectly
			\item But it \Highlight{fails to generalize} to unseen instances $\Rightarrow$ \highlight{High variance}
			\item Decrease the degree of freedom or add more training examples
			\item Also: Try \highlight{regularization}
		\end{itemize}
		\item \highlight{Bias-Variance trade-off}
	\end{itemize}
\end{frame}


% First Solution: Smaller Degree
\begin{frame}{First Solution: Smaller Degree}{}
	\bubble{12}{5}{
		\scriptsize Much better :)
	}
	One solution: Use a \highlight{smaller degree} (here: $P = 3$)
	\vspace{-3mm}
	\begin{figure}
		\includegraphics[scale=0.5]{05_regression/02_img/non_linear_data_with_poly_fit_degree_3}
	\end{figure}
\end{frame}


% Second Solution: Regularization
\begin{frame}{Second Solution: Regularization}{}
	\begin{itemize}
		\item Enrich $\mathcal{J}(\bm{\theta})$ with a \highlight{regularization term}
		\item This can \textbf{prevent overfitting} and results in a smoother function \\
			(large values for $\theta_j$ are prevented)
		\item Two forms of regularization, \highlight{L1} and \highlight{L2}:
		\begin{equation*}
			\min_{\bm{\theta}} \mathcal{J}(\bm{\theta}) + \lambda \vert \bm{\theta} \vert
			\quad \rightarrow \text{(\textbf{L1})}
			\qquad\qquad
			\min_{\bm{\theta}} \mathcal{J}(\bm{\theta}) + \lambda \Vert \bm{\theta} \Vert^2
			\quad \rightarrow \text{(\textbf{L2})}
		\end{equation*}
		\vspace*{-3mm}
		\begin{equation*}
			\vert \bm{\theta} \vert = \sum_{j=1}^{m+1} \vert \theta_j \vert
			\qquad\qquad
			\Vert \bm{\theta} \Vert^2 = \sum_{j=1}^{m+1} \theta_j^2
		\end{equation*}
		\item $\lambda \ge 0$ controls the \textbf{degree of regularization}
	\end{itemize}
\end{frame}


% Regularization visualized
\begin{frame}{Regularization visualized}{}
	\bubble{10}{13}{
		\scriptsize L2 (left), L1 (right)
	}
	\divideTwo{0.45}{
		\footnotesize
		\begin{itemize}
			\item Here: $\bm{w} \equiv \bm{\theta}$
			\item L1-Regularization \\
				$\Rightarrow$ \highlight{Lasso regression} \\
				\scriptsize (least abs. shrinkage and select. operator) \footnotesize
			\item L2-Regularization \\
				$\Rightarrow$ \highlight{Ridge regression} \\
				\scriptsize (Tikhonov regularization) \footnotesize
			\item The combination of both is called \highlight{elastic net}
		\end{itemize}
	}{0.54}{
		\begin{figure}
			\includegraphics[scale=0.35]{05_regression/02_img/regularization}
		\end{figure}
	}
\end{frame}


% Incorporating Regularization
\begin{frame}{Incorporating Regularization}{}
	\bubble{10}{4.5}{
		\scriptsize The regularization also \\[-2mm]
		\scriptsize helps to overcome numerical issues!
	}
	\begin{itemize}
		\item Normal equation with regularization:
		\begin{equation}
			\bm{\theta} = (\bm{X}^{\intercal} \bm{X}\ \textcolor{myblue1}{+\ \lambda \bm{I}})^{-1}
				\bm{X}^{\intercal} \bm{y}
		\end{equation}
		\item Regularized gradient descent update rule:
		\begin{align*}
			\bm{\theta}^{(t+1)}
				&\longleftarrow \bm{\theta}^{(t)}
					- \alpha \nabla_{\bm{\theta}}\mathcal{J}(\bm{\theta}^{(t)}) \\
			\frac{\partial}{\partial \theta_j} \mathcal{J}(\bm{\theta})
				&= (h_{\bm{\theta}}(\bm{x}) - y) x_j\ \textcolor{myblue1}{+\ \lambda \theta_j}
		\end{align*}
	\end{itemize}
\end{frame}


% Polynomial Regression with Regularization
\begin{frame}{Polynomial Regression with Regularization}{}
	\divideTwo{0.49}{
		\textbf{At least better}
		\vspace*{-4mm}
		\begin{figure}
			\includegraphics[scale=0.45]{05_regression/02_img/non_linear_data_with_poly_fit_lambda_1000}
		\end{figure}
	}{0.49}{
		\textbf{Way too much regularization}
		\vspace*{-4mm}
		\begin{figure}
			\includegraphics[scale=0.45]{05_regression/02_img/non_linear_data_with_poly_fit_lambda_too_large}
		\end{figure}
	}
\end{frame}


% Section: Wrap-Up
%______________________________________________________________________
\section{Wrap-Up}
\makedivider{Wrap-Up}

% Subsection: Summary
% --------------------------------------------------------------------------------------------------------
\subsection{Summary}

% Summary
\begin{frame}{Summary}{}

\end{frame}


% Subsection: Lecture Overview
% --------------------------------------------------------------------------------------------------------
\subsection{Lecture Overview}

\makeoverview{3}


% Subsection: Self-Test Questions
% --------------------------------------------------------------------------------------------------------
\subsection{Self-Test Questions}

% Self-Test Questions
\begin{frame}{Self-Test Questions}{}

\end{frame}


% Subsection: Recommended Literature and further Reading
% --------------------------------------------------------------------------------------------------------
\subsection{Recommended Literature and further Reading}

% Literature
%______________________________________________________________________
\begin{frame}{Recommended Literature and further Reading}{}
	\footnotesize
	\begin{thebibliography}{2}

	\end{thebibliography}
\end{frame}


% Thank you
%______________________________________________________________________
\makethanks

\end{document}