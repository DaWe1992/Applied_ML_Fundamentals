\input{../preamble_theme}

% ====================================================
% ====================================================
% PRESENTATION DATA
% ====================================================
% ====================================================

\title[Logistic Regression]{*** Applied Machine Learning Fundamentals *** Logistic Regression}
\institute[SAP\,SE]{SAP\,SE / DHBW Mannheim}
\author{Daniel Wehner, M.Sc.}
\date{Winter term 2022/2023}
\prefix{LR}

% ====================================================
% ====================================================
% BEGIN OF DOCUMENT
% ====================================================
% ====================================================

\begin{document}

% Title frame
%______________________________________________________________________
\maketitlepage


% Lecture Overview
%______________________________________________________________________
\begin{frame}{Lecture Overview}{}
	\makeoverview{6}
\end{frame}


% Agenda
%______________________________________________________________________
\begin{frame}{Agenda for this Unit}
	\begin{multicols}{2}
		\tableofcontents
	\end{multicols}
\end{frame}


% Section: Introduction
%______________________________________________________________________
\section{Introduction}
\makedivider{Introduction}

% Subsection: What is logistic Regression?
% --------------------------------------------------------------------------------------------------------
\subsection{What is logistic Regression?}

% What is logistic Regression?
\begin{frame}{What is logistic Regression?}{}
	\begin{itemize}
		\item Learning algorithm for \textbf{classification} {\footnotesize \textit{(despite the name...)}}
		\item In its standard form it's applicable to \textbf{binary classification problems only}, but you can use techniques like:
		\begin{itemize}
			\item \highlight{One-vs-One (OVO)}
			\item \highlight{One-vs-Rest (OVR)}
		\end{itemize}
		\item \textbf{Class labels:}
		\begin{itemize}
			\item The `positive class' $\oplus$ is encoded as \textbf{1}
			\item The `negative class' $\ominus$ as \textbf{0}
		\end{itemize}
		\item \textbf{Probabilistic interpretation:} The output of the algorithm is between 0 and 1
			{\footnotesize \textit{(probability of the instance belonging to the positive class)}}
	\end{itemize}
\end{frame}


% Subsection: Why you should not use linear Regression
% --------------------------------------------------------------------------------------------------------
\subsection{Why you should not use linear Regression}

% Why you should not use linear Regression...
\begin{frame}{Why you should not use linear Regression...}{}\important
	\input{07_logistic_regression/01_tikz/logreg_motivation}
\end{frame}


% Why you should not use linear Regression... (Ctd.)
\begin{frame}{Why you should not use linear Regression... (Ctd.)}{}
	\begin{itemize}
		\item Linear regression: $h_{\bm{\theta}}(\bm{x}) = \bm{\theta}^{\intercal} \bm{x}$
		\item By putting a \textbf{threshold} at 0.5, we can turn linear regression into a classifier
		\begin{itemize}
			\item If $h_{\bm{\theta}}(\bm{x}) \ge 0.5$, predict $y = 1$
			\item If $h_{\bm{\theta}}(\bm{x}) < 0.5$, predict $y = 0$
		\end{itemize}
		\item \Highlight{Problems:}
		\begin{enumerate}
			\item \textbf{Outliers heavily affect the decision boundary}
			\item Furthermore, we only want $0 \le h_{\bm{\theta}}(\bm{x}) \le 1$,
				linear regression can output values $h_{\bm{\theta}}(\bm{x}) \ll 0$ or $h_{\bm{\theta}}(\bm{x}) \gg 1$
		\end{enumerate}
		\item \highlight{We need a better strategy!}
	\end{itemize}
\end{frame}


% Section: Model Architecture
%______________________________________________________________________
\section{Model Architecture}
\makedivider{Model Architecture}

% Subsection: Sigmoid Function
% --------------------------------------------------------------------------------------------------------
\subsection{Sigmoid Function}

% Logistic Regression Model
\begin{frame}{Logistic Regression Model}{}\important
	\begin{itemize}
		\item Remember that we want: $0 \le h_{\bm{\theta}}(\bm{x}) \le 1$
		\item \textbf{Solution:} \highlight{Logistic / Sigmoid function:}
		\begin{equation}
			g(z) = \frac{1}{1 + e^{-z}}
		\end{equation}
		\item We plug $\bm{\theta}^{\intercal} \bm{\bm{x}}$ into the sigmoid function:
		\begin{equation}
			h_{\bm{\theta}}(\bm{x}) = g(\bm{\theta}^{\intercal} \bm{\bm{x}}) = \frac{1}{1 + e^{-(\bm{\theta}^{\intercal} \bm{x})}}
		\end{equation}
	\end{itemize}
\end{frame}


% Logistic/Sigmoid Function
\begin{frame}{Logistic/Sigmoid Function}{}
	\divideTwo{0.49}{
		\vspace*{2mm}
		\input{07_logistic_regression/01_tikz/sigmoid_function}
	}{0.49}{
		\begin{itemize}
			\item $g(z)$ is symmetric around $z = 0$
			\item $0 \le g(z) \le 1$ holds true
		\end{itemize}
	}
\end{frame}


% Where does the Sigmoid come from?
\begin{frame}{Where does the Sigmoid come from?}{}\optional
	{\footnotesize
	\begin{alignat*}{2}
		p(\mathcal{C}_1 \vert \bm{x})
			&=	\frac{p(\bm{x} \vert \mathcal{C}_1) p(\mathcal{C}_1)}{p(\bm{x})}
				= \frac{p(\bm{x} \vert \mathcal{C}_1) p(\mathcal{C}_1)}{\sum_j p(\bm{x}, \mathcal{C}_j)}
				= \frac{p(\bm{x} \vert \mathcal{C}_1) p(\mathcal{C}_1)}{\sum_j p(\bm{x} \vert \mathcal{C}_j) p(\mathcal{C}_j)}
			\\[1mm]
			&= \frac{p(\bm{x} \vert \mathcal{C}_1) p(\mathcal{C}_1)}{
				p(\bm{x} \vert \mathcal{C}_1) p(\mathcal{C}_1) + p(\bm{x} \vert \mathcal{C}_2) p(\mathcal{C}_2)}
			\\[1mm]
			&= \frac{1}{1 + p(\bm{x} \vert \mathcal{C}_2) p(\mathcal{C}_2) / (p(\bm{x} \vert \mathcal{C}_1) p(\mathcal{C}_1))}
			\\[1mm]
			&= \frac{1}{1 + \exp\{ -z \}} = g(z) && \longrightarrow \text{\highlight{logistic sigmoid}}
			\\[1mm]
		z 	&= \log \frac{p(\bm{x} \vert \mathcal{C}_1) p(\mathcal{C}_1)}{p(\bm{x} \vert \mathcal{C}_2) p(\mathcal{C}_2)}
			&& \longrightarrow \text{\highlight{log odds}}
	\end{alignat*}}
\end{frame}


% Subsection: Probabilistic Interpretation
% --------------------------------------------------------------------------------------------------------
\subsection{Probabilistic Interpretation}

% Interpretation of Hypothesis Output
\begin{frame}{Interpretation of Hypothesis Output}{}
	\begin{itemize}
		\item $h_{\bm{\theta}}(\bm{x})$ is interpreted as the probability of instance $\bm{x}$ belonging to class $y = 1$
		\item \textbf{Example:}
		\begin{equation}
			\bm{x} = \begin{bmatrix} x_0 \\ x_1 \end{bmatrix} = \begin{bmatrix} 1 \\ tumorSize \end{bmatrix}
		\end{equation}
		\vspace*{1mm}
		\item If $h_{\bm{\theta}}(\bm{x}) = 0.7$, we have to tell the patient that there
			is a \textbf{70\,\% chance} of the tumor being malignant $\Rightarrow p(y = 1 \vert \bm{x}, \bm{\theta})$
		\item \textbf{Binary case:} $p(y = 0 \vert \bm{x}, \bm{\theta}) = 1 - p(y = 1 \vert \bm{x}, \bm{\theta})$
	\end{itemize}
\end{frame}


% Subsection: Model Training
% --------------------------------------------------------------------------------------------------------
\subsection{Model Training}

% Training Setup
\begin{frame}{Training Setup}{}
	\begin{itemize}
		\item We have a labeled training set ($\Rightarrow$ \textbf{supervised learning}):
		\begin{equation}
			\mathcal{D} = 
				\left\{ (\bm{x}^{(1)}, y^{(1)}), (\bm{x}^{(2)}, y^{(2)}), \dots, (\bm{x}^{(n)}, y^{(n)}) \right\}
				= \left\{ (\bm{x}^{(i)}, y^{(i)}) \right\}_{i=1}^n
		\end{equation}
		\item Each $\bm{x}$ is a vector of features:
		\begin{equation}
			\bm{x} = \begin{bmatrix} x_0 \\ \vdots \\ x_{m} \end{bmatrix} \in \mathbb{R}^{m+1}
			\quad \text{and} \quad x_0 = 1 \quad \text{and} \quad y \in \{0, 1\}
		\end{equation}
		\item \highlight{How to choose the parameters $\bm{\theta}$?}
	\end{itemize}
\end{frame}


% Logistic Regression Cost Function
\begin{frame}{Logistic Regression Cost Function}{}
	\begin{itemize}
		\item Gradient descent is performed in order to find the parameters $\bm{\theta}$
		\item To this end, a cost function is needed:
		\begin{equation}
			\mathcal{J}(\bm{\theta}) = \frac{1}{n} \sum_{i=1}^n \ell(h_{\bm{\theta}}(\bm{x}^{(i)}), y^{(i)})
		\end{equation}
		\item The cost function $\ell(h_{\bm{\theta}}(\bm{x}), y)$ is defined as follows: \\
		{\footnotesize \textit{(square loss would be \textbf{non-convex...})}}
		\begin{equation}
			\ell(h_{\bm{\theta}}(\bm{x}), y) =
			\begin{cases}
				-\log(h_{\bm{\theta}}(\bm{x})) 		& \text{if}\ y = 1	\\
				-\log(1 - h_{\bm{\theta}}(\bm{x}))	& \text{if}\ y = 0
			\end{cases}
		\end{equation}
	\end{itemize}
\end{frame}


% Logistic Regression Cost Function (Ctd.)
\begin{frame}{Logistic Regression Cost Function (Ctd.)}{}
	\divideTwo{0.49}{
		\textbf{\textit{y} = 1:}
		\input{07_logistic_regression/01_tikz/cost_function_1}
	}{0.49}{
		\textbf{\textit{y} = 0:}
		\input{07_logistic_regression/01_tikz/cost_function_2}
	}
\end{frame}


% Logistic Regression Cost Function (Ctd.)
\begin{frame}{Logistic Regression Cost Function (Ctd.)}{}\important
	\begin{itemize}
		\item $\ell(h_{\bm{\theta}}(\bm{x}), y)$ can be written in a more compact form:
		\begin{equation}
			\ell(h_{\bm{\theta}}(\bm{x}), y) = -y \log(h_{\bm{\theta}}(\bm{x})) -
				(1 - y) \log(1 - h_{\bm{\theta}}(\bm{x}))
		\end{equation}
		\vspace*{-6mm}
		\begin{itemize}
			\item If $y = 1$, we get: $-\log(h_{\bm{\theta}}(\bm{x}))$
			\item If $y = 0$, we get: $-\log(1 - h_{\bm{\theta}}(\bm{x}))$
		\end{itemize}
		\item This gives the \highlight{(binary) cross entropy} cost function $\mathcal{J}(\bm{\theta})$:
		\begin{equation}
			\mathcal{J}(\bm{\theta}) = \frac{1}{n} \sum_{i=1}^n \left[
				-y^{(i)} \log(h_{\bm{\theta}}(\bm{x}^{(i)})) - (1 - y^{(i)}) \log(1 - h_{\bm{\theta}}(\bm{x}^{(i)}))
			\right]
		\end{equation}
	\end{itemize}
\end{frame}


% Derivation of (binary) Cross Entropy
\begin{frame}{Derivation of (binary) Cross Entropy}{}\optional
	\begin{itemize}
		\item The likelihood function can be written in the form:
		\begin{equation}
			\mathcal{L}(\bm{\theta}) = \prod_{i=1}^n h_{\bm{\theta}}(\bm{x}^{(i)})^{y^{(i)}} \cdot (1 - h_{\bm{\theta}}(\bm{x}^{(i)}))^{1 - y^{(i)}}
		\end{equation}
		\item The cost function is then given by the \textbf{negative log-likelihood}:
		\begin{equation}
			\mathcal{J}(\bm{\theta}) = -\frac{1}{n} \log \mathcal{L}(\bm{\theta})
		\end{equation}
	\end{itemize}
\end{frame}


% Derivation of (binary) Cross Entropy (Ctd.)
\begin{frame}{Derivation of (binary) Cross Entropy (Ctd.)}{}\optional
	\footnotesize
	\begin{alignat*}{2}
		\mathcal{J}(\bm{\theta})
			&= 	-\frac{1}{n} \log \left[ \prod_{i=1}^n h_{\bm{\theta}}(\bm{x}^{(i)})^{y^{(i)}} \cdot
				(1 - h_{\bm{\theta}}(\bm{x}^{(i)}))^{1 - y^{(i)}} \right] 								\\[2mm]
			&= 	-\frac{1}{n} \sum_{i=1}^n \log \left[ h_{\bm{\theta}}(\bm{x}^{(i)})^{y^{(i)}} \cdot
				(1 - h_{\bm{\theta}}(\bm{x}^{(i)}))^{1 - y^{(i)}} \right]
			&& 	\quad\longrightarrow \log\prod = \sum\log											\\[2mm]
			&= 	\frac{1}{n} \sum_{i=1}^n -\log \left[ h_{\bm{\theta}}(\bm{x}^{(i)})^{y^{(i)}} \right] -
				\log \left[ (1 - h_{\bm{\theta}}(\bm{x}^{(i)}))^{1 - y^{(i)}} \right]
			&&	\quad\longrightarrow \log(a \cdot b) = \log(a) + \log(b)								\\[2mm]
			&= 	\frac{1}{n} \sum_{i=1}^n \left[ -y^{(i)} \log(h_{\bm{\theta}}(\bm{x}^{(i)})) -
				(1 - y^{(i)}) \log(1 - h_{\bm{\theta}}(\bm{x}^{(i)})) \right]
			&&	\quad\longrightarrow \log(a^b) = b \cdot \log(a)
	\end{alignat*}
\end{frame}


% Optimization of (binary) Cross Entropy
\begin{frame}{Optimization of (binary) Cross Entropy}{}
	\begin{itemize}
		\item Unfortunately, there is \Highlight{no closed-form solution} to logistic regression \\
			{\footnotesize \textit{(due to the sigmoid function)}}
		\item We have to resort to an iterative method like \textbf{gradient descent}
		\item We need the gradient of $\mathcal{J}(\bm{\theta})$ which requires some math (cf. next slides)
		\begin{equation}
			\frac{\partial}{\partial \theta_j} \mathcal{J}(\bm{\theta}) = (h_{\bm{\theta}}(\bm{x}) - y) x_j
		\end{equation}
		\item Due to the chain rule we also have to find the derivative of the sigmoid function. \highlight{Let's get started:}
	\end{itemize}
\end{frame}


% Derivative of the Sigmoid Function
\begin{frame}{Derivative of the Sigmoid Function}{}\optional
	%\vspace*{-5mm}
	\footnotesize
	\begin{alignat*}{2}
		g(z) 
			&= 	\frac{1}{1 + e^{-z}} 															\\[4mm]
		\frac{\text{d}}{\text{d}z} g(z) 
			&= 	\frac{0 \cdot (1 + e^{-z}) - (-e^{-z})}{(1 + e^{-z})^2}
			&& 	\quad\longrightarrow \text{quotient rule}	 										\\[2mm]
			&= 	\frac{e^{-z}}{(1 + e^{-z})^2} = \overbracket{\frac{1 + e^{-z} - 1}{(1 + e^{-z})^2}}^{\text{a small trick...}} =
				\frac{1 + e^{-z}}{(1 + e^{-z})^2} - \frac{1}{(1 + e^{-z})^2}
			&&	\quad\longrightarrow \text{algebraic manipulation} 									\\[2mm]
			&= 	\frac{1}{1 + e^{-z}} \left[ 1 - \frac{1}{1 + e^{-z}} \right]
			&&	\quad\longrightarrow \text{factorize term} 											\\[2mm]
			&=	\boxed{g(z) (1 - g(z))}
	\end{alignat*}
\end{frame}


% Derivation of the Gradient based on a single Example $(\bm{x}, y)$
\begin{frame}{Derivation of the Gradient based on a single Example $(\bm{x}, y)$}{}\optional
	%\vspace*{-5mm}
	\footnotesize
	\begin{alignat*}{2}
		\frac{\partial}{\partial \theta_j} \mathcal{J}(\bm{\theta})
			&= 	-\frac{\partial}{\partial \theta_j} y \log(g(\bm{\theta}^{\intercal} \bm{x})) -
			     	\frac{\partial}{\partial \theta_j} (1 - y) \log(1 - g(\bm{\theta}^{\intercal} \bm{x}))
			&& 	\quad\longrightarrow \text{derivative of sum terms}			 						\\[2mm]
			&=	\left[ -\frac{y}{g(\bm{\theta}^{\intercal} \bm{x})} + \frac{1 - y}{1 - g(\bm{\theta}^{\intercal} \bm{x})} \right]
				\frac{\partial}{\partial \theta_j} g(\bm{\theta}^{\intercal} \bm{x})
			&& 	\quad\longrightarrow \text{derivative of log function} 									\\[2mm]
			&=	\left[ -\frac{y}{g(\bm{\theta}^{\intercal} \bm{x})} + \frac{1 - y}{1 - g(\bm{\theta}^{\intercal} \bm{x})} \right]
				g(\bm{\theta}^{\intercal} \bm{x}) (1 - g(\bm{\theta}^{\intercal} \bm{x}))
				\frac{\partial}{\partial \theta_j} \bm{\theta}^{\intercal} \bm{x}
			&& 	\quad\longrightarrow \text{chain rule} 											\\[2mm]
			&= 	\left[ \frac{g(\bm{\theta}^{\intercal} \bm{x}) - y}{g(\bm{\theta}^{\intercal} \bm{x}) (1 - g(\bm{\theta}^{\intercal} \bm{x}))} \right]
				g(\bm{\theta}^{\intercal} \bm{x}) (1 - g(\bm{\theta}^{\intercal} \bm{x})) x_j
			&& 	\quad\longrightarrow \text{algebraic manipulation} 									\\[2mm]
			&= 	(g(\bm{\theta}^{\intercal} \bm{x}) - y) x_j = \boxed{(h_{\bm{\theta}}(\bm{x}) - y) x_j}
			&& 	\quad\longrightarrow \text{cancel terms}
	\end{alignat*}
\end{frame}


% Gradient Descent
\begin{frame}{Gradient Descent}{}\important
	\begin{itemize}
		\item The goal is to minimize $\mathcal{J}(\bm{\theta})$: $\bm{\theta}^*
			= \argmin_{\bm{\theta}} \mathcal{J}(\bm{\theta})$
		\item \texttt{Repeat until convergence \{} \\
			$\qquad \bm{\theta}^{(t+1)} \longleftarrow \bm{\theta}^{(t)} - \alpha \nabla_{\bm{\theta}}
				\mathcal{J}(\bm{\theta}^{(t)}) \quad$
			\textcolor{myblue1}{\textit{// simultaneously update all} $\theta_j$} \\
		\texttt{\}}
		\item The gradient $\nabla_{\bm{\theta}} \mathcal{J}(\bm{\theta})$ is given by:
		{\footnotesize
		\begin{equation}
			\nabla_{\bm{\theta}} \mathcal{J}(\bm{\theta}) = \frac{1}{n} \sum_{i=1}^n \left(
				h_{\bm{\theta}}(\bm{x}^{(i)}) - y^{(i)}
			\right) \bm{x}^{(i)}
		\end{equation}}
	\end{itemize}
	\vspace*{-3mm}
	\begin{boxBlueNoFrame}
		\highlight{Algorithm looks identical to linear regression, but $h_{\bm{\theta}}(\bm{x})$ is different!}
	\end{boxBlueNoFrame}
\end{frame}


% Subsection: Decision Boundary
% --------------------------------------------------------------------------------------------------------
\subsection{Decision Boundary}

% Decision Boundary
\begin{frame}{Decision Boundary}{}
	\divideTwo{0.62}{
		\begin{itemize}
			\item \highlight{We have to set a threshold}
			\item Setting the threshold to 0.5 means:
			\begin{itemize}
				\item Predict the positive class, if
				\begin{equation*}
					h_{\bm{\theta}}(\bm{x}) \ge 0.5 \Leftrightarrow \bm{\theta}^{\intercal} \bm{x} \ge 0
				\end{equation*}
				\item Predict the negative class, if 
				\begin{equation*}
					h_{\bm{\theta}}(\bm{x}) < 0.5 \Leftrightarrow \bm{\theta}^{\intercal} \bm{x} < 0
				\end{equation*}
			\end{itemize}
		\end{itemize}
	}{0.37}{
		\vspace*{2mm}
		\input{07_logistic_regression/01_tikz/sigmoid_function_decision_boundary}
	}
\end{frame}


% Decision Boundary (Ctd.)
\begin{frame}{Decision Boundary (Ctd.)}{}
	\begin{itemize}
		\item Suppose we have the following hypothesis:
		\begin{equation*}
			h_{\bm{\theta}}(\bm{x}) = g(\theta_0 + \theta_1 x_1 + \theta_2 x_2)
		\end{equation*}
		\item Using gradient descent we obtained the following coefficients:
		\begin{equation*}
			\theta_0 = -3 \qquad \theta_1 = 1 \qquad \theta_2 = 1
		\end{equation*}
		\item Predict $y = 1$, if $-3 + x_1 + x_2 \ge 0$
	\end{itemize}
\end{frame}


% Decision Boundary (Ctd.)
\begin{frame}{Decision Boundary (Ctd.)}{}
	\divideTwo{0.33}{
		\input{07_logistic_regression/01_tikz/decision_boundary}
	}{0.66}{
		\begin{itemize}
			\item Predict $y = 1$, if $-3 + x_1 + x_2 \ge 0$
			\item The decision boundary satisfies $-3 + x_1 + x_2 = 0$
			\item If $x_2 = 0$, then $x_1 = 3$ and vice versa
			
			\vspace*{5mm}
			\begin{boxBlueNoFrame}
				\footnotesize
				\highlight{Logistic regression is not a maximum-margin classifier
				(although the cost function can be adjusted to get that $\Rightarrow$ Hinge loss)}
			\end{boxBlueNoFrame}
		\end{itemize}
	}
\end{frame}


% Example: Decision Boundary
\begin{frame}{Example: Decision Boundary}{}
	\divideTwo{0.49}{
		\begin{figure}
			\includegraphics[scale=0.45]{07_logistic_regression/02_img/logreg_example_linear}
		\end{figure}
	}{0.49}{
		\begin{figure}
			\includegraphics[scale=0.45]{07_logistic_regression/02_img/logreg_example_linear_boundary}
		\end{figure}
	}
	
	\begin{center}
		\highlight{Where is the sigmoid function?}
	\end{center}
\end{frame}


% Example: Logistic Function
\begin{frame}{Example: Logistic Function}{}
	\vspace*{-7mm}
	\begin{figure}
		\centering
		\includegraphics[scale=0.6]{07_logistic_regression/02_img/logreg_example_linear_logistic_function}
	\end{figure}
\end{frame}


% Section: Non-linear Data
%______________________________________________________________________
\section{Non-linear Data}
\makedivider{Non-linear Data}

% Subsection: Feature Mapping
% --------------------------------------------------------------------------------------------------------
\subsection{Feature Mapping}

% Non-Linear Decision Boundaries
\begin{frame}{Non-Linear Decision Boundaries}{}
	\begin{itemize}
		\item \highlight{Feature mapping} can be used to obtain non-linear decision boundaries
		\item \textbf{Example:}
		\begin{itemize}
			\item Imagine a circular data set
			\item Using the features...
			\begin{equation*}
				h_{\bm{\theta}}(\bm{x}) = g(\theta_0 + \theta_1 x_1 + \theta_2 x_2 +
				\textcolor{myblue1}{\theta_3 x_1^2} + \textcolor{myblue1}{\theta_4 x_2^2})
			\end{equation*}
			\item ...the algorithm could e.\,g. choose: $\bm{\theta} =
				\begin{bmatrix} -1, 0, 0, 1, 1 \end{bmatrix}^{\intercal}$
			\item So we would get: $x_1^2 + x_2^2 = 1 \Rightarrow$ \textbf{equation of a unit circle}
		\end{itemize}
	\end{itemize}
\end{frame}


% Example: Non-Linear Decision Boundary
\begin{frame}{Example: Non-Linear Decision Boundary}{}
	\vspace*{-2mm}
	\begin{figure}
		\includegraphics[scale=0.325]{07_logistic_regression/02_img/logreg_example_non_linear_boundary}
	\end{figure}
\end{frame}


% It is still linear!
\begin{frame}{It is still linear!}{}
	\begin{figure}
		\includegraphics[scale=0.275]{07_logistic_regression/02_img/basis_function_classification_visualization}
	\end{figure}
\end{frame}


% Subsection: Regularization
% --------------------------------------------------------------------------------------------------------
\subsection{Regularization}

% Logistic Regression Cost Function (Ctd.)
\begin{frame}{Logistic Regression with Regularization}{}
	\begin{itemize}
		\item We should apply regularization for non-linear decision boundaries:
		\footnotesize
		\begin{equation}
			\frac{1}{n} \sum_{i=1}^n \left[
				-y^{(i)} \log(h_{\bm{\theta}}(\bm{x}^{(i)})) - (1 - y^{(i)}) \log(1 - h_{\bm{\theta}}(\bm{x}^{(i)}))
			\right] + \textcolor{myblue1}{\frac{\lambda}{2m} \sum_{j=1}^m \theta_j^2}
		\end{equation}
		\normalsize
		\item The last term prevents the parameters $\theta_j$ from becoming too large
		\item $\lambda \ge 0$ controls the degree of regularization
		\item This leads to smoother decision boundaries
	\end{itemize}
\end{frame}


% Section: Multi-Class Classification
%______________________________________________________________________
\section{Multi-Class Classification}
\makedivider{Multi-Class Classification}

% Subsection: Multiple Classes
% --------------------------------------------------------------------------------------------------------
\subsection{Multiple Classes}

% Multi-Class Classification
\begin{frame}{Multi-Class Classification}{}
	\begin{itemize}
		\item In its basic form logistic regression can handle two classes only
		\item \Highlight{What if there are more than two classes?}
		\item Two approaches:
		\begin{enumerate}
			\item Change the algorithm so that it can deal with more classes \\
				($\rightarrow$ \highlight{Multinomial Logistic Regression} / \highlight{Softmax Regression})
			\item Transform the problem into several binary problems. \\
				Two common techniques are:
			\begin{itemize}
				\item \highlight{One-vs-Rest (OvR)} 	$\rightarrow$ One-against-All
				\item \highlight{One-vs-One (OvO)} 	$\rightarrow$ Pairwise classification
			\end{itemize}
		\end{enumerate}
		\item Let's examine these approaches a bit closer
	\end{itemize}
\end{frame}


% Subsection: Multinomial Logistic Regression
% --------------------------------------------------------------------------------------------------------
\subsection{Multinomial Logistic Regression}

% Multinomial Logistic Regression Introduction
\begin{frame}{Multinomial Logistic Regression Introduction}{}\optional
	\begin{itemize}
		\item The logistic regression model has to be changed in order to deal with multiple classes
		\item The sigmoid function is replaced by the \highlight{Softmax} function:
		\begin{equation}
			\bm{g} : \mathbb{R}^{\kappa} \rightarrow \mathbb{R}^{\kappa}, \qquad \bm{z} \mapsto \bm{g}(\bm{z}), \qquad
			g_k(\bm{z}) = \frac{e^{z_k}}{\sum_{n=1}^{\kappa} e^{z_n}}
		\end{equation}
		\item $\kappa$ is the number of possible outcomes / classes
		\item The softmax function returns a \textbf{probability distribution over the possible outcomes}, i.\,e. $\sum_{k=1}^{\kappa} g_k(\bm{z}) = 1$
	\end{itemize}
\end{frame}


% Multinomial Logistic Regression Introduction (Ctd.)
\begin{frame}{Multinomial Logistic Regression Introduction (Ctd.)}{}\optional
	\begin{itemize}
		\item $\bm{z} = \begin{pmatrix}
			\bm{\theta}_{1}^{\intercal} \bm{x} &
			\bm{\theta}_{2}^{\intercal} \bm{x} &
			\hdots &
			\bm{\theta}_{\kappa}^{\intercal} \bm{x}
		\end{pmatrix}^{\intercal}$ is the vector of \highlight{logits}
		\item This means we learn a separate set of parameters $\bm{\theta}_k \in \mathbb{R}^{m+1}$ for each possible class
		\item All parameter vectors $\bm{\theta}_k$ are stacked into a single matrix $\bm{\Theta}$:
		\begin{equation}
			\bm{\Theta} = \begin{pmatrix}
				\vertbar 			& \vertbar			& \hdots 			& \vertbar 				\\
				\bm{\theta}_1 		& \bm{\theta}_2 	& \hdots 			& \bm{\theta}_{\kappa}	\\
				\vertbar 			& \vertbar			& \hdots 			& \vertbar
			\end{pmatrix} \in \mathbb{R}^{(m+1) \times \kappa}
		\end{equation}
	\end{itemize}
\end{frame}


% Generalized Cross Entropy Cost Function
\begin{frame}{Generalized Cross Entropy Cost Function}{}\optional
	\begin{itemize}
		\item We have to generalize the cross entropy cost function as well:
		\begin{equation}
			\mathcal{J}(\bm{\Theta}) = -\sum_{k=1}^{\kappa} y_k \log(g_k(\bm{z})) \qquad \text{with}\ \bm{z} =
			\begin{pmatrix}
				\bm{\theta}_{1}^{\intercal} \bm{x} \\
				\vdots \\
				\bm{\theta}_{\kappa}^{\intercal} \bm{x}
			\end{pmatrix}
		\end{equation}
		\item This definition of the cross entropy function requires the label to be encoded as a \highlight{one-hot vector}, i.\,e. each label is now a vector:
		\begin{equation}
			\bm{y} = \begin{pmatrix} 0 & \hdots & 0 & 1 & 0 & \hdots & 0\end{pmatrix}^{\intercal} \in \{ 0, 1 \}^{\kappa}
		\end{equation}
	\end{itemize}
\end{frame}


% Derivative of the Cross Entropy Function
\begin{frame}{Derivative of the Cross Entropy Function}{}\optional
	\footnotesize
	\begin{alignat*}{2}
		\mathcal{J}(\bm{\Theta})
			&= 	-\sum_{k=1}^{\kappa} y_k \log(g_k(\bm{z})) \qquad \text{with}\ \bm{z} =
				\begin{pmatrix}
					\bm{\theta}_{1}^{\intercal} \bm{x} \\
					\bm{\theta}_{2}^{\intercal} \bm{x} \\
					\vdots \\
					\bm{\theta}_{\kappa}^{\intercal} \bm{x}
				\end{pmatrix} 																				\\[4mm]
		\frac{\partial}{\partial \theta_{ij}} \mathcal{J}(\bm{\Theta})
			&= 	-\sum_{k=1}^{\kappa} y_k \frac{\partial \log(g_k(\bm{z}))}{\partial g_k(\bm{z})} \cdot
				\frac{\partial g_k(\bm{z})}{\partial z_i} \cdot \frac{\partial z_i}{\partial \theta_{ij}}
			&&	\qquad\longrightarrow \text{chain rule} 															\\[2mm]
			&= 	-\sum_{k=1}^{\kappa} y_k \frac{1}{g_k(\bm{z})} \cdot \frac{\partial g_k(\bm{z})}{\partial z_i} \cdot
				\frac{\partial z_i}{\partial \theta_{ij}}
			&&	\qquad\longrightarrow \frac{\text{d}}{\text{d}x} \log(x) = \frac{1}{x}
	\end{alignat*}
\end{frame}


% Derivative of the Softmax Function
\begin{frame}{Derivative of the Softmax Function}{}\optional
	\footnotesize
	\begin{equation*}
		g_k(\bm{z}) = \frac{e^{z_k}}{\sum_{n=1}^{\kappa} e^{z_n}} \qquad\qquad
		\frac{\partial}{\partial z_i} g_k(\bm{z}) = \begin{cases}
			g_i(\bm{z}) (1 - g_i(\bm{z})) 	& \text{if}\ i = k \\
			-g_k(\bm{z}) g_i(\bm{z}) 		& \text{if}\ i \ne k
		\end{cases}
	\end{equation*}
	\vspace*{2mm}
	
	\divideTwoTop{0.49}{
		\highlight{Case \ding{182}:} $i = k$
		\begin{align*}
			\frac{\partial}{\partial z_i} g_k(\bm{z})
				&= 	\frac{e^{z_k} \sum_{n=1}^{\kappa} e^{z_n} - e^{z_k} e^{z_i}}{(\sum_{n=1}^{\kappa} e^{z_n})^2} \\[2mm]
				&= 	\frac{e^{z_k}}{\sum_{n=1}^{\kappa} e^{z_n}} \left[ 1 - \frac{e^{z_i}}{\sum_{n=1}^{\kappa} e^{z_n}} \right] 	\\[2mm]
				&= 	g_k(\bm{z}) (1 - g_i(\bm{z})) \\[2mm]
				&= 	g_k(\bm{z}) (1 - g_k(\bm{z}))
		\end{align*}
	}{0.49}{
		\highlight{Case \ding{183}:} $i \ne k$:
		\begin{align*}
			\frac{\partial}{\partial z_i} g_k(\bm{z})
				&= 	\frac{0 \cdot \sum_{n=1}^{\kappa} e^{z_n} - e^{z_k} e^{z_i}}{(\sum_{n=1}^{\kappa} e^{z_n})^2} 		\\[2mm]
				&=	-\frac{e^{z_k}}{\sum_{n=1}^{\kappa} e^{z_n}} \frac{e^{z_i}}{\sum_{n=1}^{\kappa} e^{z_n}} \\[2mm]
				&= 	-g_k(\bm{z}) g_i(\bm{z})
		\end{align*}
	}
\end{frame}


% Derivative of the Cross Entropy Function (Ctd.)
\begin{frame}{Derivative of the Cross Entropy Function (Ctd.)}{}\optional
	\footnotesize
	\begin{alignat*}{2}
		\frac{\partial}{\partial \theta_{ij}} \mathcal{J}(\bm{\Theta})	
			&= 	-\sum_{k=1}^{\kappa} \frac{y_k}{g_k(\bm{z})} \cdot \frac{\partial g_k(\bm{z})}{\partial z_i} \cdot \frac{\partial z_i}{\partial \theta_{ij}}
			&&	\quad\longrightarrow \text{see slide 33}															\\[2mm]
			&= 	\left[ -\frac{y_k}{g_k(\bm{z})} g_k(\bm{z}) (1 - g_k(\bm{z})) +
				\sum_{\substack{k=1 \\ k \ne i}}^{\kappa} \frac{y_k}{g_k(\bm{z})} g_k(\bm{z}) g_i(\bm{z}) \right] 
				\frac{\partial z_i}{\partial \theta_{ij}}
			&&	\quad\longrightarrow \text{separate cases} 															\\[2mm]
			&= 	\left[ -y_k + y_k g_k(\bm{z}) + \sum_{\substack{k=1 \\ k \ne i}}^{\kappa} y_k g_i(\bm{z}) \right]
				\frac{\partial z_i}{\partial \theta_{ij}} = 
				\left[ -y_k + \sum_{k=1}^{\kappa}  y_k g_i(\bm{z}) \right] \frac{\partial z_i}{\partial \theta_{ij}} 					
			&&	\quad\longrightarrow \text{cancel terms}\\[2mm]
			&=	(-y_k + g_k(\bm{z})) x_j = \boxed{(g_k(\bm{z}) - y_k) x_j}
	\end{alignat*}
\end{frame}


% Subsection: One-vs-Rest (OvR)
% --------------------------------------------------------------------------------------------------------
\subsection{One-vs-Rest (OvR)}

% Transforming the Problem into several binary Problems
\begin{frame}{Transforming the Problem into several binary Problems}{}\important
	\begin{itemize}
		\item Instead of adjusting the algorithm we can also \textbf{transform the multi-class problem into several binary problems}
		\item Two common techniques are:
		\begin{itemize}
			\item \highlight{One-vs-Rest (OvR)} 	$\rightarrow$ One-against-All
			\item \highlight{One-vs-One (OvO)} 	$\rightarrow$ Pairwise classification
		\end{itemize}
		\item \textbf{General idea:}
		\begin{itemize}
			\item Several classifiers are trained individually
			\item During prediction the classifiers \textbf{vote for the correct class}
		\end{itemize}
		\item Such techniques can be used \textbf{for all binary classifiers}
	\end{itemize}
\end{frame}


% Multi-Class Classification: One-vs-Rest (OvR)
\begin{frame}{Multi-Class Classification: One-vs-Rest (OvR)}{}
	\divideTwo{0.49}{
		\begin{itemize}
			\item \textbf{Train one classifier per class} (expert for that class)
			\item We get $\vert \mathcal{C} \vert$ classifiers
			\item The $k$-th classifier learns to distinguish the $k$-th class from all the others
			\item Set the labels of examples from class $k$ to \textbf{1}, all the others to \textbf{0}
		\end{itemize}
	}{0.49}{
		\vspace*{2mm}
		\input{07_logistic_regression/01_tikz/ovr}
	}
\end{frame}


% Subsection: One-vs-One (OvO)
% --------------------------------------------------------------------------------------------------------
\subsection{One-vs-One (OvO)}

% Multi-Class Classification: One-vs-One (OvO)
\begin{frame}{Multi-Class Classification: One-vs-One (OvO)}{}
	\divideTwo{0.49}{
		\vspace*{2mm}
		\input{07_logistic_regression/01_tikz/ovo}
	}{0.49}{
		\begin{itemize}
			\item \textbf{Train one classifier for each pair of classes}
			\item We get $\binom{\vert \mathcal{C} \vert}{2}$ classifiers
			\item Ignore all other examples that do not belong to either of the two classes
			\item \textbf{Voting}: Count how often each class wins; the class with the highest score is predicted
		\end{itemize}
	}
\end{frame}


% Section: Wrap-Up
%______________________________________________________________________
\section{Wrap-Up}
\makedivider{Wrap-Up}

% Subsection: Summary
% --------------------------------------------------------------------------------------------------------
\subsection{Summary}

% Summary
\begin{frame}{Summary}{}
	\begin{itemize}
		\item \Highlight{Logistic regression is used for classification (!!!)}
		\item It is used for \textbf{binary classification problems} (generalizations exist)
		\item \textbf{Output:} Probability of instance belonging to positive class
		\item Apply a \textbf{threshold} to get the classification
		\item The algorithm minimizes the \textbf{cross entropy cost function}
		\item There is \textbf{no closed-form solution} (unlike for linear regression)
		\item \textbf{Basis functions} can be used for non-linear data
		\item \textbf{Multi-class classification:} One-vs-Rest, One-vs-One
	\end{itemize}
\end{frame}


% Subsection: Self-Test Questions
% --------------------------------------------------------------------------------------------------------
\subsection{Self-Test Questions}

% Self-Test Questions
\begin{frame}{Self-Test Questions}{}\important
	\begin{enumerate}
		\item Why should you not use linear regression for classification?
		\item State the formula for the logistic function.
		\item Why do we use cross entropy instead of the squared error?
		\item Does logistic regression find the best-separating hyper-plane?
		\item What techniques do you know for multi-class classification problems?
	\end{enumerate}
\end{frame}


% Subsection: Lecture Outlook
% --------------------------------------------------------------------------------------------------------
\subsection{Lecture Outlook}

\begin{frame}{What's next...?}{}
	\makeoverview{6}
\end{frame}


% Subsection: Recommended Literature and further Reading
% --------------------------------------------------------------------------------------------------------
\subsection{Recommended Literature and further Reading}

% Literature
%______________________________________________________________________
\begin{frame}[allowframebreaks]{Recommended Literature and further Reading}{}
	\footnotesize
	\begin{thebibliography}{2}
		\literature{book}{Bishop.2006}{[1] Pattern Recognition and Machine Learning}
			{Christopher Bishop. Springer. 2006.}{$\rightarrow$ \href{
				http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop\%20-\%20Pattern\%20Recognition\%20And\%20Machine\%20Learning\%20-\%20Springer\%20\%202006.pdf
			}{\linkstyle{Link}}, cf. chapter 4.3.2}

		\literature{book}{Murphy.2012}{[2] Machine Learning: A Probabilistic Perspective}
			{Kevin Murphy. MIT Press. 2012.}{$\rightarrow$ \href{
				https://doc.lagout.org/science/Artificial\%20Intelligence/Machine\%20learning/Machine\%20Learning_\%20A\%20Probabilistic\%20Perspective\%20\%5BMurphy\%202012-08-24\%5D.pdf
			}{\linkstyle{Link}}, cf. chapter 8}
	\end{thebibliography}
\end{frame}


% Subsection: Meme of the Day
% --------------------------------------------------------------------------------------------------------
\subsection{Meme of the Day}

% Meme of the Day
\begin{frame}{Meme of the Day}{}
	\begin{figure}
		\includegraphics[scale=0.45]{07_logistic_regression/02_img/meme_of_the_day}
	\end{figure}
\end{frame}


% Thank you
%______________________________________________________________________
\makethanks

\end{document}