% select theme
\input{../preamble_theme_2}

% ====================================================
% ====================================================
% OPTIONS
% ====================================================
% ====================================================

% number of levels in toc
\dwDeepToc{false}

% ====================================================
% ====================================================
% PRESENTATION DATA
% ====================================================
% ====================================================

\title[Association Rule Learning]{***** Advanced Machine Learning ***** Association Rule Learning}
\author{M.\,Sc. Daniel Wehner}
\date{Summer term 2020}
\institute{SAP\,SE / DHBW Mannheim}

% ====================================================
% ====================================================
% BEGIN OF DOCUMENT
% ====================================================
% ====================================================

\begin{document}

% Title frame
%______________________________________________________________________
\dwPrintTitle

% Agenda
%______________________________________________________________________
\dwPrintToc

% Section: Introduction
%______________________________________________________________________
\dwSection{Introduction}

% What is Association Rule Mining?
\begin{dwHeaderFrame}{What is Association Rule Mining?}
	\begin{itemize}
		\item Association rule mining belongs to the category of unsupervised learning.
		\item Association rules describe frequent co-occurrences in the data (\textbf{not necessarily causality!})
		\item Examples:
		\begin{itemize}
			\item Market basket analysis (\textit{Which products are frequently bought together? E.\,g. Amazon})
			\item Course schedule planning (\textit{Which courses are often attended together?})
			\item Other use cases: Marketing promotions, inventory management, customer relationship management (CRM) 
		\end{itemize}
		\item The general form of a rule is given by:
		\begin{equation}
			\overbracket{\{a_1, a_2, \dots, a_n\}}^{\text{\textbf{Antecedent}}} 
			\rightarrow 
			\overbracket{\{b_1, b_2, \dots, b_m\}}^{\text{\textbf{Consequent}}}
		\end{equation}
		\item Example: $\{ bread, cheese \} \rightarrow \{ wine \}$
	\end{itemize}
\end{dwHeaderFrame}


\begin{frame}
	\dwFigure{\includegraphics[scale=0.3]{16_association_rules/02_img/amazon}}{Famous example from Amazon}{fig:amazon}
\end{frame}


% Important Terminology
\begin{dwHeaderFrame}{Important Terminology}
	\begin{itemize}
		\item Suppose $\mathcal{I}$ is a set of unique items which we have in our portfolio
			$\mathcal{T} = \{ t_1, t_2, \dots, t_n \}$ is a list of transactions (what customers bought).
		\item Each transaction $t_i \in \mathcal{T}$ is an element of $\mathfrak{P}(\mathcal{I})$, the power set of $\mathcal{I}$. (\textbf{What is a power set?})
		\item Example:
	\end{itemize}
	\dwFigure{
		\begin{minipage}{0.45\textwidth}
			\input{16_association_rules/03_tbl/list_of_transactions}
		\end{minipage}
		\hfill
		\begin{minipage}{0.45\textwidth}
			\input{16_association_rules/03_tbl/list_of_transactions_binary}
		\end{minipage}
	}{Left: List of transactions (raw), right: List of transactions in binary form}{}
	\dwInfoBox{Simplification: We ignore quantities and prices of the items sold.}
\end{dwHeaderFrame}


\begin{frame}
	\dwHeader{Item sets}
	\begin{itemize}
		\item A collection of $k$ items is called $k$-item set.
		\item Example: $\{ pizza, wine \}$ is a 2-item set.
		\item The number of items contained in a transaction $t_i$ is sometimes referred to as the \textbf{transaction width} $w(t_i) = \vert t_i \vert$.
		\item An important property of an item set $X$ is the \textbf{support count} $\bm{\sigma}$:
		\begin{equation}
			\sigma(X) = \vert \{ t_i \vert X \subseteq t_i \wedge t_i \in \mathcal{T} \} \vert
		\end{equation}
		\item \textbf{What does the support count tell us?} $\sigma(X)$ refers to the number of transactions $X$ occurs in.
	\end{itemize}
\end{frame}


% Quality Measures
\begin{dwHeaderFrame}{Quality Measures}
	\begin{itemize}
		\item \textit{Question:} How to measure the quality of an association rule?
		\item \textbf{Support:}
		\begin{itemize}
			\item Proportion of examples for which head and body are true.
			\item Example $A \rightarrow B$: How many customers bought $A$ and $B$ together?
			\begin{equation}
				\text{support}(A \rightarrow B) = \text{support}(A \cup B) = \frac{\sigma(A \cup B)}{n}
			\end{equation}
		\end{itemize}
		\item \textbf{Confidence:}
		\begin{itemize}
			\item Proportion of examples for which the head is true among those for which the body is true.
			\item Example: If customers bought $A$, how likely are they to also buy $B$?
			\begin{equation}
				\text{confidence}(A \rightarrow B) = \frac{\text{support}(A \cup B)}{\text{support}(A)} = \frac{\sigma(A \cup B)}{\sigma(A)}
			\end{equation}
		\end{itemize}
	\end{itemize}
\end{dwHeaderFrame}


\begin{frame}
	\begin{itemize}
		\item Support: There is a huge number of possible rules, but not all of them are interesting. \\
			$\Rightarrow$ \textbf{Prune (remove) rules with low support.}
		\item Confidence: The higher the confidence the more reliable is the rule.
		\item Example:
		\begin{itemize}
			\item $R = \{ bread, cheese \} \rightarrow \{ wine \}$
			\item $\text{support}(R) = 0.01$ and $\text{confidence}(R) = 0.8$
			\item 80\,\% of all customers who bought bread and cheese also bought red wine.
			\item However, only 1\,\% of the customers bought all three items together.
		\end{itemize}
	\end{itemize}
\end{frame}


% Section: Apriori
%______________________________________________________________________
\dwSection{Apriori}

% Learning Problem
\begin{dwHeaderFrame}{Learning Problem}
	\begin{itemize}
		\item The \textbf{Apriori algorithm} can be used to find association rules.
		\item The learning problem can be summarized as follows:
		\begin{quote}
			Given a set of transactions $\mathcal{T}$, find all rules having $\text{support} \ge s_{min}$ and $\text{confidence} \ge c_{min}$,
			where $s_{min}$ and $c_{min}$ are thresholds.
		\end{quote}
		\item Obviously, mining all possible rules is super expensive.
		\begin{equation}
			\vert \text{rules} \vert = 3^d - 2^{d+1} + 1 \qquad\text{where}\qquad d \equiv \vert \mathcal{I} \vert
		\end{equation}
		\item Also, rules can be spurious (i.\,e. patterns may occur by chance and are not systematic).
	\end{itemize}
	\dwAlertBox{We have to avoid considering all possible rules! $\Rightarrow$ Employ early pruning.}
\end{dwHeaderFrame}


% Early Pruning
\begin{dwHeaderFrame}{Early Pruning}
	\begin{itemize}
		\item The goal is to generate rules which have high support and high confidence.
		\item Observation: If an item set is infrequent (does not have sufficient support), calculating the confidence can be omitted.
		\item As a consequence, all rules which can be generated from this item set do not have to be considered anymore.
		\item Example for the item set $A = \{ beer, diapers, milk \}$:
		\begin{itemize}
			\item The rules derived from item set $A$ are given below.
			\item If we know item set $A$ to be infrequent, we can prune all these rules.
			\item There is no need to calculate the confidence for these rules (\textbf{decoupling of support and confidence}).
		\end{itemize}
	\end{itemize}
	\divideTwo{0.49}{
		\begin{align*}
			\{ beer, diapers \} 	&\rightarrow \{ milk \} 			\\
			\{ diapers, milk \}	&\rightarrow \{ beer \}		\\
			\{ milk \} 			&\rightarrow \{ beer, diapers \}	
		\end{align*}
	}{0.49}{
		\begin{align*}
			\{ beer, milk \} 		&\rightarrow \{ diapers \} 		\\
			\{ beer \}			&\rightarrow \{ diapers, milk \}	\\
			\{ diapers \}		&\rightarrow \{ beer, milk \}
		\end{align*}
	}
\end{dwHeaderFrame}


% Apriori Algorithm
\begin{dwHeaderFrame}{Apriori Algorithm}
	\begin{itemize}
		\item The overall algorithm consists of two major steps:
		\begin{enumerate}
			\item \textbf{Frequent item set generation:} \\
				Find all item sets which have sufficient support (satisfy the support constraint).
			\item \textbf{Rule generation:} \\
				Extract highly confident rules which satisfy the confidence constraint.
		\end{enumerate}
		\item In the following we will have a closer look at these two steps.
	\end{itemize}
\end{dwHeaderFrame}


\begin{frame}
	\dwHeader{Step 1) Frequent item set generation}
	\begin{itemize}
		\item It is possible to enumerate all possible item sets with a lattice \cref{fig:lattice}.
		\item A brute force approach could calculate the support for each candidate set and rank them by the result.
		\item \textbf{Problem:} The number of candidate sets grows exponentially with $\vert \mathcal{I} \vert$: $2^{\vert \mathcal{I} \vert} - 1$ (excluding empty set).
		\item Example: For $\mathcal{I} = \{ a, b, c, d, e \}$, we have 31 possible candidates.
		\item Therefore, the candidate sets should be generated more efficiently.
		\item We can make use of the \textbf{anti-monotonicity} of the support:
		\begin{quote}
			If an item set is frequent, then all of its subsets must be frequent as well. Also, if an item set is infrequent, then all its supersets must be infrequent too.
		\end{quote}
		\item Adding a condition can never increase the support of a rule:
		\begin{equation}
			A \subseteq B \Longrightarrow \text{support}(A) \ge \text{support}(B)
		\end{equation}
		\item \textbf{An item set can only be frequent, if all its subsets are frequent and all supersets of an infrequent item set are also infrequent.}
	\end{itemize}
\end{frame}


\begin{frame}
	\dwFigure{
		\begin{minipage}{0.30\textwidth}
			\includegraphics[scale=0.21]{16_association_rules/02_img/lattice1}
			\begin{center}(a) item set lattice\end{center}
		\end{minipage}
		\hfill
		\begin{minipage}{0.30\textwidth}
			\includegraphics[scale=0.21]{16_association_rules/02_img/lattice2}
			\begin{center}(b) frequent item set\end{center}
		\end{minipage}
		\hfill
		\begin{minipage}{0.30\textwidth}
			\includegraphics[scale=0.21]{16_association_rules/02_img/lattice3}
			\begin{center}(c) infrequent item set\end{center}
		\end{minipage}
	}{Item set lattice for $\mathcal{I} = \{ a, b, c, d, e \}$}{fig:lattice}
\end{frame}


\begin{frame}
	\begin{enumerate}
		\item $k \leftarrow 1$
		\item $C_1 \leftarrow \mathcal{I}$
		\item \textbf{while} $C_k \ne \emptyset$ \textbf{do}
		\begin{itemize}
			\item[$\triangleright$] $S_k \leftarrow C_k \backslash \{ \text{all infrequent item sets in $C_k$} \}$
			\item[$\triangleright$] $C_{k+1} \leftarrow \text{all sets with $k + 1$ elements which can be formed by uniting two item sets in $S_k$}$
			\item[$\triangleright$] $C_{k+1} \leftarrow C_{k+1} \backslash \{ \text{item sets, where not all subsets of size $k$ are in $S_k$} \}$
			\item[$\triangleright$] $S \leftarrow S \cup S_k$
			\item[$\triangleright$] $k \leftarrow k + 1$
		\end{itemize}
		\item \textbf{return} $S$
	\end{enumerate}

	\dwAlertBox{The algorithm leaves it open how the candidate set $C_{k+1}$ is generated. How can this be done efficiently?}
\end{frame}


\begin{frame}
	\begin{itemize}
		\item Requirements for efficient candidate generation:
		\begin{itemize}
			\item We have to avoid producing too many candidates.
			\item At the same time we have to ensure that all frequent item sets are found (\textbf{completeness})
			\item We don't want to produce duplicates (\textbf{efficiency})
		\end{itemize}
		\item The Apriori algorithm uses the following method:
		\begin{itemize}
			\item Merge a pair of $k$-item sets only if their first $k - 1$ items are identical.
			\begin{equation}
				A = \{ a_1, a_2, \dots, a_{k} \} \qquad\qquad\qquad B = \{ b_1, b_2, \dots, b_{k} \}
			\end{equation}
			\item Merge $A$ and $B$, if $a_j = b_j\ (j = 1, 2, \dots, k - 1) \wedge a_{k} \ne b_{k}$
			\item Example:
			\begin{itemize}
				\item $A = \{ bread, milk, pizza \}, B = \{ bread, milk, wine \}$
				\item $A$ and $B$ are merged into $\{ bread, milk, pizza, wine \}$.
			\end{itemize}
			\item This method still requires pruning non-frequent item sets.
			\item \textbf{Important: The item sets have to be in lexicographic order.}
		\end{itemize}
	\end{itemize}
\end{frame}


\begin{frame}
	Let's calculate the frequent item sets from the introductory example ($s_{min} = 0.25$):
	\input{16_association_rules/03_tbl/list_of_transactions_binary}
	\begin{align*}
		C_1 	&= \{ \{ beer \}, \{ chips \}, \{ pizza \}, \{ wine \} \} \\
		S_1 	&= \{ \{ beer \}, \{ chips \}, \{ pizza \}, \{ wine \} \} \\
		C_2 	&= \{ \{ beer, chips \}, \{ beer, pizza \}, \{ beer, wine \}, \{ chips, pizza \}, \{ chips, wine \}, \{ pizza, wine \} \} \\
		S_2 	&= \{ \{ beer, chips \}, \{ beer, wine \}, \{ chips, pizza \}, \{ chips, wine \}, \{ pizza, wine \} \} \\
		C_3 	&= \{ \{ beer, chips, wine \}, \{ chips, pizza, wine \} \} \\
		S_3 	&= \{ \{ beer, chips, wine \} \} \\
		C_4 	&= \emptyset \\
		S 	&= \bigcup_{k=1}^3 S_k
	\end{align*}
\end{frame}


\begin{frame}
	\begin{itemize}
		\item The search space for frequent item sets can be structured using the subset relationship.
		\item \textbf{Border:}
		\begin{itemize}
			\item The border \cref{fig:border} consists of all item sets for which...
			\begin{itemize}
				\item ...all subsets are frequent and...
				\item ...no superset is frequent.
			\end{itemize}
			\item \textbf{Positive border:} Elements of the border which are frequent.
			\item \textbf{Negative border:} Elements of the border which are infrequent.
		\end{itemize}
	\end{itemize}
	
	\dwAlertBox{Frequent item sets = positive border plus all subsets of border elements}
\end{frame}


\begin{frame}
	\dwFigure{\input{16_association_rules/01_tikz/border}}{Border for the example above}{fig:border}
\end{frame}


\begin{frame}
	\dwHeader{Step 2) Generation of association rules}
	\begin{itemize}
		\item The frequent item sets can now be used to generate association rules.
		\item For each frequent $k$-item set $X$, there are $2^k - 2$ possible association rules (without $X \rightarrow \emptyset$ and $\emptyset \rightarrow X$) of the general form \cref{fig:rules}:
		\begin{equation}
			A \rightarrow B \qquad\text{with}\qquad A \cup B = X \wedge A \cap B = \emptyset 
		\end{equation}
		\item Calculate the confidence for the rules and check whether they fulfill the confidence constraint.
		\item We can also define \textbf{anti-monotonicity} for the confidence:
		\begin{quote}
			If a rule is not confident, moving conditions from body to head results in rules which are also not confident.
		\end{quote}
		\begin{equation}
			\text{confidence}(A \rightarrow B \cup C) \le \text{confidence}(A \cup B \rightarrow C)
		\end{equation}
		\item This circumstance can again be used for pruning the search space!
	\end{itemize}
\end{frame}


\begin{frame}
	\dwFigure{\input{16_association_rules/01_tikz/generated_rules}}{Search space for association rules (frequent item set $\{ beer, chips, wine \}$)}{fig:rules}
\end{frame}


\begin{frame}
	Let's make a full example for the Apriori algorithm ($s_{min} = 0.5, c_{min} = 1.0$):
	\input{16_association_rules/03_tbl/full_example}
	\begin{align*}
		C_1 	&= \{ \{ bread \}, \{ butter \}, \{ coffee \}, \{ milk \}, \{ sugar \} \} \\
		S_1 	&= \{ \{ bread \}, \{ coffee \}, \{ milk \}, \{ sugar \} \} \\
		C_2 	&= \{ \{ bread, coffee \}, \{ bread, milk \}, \{ bread, sugar \}, \{ coffee, milk \}, \{ coffee, sugar \}, \{ milk, sugar \} \} \\
		S_2 	&= \{ \{ bread, sugar \}, \{ coffee, milk \}, \{ coffee, sugar \}, \{ milk, sugar \} \} \\
		C_3 	&= \{ \{ coffee, milk, sugar \} \} \\
		S_3 	&= \{ \{ coffee, milk, sugar \} \} \\
		C_4 	&= \emptyset \\
		S 	&= \bigcup_{k=1}^3 S_k
	\end{align*}
\end{frame}


\begin{frame}
	\begin{itemize}
		\item Rules with $c_{min} = 1.0$:
		\begin{tabbing}
			\hspace*{5cm}\=\hspace*{3cm}\=\kill
			$\{ bread \} \rightarrow \{ sugar \}$		\>	$s = 0.50$		\>	$c = 1.00$ 	\\[1.5mm]
			$\{ milk \} \rightarrow \{ coffee \}$		\>	$s = 0.75$		\>	$c = 1.00$ 	\\[1.5mm]
			$\{ coffee \} \rightarrow \{ milk \}$ 		\>	$s = 0.75$		\>	$c = 1.00$		\\[1.5mm]
			$\{ milk, sugar \} \rightarrow \{ coffee \}$	\>	$s = 0.50$		\>	$c = 1.00$		\\[1.5mm]
			$\{ sugar, coffee \} \rightarrow \{ milk \}$	\>	$s = 0.50$		\>	$c = 1.00$
		\end{tabbing}
		\item Other rules are either not frequent enough and are filtered out in step 1; \\
			e.\,g. $\{ butter \} \rightarrow \{ bread, sugar \}$, for which $s = 0.25$ and $c = 1.0$...
		\item ...or not confident enough and filtered out in step 2; \\
			e.\,g. $\{ milk, coffee \} \rightarrow \{ sugar \}$, for which $s = 0.5$ and $c = 0.67$.
	\end{itemize}
\end{frame}


% Section: Miscellaneous
%______________________________________________________________________
\dwSection{Miscellaneous}

% Interestingness
\begin{dwHeaderFrame}{Interestingness}
	\begin{itemize}
		\item \textbf{Problem:} There might still be way too many rules.
		\item Assume the following two rules:
		\begin{equation}
			R_1 =  A \cup B \rightarrow C \qquad\qquad\qquad R_2 = A \rightarrow C
		\end{equation}
		\item Filter out $R_1$, if the rule is...
		\begin{itemize}
			\item ...\textit{trivial} ($R_2$ covers the same examples)
			\item ...\textit{unproductive} ($R_2$ has equal or higher confidence)
			\item ...\textit{insignificant} (Confidence of $R_2$ is not significantly worse)
		\end{itemize}
		\item Filter by \textbf{interestingness} (\textit{How can we measure interestingness?})
	\end{itemize}
\end{dwHeaderFrame}


\begin{frame}
	\begin{itemize}
		\item Support and confidence are not sufficient to capture whether a rule is interesting or not.
		\item A rule may have high support and confidence, but still may not be interesting.
		\item Example:
		\begin{itemize}
			\item Consider the rule: $\{ diapers \} \rightarrow \{ beer \}; c = 0.90$
			\item 90\,\% of all customers who buy diapers also buy beer.
			\item Sounds like and interesting association rule.
			\item \textbf{But:} If we know, that 90\,\% of all customers buy beer, this rule is not interesting anymore.
		\end{itemize}
	\end{itemize}
\end{frame}


% Lift, Leverage and Conviction
\begin{dwHeaderFrame}{Lift, Leverage and Conviction}
	\begin{itemize}
		\item Consider rule $R = A \rightarrow B$
		\item \textbf{Lift:} Rule $R$ is interesting, if $\text{lift}(R) \gg 1$.
		\begin{equation}
			\text{lift}(A \rightarrow B) = \frac{\text{support}(A \rightarrow B)}{\text{support}(A) \cdot \text{support}(B)}
		\end{equation}
		\item \textbf{Leverage:} Rule $R$ is interesting, if $\text{leverage}(R) \gg 0$.
		\begin{equation}
			\text{leverage}(A \rightarrow B) = \text{support}(A \rightarrow B) - \text{support}(A) \cdot \text{support}(B)
		\end{equation}
		\item \textbf{Conviction:} Expected ratio that $A$ occurs without $B$ (incorrect prediction of $R$).
		\begin{equation}
			\text{conviction}(A \rightarrow B) = \frac{1 - \text{support}(B)}{1 - \text{confidence}(A \rightarrow B)}
		\end{equation}
	\end{itemize}
\end{dwHeaderFrame}

\end{document}
