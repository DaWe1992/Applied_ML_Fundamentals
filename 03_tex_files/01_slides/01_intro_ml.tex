\input{../preamble_theme}

% ====================================================
% ====================================================
% PRESENTATION DATA
% ====================================================
% ====================================================

\title[Machine Learning Introduction]{*** Applied Machine Learning Fundamentals *** Machine Learning Introduction}
\institute[SAP\,SE]{SAP\,SE / DHBW Mannheim}
\author{Daniel Wehner, M.Sc.}
\date{Winter term 2022/2023}
\prefix{INTRO}

% ====================================================
% ====================================================
% BEGIN OF DOCUMENT
% ====================================================
% ====================================================

\begin{document}

% Title frame
%______________________________________________________________________
\maketitlepage


% Lecture Overview
%______________________________________________________________________
\begin{frame}{Lecture Overview}{}
	\makeoverview{1}
\end{frame}


% Agenda
%______________________________________________________________________
\begin{frame}{Agenda for this Unit}
	\begin{multicols}{2}
		\tableofcontents
	\end{multicols}
\end{frame}


% Section: Introduction
%______________________________________________________________________
\section{Introduction}
\makedivider{Introduction}

% Subsection: Motivation
% --------------------------------------------------------------------------------------------------------
\subsection{Motivation}

% Why Machine Learning?
\begin{frame}{Why Machine Learning?}{}
	\begin{itemize}
		\item \textit{`We are drowning in information and starving for knowledge.' \\
			\hfill\textbf{-- John Naisbitt}}
		\item \textbf{Era of big data:}
		\begin{itemize}
			\item In 2017 there are about \textbf{1.8 trillion} web-pages on the internet
			\item \textbf{20 hours} of video are uploaded to YouTube every minute
			\item Walmart handles more than \textbf{1 million} transactions per hour and has data bases containing more 
				than \textbf{2.5 peta-bytes} ($2.5 \times 10^{15}$) of information
		\end{itemize}
		\item \Highlight{No human being can deal with this data avalanche!}
	\end{itemize}
\end{frame}


% Why Machine Learning? (Ctd.)
\begin{frame}{Why Machine Learning? (Ctd.)}{}
	\textit{`I keep saying the sexy job in the next ten years will be \textbf{statisticians} and \textbf{machine learners}.
		People think I’m joking, but who would’ve guessed that computer engineers would’ve been the sexy job of the
		1990s? The ability to take data - to be able to understand it, to process it, to extract value from it, to visualize it,
		to communicate it - that’s going to be a hugely important skill in the next decades.' \\
		\hfill\textbf{-- Hal Varian}, Chief Economist at Google, 2009}
\end{frame}


% Subsection: Definition of Machine Learning
% --------------------------------------------------------------------------------------------------------
\subsection{Definition of Machine Learning}

% Definition of Machine Learning
\begin{frame}{Definition of Machine Learning}{}
	\begin{itemize}
		\item \textit{`[Machine Learning is the] field of study that gives computers the ability
			to learn without being explicitly programmed.' \\
			\hfill\textbf{-- Arthur Samuel}, 1959}
		\vspace*{5mm}
		\item \textit{`A computer program is said to learn from experience $E$ with respect to some class of 
			tasks $T$ and performance measure $P$, if its performance at tasks in $T$, as measured by $P$, improves with
			experience $E$.' \\
			\hfill\textbf{-- Tom Mitchell}, 1997}
	\end{itemize}
\end{frame}


% A more abstract Definition
\begin{frame}{A more abstract Definition}{}\important
	\begin{itemize}
		\item Our task is to learn a mapping from input to output: $h : \mathcal{I} \mapsto \mathcal{O}$
		\item Put differently, we want to predict the output from the input:
		\begin{equation*}
			y = h(\bm{x}; \bm{\theta}) \qquad\text{also:}\quad y = h_{\bm{\theta}}(\bm{x})
		\end{equation*}
		\begin{boxBlueNoFrame}
			\item $\bm{x} \in \mathcal{I}$ (Input)
			\item $y \in \mathcal{O}$ (Output)
			\item $\bm{\theta} \in \bm{\Theta}$ (Parameters: What needs to be `learned')
		\end{boxBlueNoFrame}
	\end{itemize}
\end{frame}


% General Parardigm
\begin{frame}{General Paradigm}{}\important
	\begin{figure}
		\centering
		\includegraphics[scale=0.25]{01_intro_ml/02_img/general_paradigm}
	\end{figure}
\end{frame}


% Section: Problem Types in Machine Learning
%______________________________________________________________________
\section{Problem Types in Machine Learning}
\makedivider{Problem Types in Machine Learning}

% Subsection: Type of Training Information
% --------------------------------------------------------------------------------------------------------
\subsection{Type of Training Information}

% Type of Training Information
\begin{frame}{Type of Training Information}{}
	\begin{itemize}
		\item \highlight{Supervised learning}
		\begin{itemize}
			\item `Teacher' provides \textbf{gold labels}
			\item E.\,g. neural networks, decision trees, linear regression
		\end{itemize}
		\item \highlight{Unsupervised learning}
		\begin{itemize}
			\item Labels are \textbf{not} known during training
			\item E.\,g. clustering, density estimation, association rule mining
		\end{itemize}
		\item \highlight{Reinforcement learning}
		\begin{itemize}
			\item Environment provides rewards for actions, but correct action is unknown
			\item E.\,g. policy-iteration, Q-learning, SARSA
		\end{itemize}
		\item \highlight{Semi-supervised learning} (partly labeled data)
	\end{itemize}
\end{frame}


% Type of Training Information (Ctd.)
\begin{frame}{Type of Training Information (Ctd.)}{}
	\input{01_intro_ml/01_tikz/type_of_training_information}
\end{frame}


% Supervised Learning
\begin{frame}{Supervised Learning}{}
	\divideTwo{0.49}{
		\begin{itemize}
			\small
			\item A single row is called \highlight{example}
			\item An example without class label is called \highlight{instance}
			\item \highlight{Predictors:}
			\begin{itemize}
				\scriptsize
				\item \texttt{Outlook} $\in \{sunny, overcast, rainy\}$
				\item \texttt{Temperature} $\in \{hot, mild, cool\}$
				\item \texttt{Humidity} $\in \{high, normal\}$
				\item \texttt{Wind} $\in \{weak, strong\}$
			\end{itemize}
			\item \highlight{Label:}
			\begin{itemize}
				\scriptsize
				\item \texttt{PlayGolf} $\in \{yes, no\}$
				\item Given a new instance, we want to predict its label
			\end{itemize}
			\item \textbf{Label for the new instance???}
		\end{itemize}
	}{0.49}{
		\vspace*{2mm}
		\input{01_intro_ml/03_tbl/example_data_set}
	}
\end{frame}


% Supervised Learning: General Approach
\begin{frame}{Supervised Learning: General Approach}{}
	\vspace*{-5mm}
	\input{01_intro_ml/01_tikz/supervised_learning_general_approach}
\end{frame}


% Unsupervised Learning
\begin{frame}{Unsupervised Learning}{}
	\divideTwo{0.49}{
		\vspace*{5mm}
		\input{01_intro_ml/01_tikz/unsupervised_learning}
	}{0.49}{
		\begin{itemize}
			\item There are \textbf{no} labels
			\item Try to find regularities in the data
			\item Examples for unsupervised learning:
			\begin{itemize}
				\item \highlight{Clustering}
				\item \highlight{Density estimation}
				\item \highlight{Dimensionality reduction}
			\end{itemize}
		\end{itemize}
	}
\end{frame}


% Subsection: Availability of Training Examples
% --------------------------------------------------------------------------------------------------------
\subsection{Availability of Training Examples}

% Availability of Training Examples
\begin{frame}{Availability of Training Examples}{}
	\begin{itemize}
		\item \highlight{Batch Learning}
		\begin{itemize}
			\item The learner is provided with a fixed set of training examples
			\item Cf. weather data set
			\item E.\,g. neural networks, decision trees
		\end{itemize}
		\item \highlight{Incremental / Online Learning}
		\begin{itemize}
			\item Constant stream of training examples
			\item The model is updated as new training examples arrive
			\item E.\,g. $k$-nearest-neighbors
		\end{itemize}
		\item Active Learning (\textit{not covered})
	\end{itemize}
\end{frame}


% Subsection: Type of Target Variable
% --------------------------------------------------------------------------------------------------------
\subsection{Type of Target Variable}

% Type of Target Variable: Regression
\begin{frame}{Type of Target Variable: Regression}{}
	\divideTwo{0.49}{
		\highlight{Regression}
		\begin{itemize}
			\item Learn a mapping into a continuous space
			\begin{itemize}
				\item $\mathcal{O} = \mathbb{R}$
				\item $\mathcal{O} = \mathbb{R}^3$
			\end{itemize}
			\item E.\,g. curve fitting, financial analysis, housing prices, ...
		\end{itemize}
	}{0.49}{
		\includegraphics[scale=0.5]{01_intro_ml/02_img/regression_example}
	}
\end{frame}


% Type of Target Variable: Classification
\begin{frame}{Type of Target Variable: Classification}{}
	\divideTwo{0.49}{
		\includegraphics[scale=0.25]{01_intro_ml/02_img/classification_example}
	}{0.49}{
		\highlight{Classification}
		\begin{itemize}
			\item Learn a mapping into a discrete space, e.\,g.
			\begin{itemize}
				\item $\mathcal{O} = \{0, 1\}$ (binary classification)
				\item $\mathcal{O} = \{0, 1, 2, 3, ...\}$
				\item $\mathcal{O} = \{verb, noun, adverb, ...\}$
			\end{itemize}
			\item Examples:
			\begin{itemize}
				\item Spam / no spam
				\item Digit recognition
				\item Part of speech tagging
			\end{itemize}
		\end{itemize}
	}
\end{frame}


% Section: Key Challenges in Machine Learning
%______________________________________________________________________
\section{Key Challenges in Machine Learning}
\makedivider{Key Challenges in Machine Learning}

% Subsection: Generalization from Training Data
% --------------------------------------------------------------------------------------------------------
\subsection{Generalization from Training Data}

% Generalization from Training Data
\begin{frame}{Generalization from Training Data}{}\important
	\begin{itemize}
		\item Learning does not mean memorizing the training data
		\item What if we see input that we \textbf{haven't seen before}?
		\item Example OCR (\textbf{O}ptical \textbf{C}haracter \textbf{R}ecognition):
		\begin{figure}
			\centering
			\includegraphics[scale=0.4]{01_intro_ml/02_img/mnist_digits}
		\end{figure}
		\vspace*{-4mm}
		\hfill{\scriptsize Hand-written digits from the MNIST data set}
		\begin{itemize}
			\item Predict the character given the input image
			\item \Highlight{People have different hand-writings}
		\end{itemize}
	\end{itemize}
\end{frame}


% Generalization from Training Data (Ctd.)
\begin{frame}{Generalization from Training Data (Ctd.)}{}\important
	\divideTwo{0.49}{
		\highlight{What is the problem here?}
		\begin{itemize}
			\item Complex decision boundary
			\item This leads to \Highlight{$\skull$ Overfitting $\skull$}
			\begin{itemize}
				\item The model is too expressive...
				\item ...and adapts to \textbf{idiosyncrasies} of the training data
			\end{itemize}
		\end{itemize}
	}{0.49}{
		\includegraphics[scale=0.3]{01_intro_ml/02_img/salmon_sea_bass_overfitting}
	}
	\vspace*{-3mm}
	\begin{boxBlueNoFrame}
		\textbf{Solution:} Choose a simpler model (cf. \textbf{Occam's razor})
	\end{boxBlueNoFrame}
\end{frame}


% Generalization from Training Data (Ctd.)
\begin{frame}{Generalization from Training Data (Ctd.)}{}\important
	\divideTwo{0.49}{
		\vspace*{1mm}
		\includegraphics[scale=0.3]{01_intro_ml/02_img/salmon_sea_bass}
	}{0.49}{
		\begin{itemize}
			\item Linear (less complex) model
			\item Allow for \textbf{misclassifications} of some training examples
			\item \textbf{Better generalization} to unseen instances
		\end{itemize}
	}
\end{frame}


% A prominent Example of Overfitting
\begin{frame}{A prominent Example of Overfitting}{}
	\begin{figure}
		\centering
		\includegraphics[scale=0.5]{01_intro_ml/02_img/tanks_overfitting}
	\end{figure}
\end{frame}


% Subsection: Feature Selection / Feature Engineering
% --------------------------------------------------------------------------------------------------------
\subsection{Feature Selection / Feature Engineering}

% Choosing the right Features
\begin{frame}{Choosing the right Features}{}
	\highlight{When stuck, move to a different perspective!}
	\vspace*{-2mm}
	\input{01_intro_ml/01_tikz/feature_mapping}
	\vspace*{-2mm}
	\begin{equation*}
		\varphi(x_1, x_2) \mapsto \left( x_1^2, \sqrt{2} x_1 x_2, x_2^2 \right) = (z_1, z_2, z_3)
	\end{equation*}
\end{frame}


% Curse of Dimensionality
\begin{frame}{Choosing the right Features}{}
	\highlight{But: Beware of the \highlight{curse of dimensionality}!}
	
	\begin{figure}
		\divideTwo{0.59}{
			\includegraphics[scale=0.2]{01_intro_ml/02_img/curse_of_dimensionality}
		}{0.39}{
			\begin{itemize}
				\item Too many features significantly \textbf{slow down} the ML algorithm
				\item Need \textbf{exponential} amount of training data
				\item Dimensionality reduction
			\end{itemize}
		}
	\end{figure}
	\citeAuthor{Image taken from}{Bishop.2006}{p.\,35}
\end{frame}


% Subsection: Performance Measurement
% --------------------------------------------------------------------------------------------------------
\subsection{Performance Measurement}

% Performance Measurement
\begin{frame}{Performance Measurement}{}
	\begin{itemize}
		\item How do we measure performance?
		\begin{itemize}
			\item 99\,\% correct classification in speech recognition: What does it really mean?
			\item \textit{We understand the meaning of the sentence?}, \textit{We understand every word?},
				\textit{For all speakers?}
		\end{itemize}
		\item We need more \textbf{concrete numbers}:
		\begin{itemize}
			\item \% of correctly classified letters
			\item Average distance driven (until accident, ...)
			\item \% of games won
			\item \% of correctly recognized words, sentences, etc.
		\end{itemize}
		\item \Highlight{Training vs. testing performance}
	\end{itemize}
\end{frame}


% Training vs. Testing Performance
\begin{frame}{Training vs. Testing Performance}{}
	\divideTwo{0.49}{
		\begin{itemize}
			\item Evaluate on data which was \textbf{not used} for training {\footnotesize \highlight{(out-of-sample)}}
			\item Two-way split: \\
				\texttt{Train - Test}
			\item Even better: \\
				\texttt{Train - Dev - Test}
			\vspace{2mm}
			\begin{tabbing}
				\hspace*{1.5cm}\= \kill
				\textbf{Train:}	\>	Train model 			\\
				\textbf{Dev:} 	\>	Tune hyper-parameters 	\\
				\textbf{Test:} 	\>	Test final model
			\end{tabbing}
		\end{itemize}
	}{0.49}{
		\centering
		\includegraphics[scale=0.4]{01_intro_ml/02_img/train_test_split_meme}
	}
\end{frame}


% Performance Measurement (Ctd.)
\begin{frame}{Performance Measurement (Ctd.)}{}
	\begin{itemize}
		\item We also need to define the right \highlight{error metric}:
	\end{itemize}
	\begin{figure}
		\centering
		\includegraphics[scale=0.5]{01_intro_ml/02_img/error_metric}
	\end{figure}
	\vspace*{-4mm}
	\begin{itemize}
		\item Which is better?
		\item Euclidean distance (L2-norm) might be useless
	\end{itemize}
\end{frame}


% Subsection: Model Selection
% --------------------------------------------------------------------------------------------------------
\subsection{Model Selection}

% Model Selection
\begin{frame}{Model Selection}{}
	\begin{itemize}
		\item What is the \textbf{right model}?
		\item The learned parameters (here: $\bm{w}$) can mean a lot of different things:
	\end{itemize}
	
	\begin{figure}
		\divideTwo{0.49}{
			\footnotesize
			\begin{itemize}
				\item May characterize a family of functions
				\item May be parameters of a probability distribution
				\item $\bm{w}$ may be a vector, adjacency matrix, graph, ...
			\end{itemize}
		}{0.49}{
			\begin{figure}
				\centering
				\includegraphics[scale=0.25]{01_intro_ml/02_img/model_selection}
			\end{figure}
		}
	\end{figure}
\end{frame}


% Subsection: Computation
% --------------------------------------------------------------------------------------------------------
\subsection{Computation}

% Computation
\begin{frame}{Computation}{}
	Even if the other problems are solved, \Highlight{computation is usually quite hard}:
	\begin{itemize}
		\item Learning involves optimization of parameters
		\item Find / Search for best model parameters
		\begin{itemize}
			\item GoogleNet has $\approx$ 6.5 million parameters
			\item Often GPUs (\textbf{G}raphics \textbf{P}rocessing \textbf{U}nit) are needed
			\item Google invented TPUs (\textbf{T}ensor \textbf{P}rocessing \textbf{U}nit)
		\end{itemize}
		\item Often we have to deal with thousands, millions, ... of training examples
		\item Given a model, the prediction has to be computed efficiently
	\end{itemize}
\end{frame}


% Section: Machine Learning Applications
%______________________________________________________________________
\section{Machine Learning Applications}
\makedivider{Machine Learning Applications}

% Subsection: Natural Language Processing
% --------------------------------------------------------------------------------------------------------
\subsection{Natural Language Processing}

% Natural Language Processing
\begin{frame}{Applications in Natural Language Processing}{}
	\begin{itemize}
		\item \textbf{E-mail filtering:}
		\begin{figure}
			\includegraphics[scale=0.5]{01_intro_ml/02_img/email_filtering}
		\end{figure}
		\item \textbf{Speech Recognition:}
		\begin{figure}
			\includegraphics[scale=0.5]{01_intro_ml/02_img/speech_recognition}
		\end{figure}
	\end{itemize}
\end{frame}


% Subsection: Computer Vision
% --------------------------------------------------------------------------------------------------------
\subsection{Computer Vision}

% Applications in Computer Vision
\begin{frame}{Applications in Computer Vision}{}
	\divideTwoTop{0.49}{
		\textbf{Face detection:}
		\begin{figure}
			\includegraphics[scale=0.35]{01_intro_ml/02_img/face_detection}
		\end{figure}
	}{0.49}{
		\textbf{Traffic sign detection:}
		\begin{figure}
			\includegraphics[scale=0.3]{01_intro_ml/02_img/traffic_sign_detection}
		\end{figure}
	}
\end{frame}


% Applications in Computer Vision (Ctd.)
\begin{frame}{Applications in Computer Vision (Ctd.)}{}
	\textbf{Optical character recognition:}
	\begin{figure}
		\includegraphics[scale=0.3]{01_intro_ml/02_img/digit_recognition}
	\end{figure}
	{\footnotesize Cf. \href{https://www.youtube.com/watch?v=yxuRnBEczUU}{\linkstyle{Demonstration LeNet}}}
\end{frame}


% Subsection: Robotics
% --------------------------------------------------------------------------------------------------------
\subsection{Robotics}

% Applications in Robotics
\begin{frame}{Applications in Robotics}{}
	\divideTwoTop{0.49}{
		\textbf{Robot control:}
		\vspace*{0.8mm}
		\begin{figure}
			\includegraphics[scale=0.1125]{01_intro_ml/02_img/robot_control}
		\end{figure}
	}{0.49}{
		\textbf{Autonomous driving:}
		\begin{figure}
			\includegraphics[scale=0.35]{01_intro_ml/02_img/autonomous_driving}
		\end{figure}
	}
\end{frame}


% Section: Wrap-Up
%______________________________________________________________________
\section{Wrap-Up}
\makedivider{Wrap-Up}

% Subsection: Summary
% --------------------------------------------------------------------------------------------------------
\subsection{Summary}

% Summary
\begin{frame}{Summary}{}
	\begin{itemize}
		\item Vast amounts of data are prohibitive for manual inspection
		\item Machine learning algorithms learn \textbf{without being explicitly programmed}
		\item \textbf{Important distinctions:}
		\begin{itemize}
			\item Supervised learning $\Leftrightarrow$ unsupervised learning
			\item Classification $\Leftrightarrow$ regression
		\end{itemize}
		\item \textbf{Challenges:}
		\begin{enumerate}
			\item Generalization
			\item Feature engineering
			\item Performance measurement
			\item Model selection
			\item Computation
		\end{enumerate}
	\end{itemize}
\end{frame}


% Subsection: Self-Test Questions
% --------------------------------------------------------------------------------------------------------
\subsection{Self-Test Questions}

% Self-Test Questions
\begin{frame}{Self-Test Questions}{}\important
	\begin{enumerate}
		\item What is the difference between supervised and unsupervised learning?
		\item What is regression?
		\item What does generalization mean?
		\item \textit{'The more features the better.'} Is this statement correct or not? Give reasons for your answer.
		\item Why do we need \texttt{train}, \texttt{dev} and \texttt{test} sets?
		\item True or false: 100\,\% train accuracy is desirable.
		\item State some applications of machine learning.
	\end{enumerate}
\end{frame}


% Subsection: Lecture Outlook
% --------------------------------------------------------------------------------------------------------
\subsection{Lecture Outlook}

\begin{frame}{What's next...?}{}
	\makeoverview{2}
\end{frame}


% Subsection: Recommended Literature and further Reading
% --------------------------------------------------------------------------------------------------------
\subsection{Recommended Literature and further Reading}

% Literature
%______________________________________________________________________
\begin{frame}[allowframebreaks]{Recommended Literature and further Reading}{}
	\footnotesize
	\begin{thebibliography}{2}
		\literature{book}{Mitchell.1997}{[1] Machine Learning}
			{Tom Mitchell. McGraw-Hill Science. 1997.}{$\rightarrow$ \href{
				https://www.cs.ubbcluj.ro/~gabis/ml/ml-books/McGrawHill\%20-\%20Machine\%20Learning\%20-Tom\%20Mitchell.pdf
			}{\linkstyle{Link}}, cf. chapter 1}

		\literature{book}{Bishop.2006}{[2] Pattern Recognition and Machine Learning}
			{Christopher Bishop. Springer. 2006.}{$\rightarrow$ \href{
				http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop\%20-\%20Pattern\%20Recognition\%20And\%20Machine\%20Learning\%20-\%20Springer\%20\%202006.pdf
			}{\linkstyle{Link}}, cf. chapter 1}
	\end{thebibliography}
\end{frame}


% Subsection: Meme of the Day
% --------------------------------------------------------------------------------------------------------
\subsection{Meme of the Day}

% Meme of the Day
\begin{frame}{Meme of the Day}{}
	\begin{figure}
		\includegraphics[scale=0.45]{01_intro_ml/02_img/meme_of_the_day}
	\end{figure}
\end{frame}


% Thank you
%______________________________________________________________________
\makethanks

\end{document}