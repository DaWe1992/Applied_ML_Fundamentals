% select theme
\input{../preamble_theme_2}

% ====================================================
% ====================================================
% OPTIONS
% ====================================================
% ====================================================

% number of levels in toc
\dwDeepToc{false}

% ====================================================
% ====================================================
% PRESENTATION DATA
% ====================================================
% ====================================================

\title[Advanced Regression Techniques]{***** Advanced Machine Learning ***** Advanced Regression Techniques}
\author{M.\,Sc. Daniel Wehner}
\date{Summer term 2020}
\institute{SAP\,SE / DHBW Mannheim}

% ====================================================
% ====================================================
% BEGIN OF DOCUMENT
% ====================================================
% ====================================================

\begin{document}

% Title frame
%______________________________________________________________________
\dwPrintTitle

% Agenda
%______________________________________________________________________
\dwPrintToc

% Section: Bayesian Regression
%______________________________________________________________________
\dwSection{Bayesian Regression}

% Introduction
\begin{dwHeaderFrame}{Introduction}
	\begin{itemize}
		\item You already know what linear regression is and how you can solve it:
		\begin{equation}
			\bm{\theta} = (\bm{\Phi}^{\intercal} \bm{\Phi})^{-1} \bm{\Phi}^{\intercal} \bm{y}
		\end{equation}
		\item It is possible to treat regression in a more probabilistic fashion.
		\item With this probabilistic perspective we can see where regularization comes from and we can derive the least squares error function.
		\item \textbf{Bayes theorem} will play an important role (you should keep it in mind):
		\begin{equation}
			p(A \vert B) = \frac{p(B \vert A) \cdot p(A)}{p(B)}
		\end{equation}
		\item It gives rise to what it referred to as \textbf{Bayesian learning}.
	\end{itemize}
\end{dwHeaderFrame}


% Maximum Likelihood Regression
\begin{dwHeaderFrame}{Maximum Likelihood Regression}
	\begin{itemize}
		\item In probabilistic regression we make two general assumptions:
		\begin{enumerate}
			\item The data is noisy. Therefore, we add an additive noise term $\varepsilon$ to the function estimates:
			\begin{equation}
				y = h(\bm{x}; \bm{\theta}) + \varepsilon
			\end{equation}
			\item The noise term $\varepsilon$ is considered a Gaussian random variable with zero mean:
			\begin{equation}
				\varepsilon \thicksim \mathcal{N}(0, \beta^{-1})
			\end{equation}
		\end{enumerate}
		\item \textbf{With these assumptions $y$ is now a random variable.} It has the following (Gaussian) probability distribution:
		\begin{equation}
			p(y \vert \bm{x}, \bm{\theta}, \beta) = \mathcal{N}(y \vert h(\bm{x}; \bm{\theta}), \beta^{-1})
		\end{equation}
	\end{itemize}
\end{dwHeaderFrame}


\begin{frame}
	\begin{itemize}
		\item We are given a labeled data set $\mathcal{D} = \{ (\bm{x}^{(i)}, y^{(i)}) \}_{i=1}^n$.
		\item Assuming the data is i.\,i.\,d. (independent and identically distributed), the conditional likelihood is computed as follows:
		\begin{align}
			p(\bm{y} \vert \bm{X}, \bm{\theta}, \beta)
				&= \prod_{i=1}^n \mathcal{N}(y^{(i)} \vert h(\bm{x}^{(i)}; \bm{\theta}), \beta^{-1}) \\
				&= \prod_{i=1}^n \mathcal{N}(y^{(i)} \vert \bm{\theta}^{\intercal} \bm{x}^{(i)}, \beta^{-1})
		\end{align}
		\item Compute the log-likelihood:
		\begin{align}
			\log p(\bm{y} \vert \bm{X}, \bm{\theta}, \beta)
				&= \sum_{i=1}^n \log \mathcal{N}(y^{(i)} \vert \bm{\theta}^{\intercal} \bm{x}^{(i)}, \beta^{-1}) \\
				&= \sum_{i=1}^n \left[ \log \left( \frac{\sqrt{\beta}}{\sqrt{2 \pi}} \right) - \frac{\beta}{2} (y^{(i)} - \bm{\theta}^{\intercal} \bm{x}^{(i)})^2 \right] \\
				&= \frac{n}{2} \log \beta - \frac{n}{2} \log 2 \pi - \frac{\beta}{2} \sum_{i=1}^n (y^{(i)} - \bm{\theta}^{\intercal} \bm{x}^{(i)})^2
		\end{align}
	\end{itemize}
\end{frame}


\begin{frame}
	\dwHeader{Computation of the gradient}
	\begin{align}
		\nabla_{\bm{\theta}} \log p(\bm{y} \vert \bm{X}, \bm{\theta}, \beta)
			&\overset{!}{=} \bm{0} \\
		-\beta \sum_{i=1}^n (y^{(i)} - \bm{\theta}^{\intercal} \bm{x}^{(i)}) \bm{x}^{(i)}
			&\overset{!}{=} \bm{0}
	\end{align}
	
	\begin{itemize}
		\item Solving for $\bm{\theta}$ gives the normal equation: $\bm{\theta}_{ml} = (\bm{X}^{\intercal} \bm{X})^{-1} \bm{X}^{\intercal} \bm{y}$.
		\item This is the same result as in least squares regression.
		\item Additionally, we can get a global estimate of the uncertainty: $\beta_{ml} = \left( \frac{1}{n} \sum_{i=1}^n (y^{(i)} - \bm{\theta}^{\intercal} \bm{x}^{(i)})^2 \right)^{-1}$
	\end{itemize}

	\dwAlertBox{Important: Minimizing the squared error gives the maximum likelihood solution for the parameters $\bm{\theta}$ assuming Gaussian noise.}
\end{frame}


% Maximum Aposteriori (MAP) Regression
\begin{dwHeaderFrame}{Maximum Aposteriori (MAP) Regression}
	\begin{itemize}
		\item The problem with maximum likelihood regression is that it might lead to \textbf{overfitting}.
		\item \textit{What can we do to tackle this kind of problem?}
		\item We can use a more Bayesian approach and put a \textbf{prior} on the parameters $\bm{\theta}$:
		\begin{equation}
			\overbracket{p(\bm{\theta} \vert \bm{X}, \bm{y}, \alpha, \beta)}^{\text{posterior}} \propto
				\overbracket{p(\bm{y} \vert \bm{X}, \bm{\theta}, \beta)}^{\text{likelihood}} \cdot
				\overbracket{p(\bm{\theta} \vert \alpha)}^{\text{prior}}
		\end{equation}
		\item The prior probability distribution $p(\bm{\theta} \vert \alpha)$ encodes our \textbf{prior belief} about the parameters $\bm{\theta}$.
	\end{itemize}
	
	\dwAlertBox{Please not the very important difference: In this setting you do not get a single parameter vector $\bm{\theta}$,
		rather a probability distribution over the parameters given the data $p(\bm{\theta} \vert \bm{X}, \bm{y}, \alpha, \beta)$!}
\end{dwHeaderFrame}


\begin{frame}
	\dwHeader{The prior for the parameters}
	\begin{itemize}
		\item We decided to put a prior on the parameters $\bm{\theta}$.
		\item One obvious choice is to use a Gaussian distribution for the prior (with zero mean and spherical covariance):
		\begin{equation}
			\bm{\theta} \thicksim p(\bm{\theta} \vert \alpha) = \mathcal{N}(\bm{\theta} \vert \bm{0}, \alpha^{-1} \bm{I})
		\end{equation}
		\item The posterior then becomes:
		\begin{align}
			\nonumber
			p(\bm{\theta} \vert \bm{X}, \bm{y}, \alpha, \beta)
				&\propto p(\bm{y} \vert \bm{X}, \bm{\theta}, \beta) \cdot p(\bm{\theta} \vert \alpha) \\
				&\propto p(\bm{y} \vert \bm{X}, \bm{\theta}, \beta) \cdot \mathcal{N}(\bm{\theta} \vert \bm{0}, \alpha^{-1} \bm{I}) 
		\end{align}
	\end{itemize}
\end{frame}


\begin{frame}
	\begin{itemize}
		\item Compute the log-likelihood:
		\begin{align}
			\log p(\bm{\theta} \vert \bm{X}, \bm{y}, \alpha, \beta)
				&= \log p(\bm{y} \vert \bm{X}, \bm{\theta}, \beta) + \log \mathcal{N}(\bm{\theta} \vert \bm{0}, \alpha^{-1} \bm{I}) + \text{const} \\
				&= \sum_{i=1}^n \log \mathcal{N}(y^{(i)} \vert \bm{\theta}^{\intercal} \varphi(\bm{x}^{(i)}), \beta^{-1}) +
					\log \mathcal{N}(\bm{\theta} \vert \bm{0}, \alpha^{-1} \bm{I}) + \text{const} \\
				&= -\frac{\beta}{2} \sum_{i=1}^n (y^{(i)} - \bm{\theta}^{\intercal} \varphi(\bm{x}^{(i)}))^2 - \frac{\alpha}{2}\bm{\theta}^{\intercal} \bm{\theta} + \text{const}
		\end{align}
		\item Computation of the gradient:
		\begin{align}
			\nabla_{\bm{\theta}} \log p(\bm{\theta} \vert \bm{X}, \bm{y}, \alpha, \beta)
				&= \beta \sum_{i=1}^n (y^{(i)} - \bm{\theta}^{\intercal} \varphi(\bm{x}^{(i)})) \varphi(\bm{x}^{(i)}) - \alpha \bm{\theta} \overset{!}{=} \bm{0} \\
			\beta \sum_{i=1}^n y^{(i)} \varphi(\bm{x}^{(i)})
				&= \beta \left[ \sum_{i=1}^n \varphi(\bm{x}^{(i)})^{\intercal} \varphi(\bm{x}^{(i)}) \right] \bm{\theta} + \alpha \bm{\theta} \\
			\beta \sum_{i=1}^n y^{(i)} \varphi(\bm{x}^{(i)})
				&= \left[ \beta \sum_{i=1}^n \varphi(\bm{x}^{(i)})^{\intercal} \varphi(\bm{x}^{(i)}) + \alpha \right] \bm{\theta} \\
			\beta \bm{\Phi}^{\intercal} \bm{y}
				&= (\beta \bm{\Phi}^{\intercal} \bm{\Phi} + \alpha \bm{I}) \bm{\theta} \qquad \Rightarrow
					\bm{\theta}_{map} = \left( \bm{\Phi}^{\intercal} \bm{\Phi} + \frac{\alpha}{\beta} \bm{I} \right)^{-1} \bm{\Phi}^{\intercal} \bm{y}
		\end{align}
	\end{itemize}
\end{frame}


\begin{frame}
	\begin{itemize}
		\item The prior \textbf{regularizes} the parameters $\bm{\theta}$.
		\item This approach is referred to as \textbf{ridge regression}.
		\item You already know this result from regularized least squares regression:
		\begin{equation}
			\argmin_{\bm{\theta}} \frac{1}{2} \Vert \bm{\Phi} \bm{\theta} - \bm{y} \Vert^2 + \frac{\lambda}{2} \Vert \bm{\theta} \Vert^2
		\end{equation}
		\item Solving for $\bm{\theta}$, we get the estimate:
		\begin{equation}
			\bm{\theta} = (\bm{\Phi}^{\intercal} \bm{\Phi} + \lambda \bm{I})^{-1} \bm{\Phi}^{\intercal} \bm{y}
		\end{equation}
		\item Here: $\lambda = \frac{\alpha}{\beta}$
	\end{itemize}
	
	\dwAlertBox{You assume two things when you put a regularizer $\lambda$ in least-squares regression: \\
		\ding{182} The targets are noisy, where the noise is distributed according to a Gaussian distribution. \\
		\ding{183} The parameters are Gaussian distributed as well.}
\end{frame}


% Full Bayesian Regression
\begin{dwHeaderFrame}{Full Bayesian Regression}
	\begin{itemize}
		\item Again, we put a prior on the parameters $\bm{\theta}$.
		\begin{equation}
			p(\bm{\theta} \vert \alpha) = \mathcal{N}(\bm{\theta} \vert \bm{\mu}_0, \bm{\Lambda}_0)
			\label{eq:param-prior}
		\end{equation}
		\item In \cref{eq:param-prior}, the mean $\bm{\mu}_0$ and the precision matrix $\bm{\Lambda}_0$ are given by $\bm{0}$ and $\alpha^{-1} \bm{I}$, respectively.
			Therefore, the prior is a zero-mean, isotropic ($\widehat{=}$ rotation-invariant) Gaussian distribution.
		\item The posterior distribution of the parameters $p(\bm{\theta} \vert \bm{X}, \bm{y}, \alpha, \beta)$ is then:
		\begin{align}
			p(\bm{\theta} \vert \bm{X}, \bm{y}, \alpha, \beta)
				&= \mathcal{N}(\bm{\theta} \vert \bm{\mu}_n, \bm{\Lambda}_n) \\
			\shortintertext{with:}
			\bm{\mu}_n
				&= \beta \bm{\Lambda}_n^{-1} \bm{\Phi}^{\intercal} \bm{y}\\
			\bm{\Lambda}_n
				&= \bm{\Lambda}_0^{-1} + \beta \bm{\Phi}^{\intercal} \bm{\Phi}
		\end{align}
		\item This can be phrased as a sequential update rule. The prior must be \textbf{conjugate} in order for this to work.
	\end{itemize}
\end{dwHeaderFrame}


\begin{frame}
	\begin{itemize}
		\item In Bayesian probability theory, if the posterior distribution is in the
			\textbf{same probability distribution family} as the prior probability distribution, the prior and posterior are then called \textbf{conjugate distributions},
			and the prior is called \textbf{conjugate prior}.
		 \item Here: If we multiply two Gaussian distributions, the result is again Gaussian.
	\end{itemize}
	\vspace*{2mm}
	\dwHeader{Example for Bayesian regression}
	\begin{itemize}
		\item We can illustrate Bayesian learning in a linear basis function model.
		\item Consider a single input variable $x$ (scalar) and a linear model of the form $h(x; \bm{\theta}) = \theta_0 + \theta_1 \cdot x$.
		\item Because this model only has two adaptive parameters, we can plot the prior and the posterior distributions directly in parameter space.
		\item We generate synthetic data from the function $f(x; \bm{a}) = a_0 + a_1 \cdot x$, with $a_0 = -0.3$ and $a_1 = 0.5$ by first choosing values of $x^{(i)}$ from the
			uniform distribution $\mathcal{U}(x \vert -1, 1)$, then evaluating $f(x^{(i)}; \bm{a})$, and finally adding some Gaussian noise with precision $\beta = \frac{1}{0.2}$.
			$\alpha$ is fixed to 2.0.
	\end{itemize}
\end{frame}


\begin{frame}
	\dwFigure{
		\begin{minipage}{0.08\textwidth}
			\begin{tikzpicture} \node[rotate=90] at (0,0) {\textbf{prior/posterior}}; \end{tikzpicture} \\[11mm]
			\begin{tikzpicture} \node[rotate=90] at (0,0) {\textbf{data space}}; \end{tikzpicture}
		\end{minipage}
		\hfill
		\begin{minipage}{0.45\textwidth}
			\centering
			$n = 0$ \\
			\includegraphics[scale=0.17]{18_advanced_regression/02_img/prior} \\[-2mm]
			\includegraphics[scale=0.17]{18_advanced_regression/02_img/samples_0}			
		\end{minipage}
		\hfill
		\begin{minipage}{0.45\textwidth}
			\centering
			$n = 1$ \\
			\includegraphics[scale=0.17]{18_advanced_regression/02_img/posterior_1} \\[-2mm]
			\includegraphics[scale=0.17]{18_advanced_regression/02_img/samples_1}
		\end{minipage}
	}{Example for Bayesian regression (part I)}{fig:bayesian-regression-1}
\end{frame}


\begin{frame}
	\dwFigure{
		\begin{minipage}{0.08\textwidth}
			\begin{tikzpicture} \node[rotate=90] at (0,0) {\textbf{prior/posterior}}; \end{tikzpicture} \\[11mm]
			\begin{tikzpicture} \node[rotate=90] at (0,0) {\textbf{data space}}; \end{tikzpicture}
		\end{minipage}
		\hfill
		\begin{minipage}{0.45\textwidth}
			\centering
			$n = 2$ \\
			\includegraphics[scale=0.17]{18_advanced_regression/02_img/posterior_2} \\[-2mm]
			\includegraphics[scale=0.17]{18_advanced_regression/02_img/samples_2}
		\end{minipage}
		\hfill
		\begin{minipage}{0.45\textwidth}
			\centering
			$n = 5$ \\
			\includegraphics[scale=0.17]{18_advanced_regression/02_img/posterior_5} \\[-2mm]
			\includegraphics[scale=0.17]{18_advanced_regression/02_img/samples_5}
		\end{minipage}
	}{Example for Bayesian regression (part II)}{fig:bayesian-regression-2}
\end{frame}


\begin{frame}
	\dwFigure{
		\begin{minipage}{0.08\textwidth}
			\begin{tikzpicture} \node[rotate=90] at (0,0) {\textbf{prior/posterior}}; \end{tikzpicture} \\[11mm]
			\begin{tikzpicture} \node[rotate=90] at (0,0) {\textbf{data space}}; \end{tikzpicture}
		\end{minipage}
		\hfill
		\begin{minipage}{0.45\textwidth}
			\centering
			$n = 20$ \\
			\includegraphics[scale=0.17]{18_advanced_regression/02_img/posterior_20} \\[-2mm]
			\includegraphics[scale=0.17]{18_advanced_regression/02_img/samples_20}
		\end{minipage}
		\hfill
		\begin{minipage}{0.45\textwidth}
			\centering
			$n = 300$ \\
			\includegraphics[scale=0.17]{18_advanced_regression/02_img/posterior_300} \\[-2mm]
			\includegraphics[scale=0.17]{18_advanced_regression/02_img/samples_300}
		\end{minipage}
	}{Example for Bayesian regression (part III)}{fig:bayesian-regression-3}
\end{frame}


\begin{frame}
	\dwHeader{Predictive distribution}
	\begin{itemize}
		\item Usually, we are not interested in $\bm{\theta}$ itself, but rather in making a prediction $y_q$ for a new instance $\bm{x}_q$.
		\item This requires that we evaluate the \textbf{predictive distribution}:
		\begin{equation}
			p(y_q \vert \bm{x}_q, \bm{X}, \bm{y}, \alpha, \beta) = \int_{\bm{\theta}} \underbracket{p(y_q \vert \bm{x}_q, \bm{X}, \bm{\theta}, \beta)}_{\text{regression model}} \cdot
				\underbracket{p(\bm{\theta} \vert \bm{X}, \bm{y}, \alpha, \beta)}_{\text{parameter posterior}} \diff \bm{\theta}
		\end{equation}
		\item We integrate over all possible models and give each model a weight corresponding to how probable it is.
		\item Think of it as a weighted average.
		\item The predictive distribution takes the form:
		\begin{align}
			p(y_q \vert \bm{x}_q, \bm{X}, \bm{y}, \alpha, \beta)
				&= \mathcal{N}(y_q \vert \bm{\mu}_n^{\intercal} \varphi(\bm{x}_q), \sigma_n^2(\bm{x}_q)) \\
			\shortintertext{with:}
			\sigma_n^2(\bm{x}_q)
				&= \frac{1}{\beta} + \varphi(\bm{x}_q)^{\intercal} \bm{\Lambda}_n^{-1} \varphi(\bm{x}_q)
				\label{eq:variance-predictive-dist}
		\end{align}
		\item The first term in \cref{eq:variance-predictive-dist} reflects the noise in the data, the second one the uncertainty associated with the model parameters $\bm{\theta}$.
	\end{itemize}
\end{frame}


\begin{frame}
	\dwFigure{\includegraphics[scale=0.30]{18_advanced_regression/02_img/scatter}}{Bayesian regression and uncertainty estimate}{fig:bayes-regression-uncertainty}
\end{frame}


% Section: Kernel Ridge Regression
%______________________________________________________________________
\dwSection{Kernel Ridge Regression}

% Introduction
\begin{dwHeaderFrame}{Introduction}
	\begin{itemize}
		\item In ridge regression, the optimal parameters $\bm{\theta}$ can be found using the \textbf{normal equation}:
		\begin{equation}
			\bm{\theta} = (\bm{\Phi}^{\intercal} \bm{\Phi} + \lambda \bm{I})^{-1} \bm{\Phi}^{\intercal} \bm{y}
			\label{eq:normal-equation}
		\end{equation}
		\item In the above formula, $\bm{\Phi}$ denotes the design matrix (regressor matrix), $\bm{y}$ is the label vector and $\lambda$ is the regularization parameter.
		\item In order to apply kernels, we have to rephrase this equation in terms of dot products of the input features. Replacing these dot products by kernels avoids operating in feature space.
		\item This can be achieved by using the \textbf{Woodbury matrix identity}.
	\end{itemize}
\end{dwHeaderFrame}


% Woodbury Matrix Identity
\begin{dwHeaderFrame}{Woodbury Matrix Identity}
	\begin{itemize}
		\item For the prediction $y_q$ of a new query data point $\bm{x}_q$, we have to calculate:
		\begin{align}
			y_q
				&= \varphi(\bm{x}_q)^{\intercal} \bm{\theta} \\[2mm]
			\shortintertext{\footnotesize Step \ding{182}: Insert normal equation \cref{eq:normal-equation}:}
				&= \varphi(\bm{x}_q)^{\intercal} (\bm{\Phi}^{\intercal} \bm{\Phi} + \lambda \bm{I})^{-1} \bm{\Phi}^{\intercal} \bm{y} \\[2mm]
			\shortintertext{\footnotesize Step \ding{183}: Apply Woodbury matrix identity:}
				&= \varphi(\bm{x}_q)^{\intercal} \bm{\Phi}^{\intercal}(\bm{\Phi} \bm{\Phi}^{\intercal} + \lambda \bm{I})^{-1} \bm{y}
			\label{eq:woodbury}
		\end{align}
		\item The formula given in \cref{eq:woodbury} exclusively uses dot products of input features and is therefore susceptible to kernels.
	\end{itemize}
\end{dwHeaderFrame}


\begin{frame}
	\begin{itemize}
		\item Replace the dot products by kernel functions:
		\begin{alignat}{2}
		\shortintertext{\footnotesize Rewrite of $\varphi(\bm{x}_q)^{\intercal} \bm{\Phi}^{\intercal}$:}
			\varphi(\bm{x}_q)^{\intercal} \bm{\Phi}^{\intercal} &=
				\varphi(\bm{x}_q)^{\intercal}
				\begin{bmatrix}
					\varphi(\bm{x}^{(1)})^{\intercal} 	\\
					\vdots 						\\
					\varphi(\bm{x}^{(n)})^{\intercal}
				\end{bmatrix}^{\intercal} =
				\begin{bmatrix}
					\mathcal{K}(\bm{x}_q, \bm{x}^{(1)})	\\
					\vdots 						\\
					\mathcal{K}(\bm{x}_q, \bm{x}^{(n)})
				\end{bmatrix} = \bm{K}_*(\bm{x}_q)		\\[8mm]
		\shortintertext{\footnotesize Rewrite of $\bm{\Phi} \bm{\Phi}^{\intercal}$:}
			\bm{\Phi} \bm{\Phi}^{\intercal} &=
				\begin{bmatrix}
					\varphi(\bm{x}^{(1)})^{\intercal} 	\\
					\vdots 						\\
					\varphi(\bm{x}^{(n)})^{\intercal}
				\end{bmatrix}
				\begin{bmatrix}
					\varphi(\bm{x}^{(1)})^{\intercal} 	\\
					\vdots 						\\
					\varphi(\bm{x}^{(n)})^{\intercal}
				\end{bmatrix}^{\intercal} =
				\begin{bmatrix}
					\mathcal{K}(\bm{x}^{(1)}, \bm{x}^{(1)}) 	& \hdots 	& \mathcal{K}(\bm{x}^{(n)}, \bm{x}^{(1)}) \\
					\vdots 							& \ddots 	& \vdots 							\\
					\mathcal{K}(\bm{x}^{(1)}, \bm{x}^{(n)}) 	& \hdots 	& \mathcal{K}(\bm{x}^{(n)}, \bm{x}^{(n)})
				\end{bmatrix} = \bm{K}
		\end{alignat}
		\vspace*{1mm}
		\item The kernel matrices $\bm{K}$ and $\bm{K}_*$ must fulfill \textbf{Mercer's condition} and therefore have to be \textbf{positive-semi definite (psd)}.
			Famous choices: Polynomial kernel or radial basis function (RBF) kernel.
	\end{itemize}
\end{frame}


\begin{frame}
	\begin{itemize}
		\item The final kernel ridge regression formula is given by:
		\begin{equation}
			y_q = \bm{K}_*(\bm{x_q}) (\bm{K} + \lambda \bm{I})^{-1} \bm{y}
		\end{equation}
		\item Like all kernel methods, it is a \textbf{non-parametric} approach.
	\end{itemize}
	
	\dwAlertBox{Kernel methods do not work well for very large data sets (> 10,000 data points), since we have to calculate all pairwise similarities!}
\end{frame}


% Example
\begin{dwHeaderFrame}{Example}
	\dwFigure{\includegraphics[scale=0.475]{18_advanced_regression/02_img/kernel_regression}}{Result of kernel ridge regression}{fig:kernel-regression}
\end{dwHeaderFrame}


% Section: Gaussian Process Regression
%______________________________________________________________________
\dwSection{Gaussian Process Regression}

% Introduction
\begin{dwHeaderFrame}{Introduction}
	\begin{itemize}
		\item Similarly to kernel ridge regression, Gaussian processes do not make any assumptions about the type of regression function (e.\,g. linear, quadratic, ...)
		\item It is non-parametric and a form of supervised learning:
		\begin{equation}
			h(\bm{x}) = \mathcal{GP}(m(\bm{x}), \mathcal{K}(\bm{x}, \bm{x}'))
			\label{eq:gp}
		\end{equation}
		\item In \cref{eq:gp}, $m(\bm{x})$ denotes the mean function, whereas $\mathcal{K}(\bm{x}, \bm{x}')$ denotes the kernel function, which -- in the context of Gaussian processes -- is
			referred to as the covariance function.
		\item Definition of a Gaussian process:
		\begin{quote}
			Formally, a Gaussian process is a collection of random variables, any finite number of which has a \textbf{joint Gaussian distribution}.
		\end{quote}
	\end{itemize}
\end{dwHeaderFrame}


\begin{frame}
	\begin{itemize}
		\item Instead of modeling a distribution over parameters (cf. Bayesian regression), we model a \textbf{distribution over possible regression functions}.
		\item Thus, Gaussian processes extend multivariate Gaussian distributions to \textbf{infinite dimensions}.
		\begin{itemize}
			\item E.\,g. a function $f : \mathbb{R} \mapsto \mathbb{R}$ can be thought of as a sample from some infinite Gaussian distribution.
			\item Pick the function which maximizes the posterior distribution over functions.
		\end{itemize}
		\item The mean of the prior $m(\bm{x})$ distribution is usually set to 0 everywhere.
		\item In practice, the squared exponential function ($\widehat{=}$ RBF-kernel) is frequently used:
		\begin{equation}
			\mathcal{K}(\bm{x}, \bm{x}') = \sigma_f^2 \cdot \exp\left\{ \frac{-\Vert \bm{x} - \bm{x}' \Vert^2}{2 \cdot l^2} \right\}
		\end{equation}
		\item Hyper-Parameters:
		\begin{itemize}
			\item $\sigma_f^2$ denotes the maximum allowable covariance. It should be high for functions covering a broad range of the $y$-axis. If $\bm{x} \approx \bm{x}'$,
				$\mathcal{K}(\bm{x}, \bm{x}')$ approaches this maximum.
			\item $l$ (landmark) controls how much the data points influence each other.
		\end{itemize}
	\end{itemize}
\end{frame}


% Learning a Gaussian Process Model
\begin{dwHeaderFrame}{Learning a Gaussian Process Model}
	\begin{itemize}
		\item We are given a training data set $\mathcal{D}$ comprising $n$ observations:
		\begin{equation*}
			\mathcal{D} = \{ (\bm{x}^{(1)}, y^{(1)}), (\bm{x}^{(2)}, y^{(2)}), \dots, (\bm{x}^{(n)}, y^{(n)}) \} = \{ (\bm{x}^{(i)}, y^{(i)}) \}_{i=1}^n
		\end{equation*}
		\item Also, we have a query data point $\bm{x}_q$, for which $y_q$ has to be predicted.
		\item To do so, we compute the covariance between all example pairs.
		\item This results in three matrices $\bm{K}$ (matrix), $\bm{K}_*$ (vector) and $\bm{K}_{**}$ (scalar).
	\end{itemize}
\end{dwHeaderFrame}


\begin{frame}
	The matrices have the following form:

	\begin{align}
		\bm{K} &=
		\begin{bmatrix}
			\mathcal{K}(\bm{x}^{(1)}, \bm{x}^{(1)}) 	& \mathcal{K}(\bm{x}^{(2)}, \bm{x}^{(1)}) & \hdots & \mathcal{K}(\bm{x}^{(n)}, \bm{x}^{(1)}) 	\\
			\mathcal{K}(\bm{x}^{(1)}, \bm{x}^{(2)}) 	& \mathcal{K}(\bm{x}^{(2)}, \bm{x}^{(2)}) & \hdots & \mathcal{K}(\bm{x}^{(n)}, \bm{x}^{(2)}) 	\\
			\vdots 							& \vdots 							& \ddots & \vdots 							\\
			\mathcal{K}(\bm{x}^{(1)}, \bm{x}^{(n)}) 	& \mathcal{K}(\bm{x}^{(2)}, \bm{x}^{(n)}) & \hdots & \mathcal{K}(\bm{x}^{(n)}, \bm{x}^{(n)})
		\end{bmatrix} \\[8mm]
		\bm{K}_* &=
		\begin{bmatrix}
			\mathcal{K}(\bm{x}_q, \bm{x}^{(1)}) & \mathcal{K}(\bm{x}_q, \bm{x}^{(2)}) & \dots & \mathcal{K}(\bm{x}_q, \bm{x}^{(n)})
		\end{bmatrix}^{\intercal} \\[8mm]
		\bm{K}_{**} &= \mathcal{K}(\bm{x}_q, \bm{x}_q)
	\end{align}

	\dwAlertBox{$\bm{K}$ is a matrix (contains the similarities of training data pairs), $\bm{K}_*$ is a vector (contains similarities of the query data point with the training data),
		while $\bm{K}_{**}$ is actually a scalar (comparison of data point $\bm{x}_q$ to itself)!}
\end{frame}


\begin{frame}
	\begin{itemize}
		\item Since we assume that the data can be modeled as a sample from a multivariate Gaussian distribution, we can model the Gaussian process prior as follows:
		\begin{equation}
			\begin{bmatrix} \bm{y} \\ y_q \end{bmatrix} \thicksim
				\mathcal{N} \left(
					\bm{0}, 
					\begin{bmatrix} \bm{K} & \bm{K_*^{\intercal}} \\ \bm{K_*} & \bm{K_{**}} \end{bmatrix}
				\right)
		\end{equation}
		\item What we actually want is the \textbf{posterior distribution} $p(y_q \vert \bm{y})$: \textit{`Given the data, what is $y_q$?'}
		\item For Gaussian distributions, the posterior distribution can be computed analytically:
		\begin{equation}
			y_q \vert \bm{y} \thicksim \mathcal{N}(
				\underbracket{\bm{K}_* \bm{K}^{-1} \bm{y}}_{
					\substack{\text{\textbf{Matrix of}} \\ \text{\textbf{regr. coeff.}}}
				},
				\underbracket{\bm{K}_{**} - \bm{K}_* \bm{K}^{-1} \bm{K}_*^{\intercal}}_{
					\text{\textbf{Schur complement}}
				}
			)
		\end{equation}
		\item The mean of the posterior distribution is given by the \textbf{matrix of regression coefficients},
			its variance can be computed using the \textbf{Schur complement}.
		\item We can compute confidence intervals (e.\,g. 90\,\% | 95\,\% | 99\,\%):
		\begin{equation}
			(1.65\ |\ 1.96\ |\ 2.58) \cdot \sqrt{var(y_q)}
		\end{equation}
	\end{itemize}
\end{frame}


% Example
\begin{dwHeaderFrame}{Example}
	\dwFigure{
		\divideTwo{0.49}{
			\input{18_advanced_regression/03_tbl/datapoints_gp}
		}{0.49}{
			\vspace*{3mm}
			\input{18_advanced_regression/01_tikz/datapoints_gp}
		}
	}{Example data set for a Gaussian process}{fig:example-gp}
	
	\begin{itemize}
		\item Suppose $\sigma_f = 1.27, l = 1.00$. What is $y_q$ for $\bm{x}_q = 8$?
		\item Let's plot the prior distribution first.
	\end{itemize}
\end{dwHeaderFrame}


\begin{frame}
	\dwHeader{Prior distribution}
	\dwFigure{\input{18_advanced_regression/01_tikz/gp_prior}}{Prior distribution for the Gaussian process}{fig:gp-prior}
	\begin{itemize}
		\item Naturally, the prior does not fit the data well (we have not fitted the model yet).
		\item We have zero mean everywhere.
	\end{itemize}
\end{frame}


\begin{frame}
	\dwHeader{Posterior distribution}
	\dwFigure{\input{18_advanced_regression/01_tikz/gp}}{Posterior distribution for the Gaussian process}{fig:gp-posterior}
	\dwAlertBox{Wait a minute: Isn't this model overfitting the training data?}
\end{frame}


\begin{frame}
	\begin{itemize}
		\item The model clearly overfits the data as can be seen from the previous slide (the regression line goes through each training data point perfectly).
		\item This is because the model assumes the data to be \textbf{noise-free}.
		\item It is possible to add a little bit of noise, in order to deal with this easily ($\sigma_n$ is the variance of the noise):
		\begin{equation}
			\bm{K}_{\sigma_n} \longleftarrow \bm{K} + \sigma_n \bm{I}
		\end{equation}
		\item The updated formulas look like this:
		\begin{itemize}
			\item Matrix of regression coefficients (\textbf{same result as in kernel ridge regression}):
			\begin{equation}
				\bm{K_*} \bm{K}_{\sigma_n}^{-1} \bm{y}
			\end{equation} 
			\item Schur complement:
			\begin{equation}
				\bm{K}_{**} - \bm{K}_* \bm{K}_{\sigma_n}^{-1} \bm{K}_*^{\intercal}
			\end{equation}
		\end{itemize}
	\end{itemize}
\end{frame}


\begin{frame}
	\dwHeader{Prior distribution (with noise)}
	\dwFigure{\input{18_advanced_regression/01_tikz/gp_with_noise}}{Posterior distribution for the Gaussian process with noise}{fig:gp-posterior-noise}
\end{frame}


% Learning the Hyper-Parameters
\begin{dwHeaderFrame}{Learning the Hyper-Parameters}
	\begin{itemize}
		\item The results of Gaussian process regression depend heavily on the parameters $\{ \sigma_f, l \}$, which is why these parameters should be optimized for the task at hand.
		\item This can be down by maximizing the \textbf{marginal likelihood} (e.\,g. by using gradient ascent).
	\end{itemize}
	
	\dwInfoBox{The exact procedure is very involved and out of scope for this lecture.}
\end{dwHeaderFrame}


% Section: Support Vector Regression
%______________________________________________________________________
\dwSection{Support Vector Regression}

% Introduction
\begin{dwHeaderFrame}{Introduction}
	\begin{itemize}
		\item Support vector machines can be extended to regression problems, while preserving the property of sparseness.
		\item In ordinary least squares, we minimize a regularized error function given by:
		\begin{equation}
			\mathcal{J}(\bm{\theta}) = \frac{1}{2} \sum_{i=1}^n (\widehat{h}(\bm{x}^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2} \Vert \bm{w} \Vert^2
		\end{equation}
		\item In the following, $\bm{\theta} = \{ \bm{w}, b \}$ and $\widehat{h}(\bm{x}) = \bm{w}^{\intercal} \varphi(\bm{x}) + b$.
		\item To obtain sparse solutions, the quadratic error is replaced by an \textbf{$\bm{\varepsilon}$-insensitive error function}, which gives zero error if the absolute difference between the
			prediction and the target is less than $\varepsilon$:
		\begin{equation}
			\ell_{\varepsilon}(\widehat{h}(\bm{x}) - y) =
			\begin{cases}
				0 									& \text{if $\vert \widehat{h}(\bm{x}) - y \vert < \varepsilon$} \\
				\vert \widehat{h}(\bm{x}) - y \vert - \varepsilon 	& \text{otherwise}
			\end{cases}
		\end{equation}
	\end{itemize}
\end{dwHeaderFrame}


\begin{frame}
	\dwFigure{\input{18_advanced_regression/01_tikz/epsilon_insensitive_error}}
		{An $\varepsilon$-insensitive error function (red) compared to the quadratic error function (blue)}{fig:eps-insensitive-error}
\end{frame}


\begin{frame}
	\begin{itemize}
		\item We therefore minimize a regularized error function given by:
		\begin{equation}
			\mathcal{J}(\bm{\theta}) = C \sum_{i=1}^n \ell_{\varepsilon}(\widehat{h}(\bm{x}^{(i)}) - y^{(i)}) + \frac{1}{2} \Vert \bm{w} \Vert^2
		\end{equation}
		\item Analogously to support vector machines for classification, $C$ denotes the (inverse) regularization parameter.
		\item Again, we introduce slack variables:
		\begin{itemize}
			\item We now need two slack variables $\xi_i \ge 0$ and $\widehat{\xi}_i \ge 0$ for each data point $\bm{x}^{(i)}$.
			\item $\xi_i > 0$ corresponds to a point for which $y^{(i)} > \widehat{h}(\bm{x}^{(i)}) + \varepsilon$.
			\item $\widehat{\xi}_i \ge 0$ corresponds to a point for which $y^{(i)} < h(\bm{x}^{(i)}) - \varepsilon$.
		\end{itemize}
		\item The error function for support vector regression can then be rewritten as:
		\begin{equation}
			\mathcal{J}(\bm{\theta}) = C \sum_{i=1}^n (\xi_i + \widehat{\xi}_i) + \frac{1}{2} \Vert \bm{w} \Vert^2
			\label{eq:svr-error-function}
		\end{equation}
	\end{itemize}
\end{frame}


\begin{frame}
	\divideTwo{0.29}{
		Illustration of SVM regression, showing the regression curve together with the $\varepsilon$-insensitive `tube'. Also shown are examples
		of the slack variables $\xi$ and $\widehat{\xi}$. \\[3mm]

		Points above the $\varepsilon$-tube have $\xi > 0$ and $\widehat{\xi} = 0$, points below the tube have $\xi = 0$ and $\widehat{\xi} > 0$.
		Points inside the tube are characterized by $\xi = \widehat{\xi} = 0$.
	}{0.69}{
		\vspace*{3mm}
		\dwFigure{\input{18_advanced_regression/01_tikz/svr}}{Illustration of support vector regression}{fig:svr}
	}
\end{frame}


% Optimization
\begin{dwHeaderFrame}{Optimization}
	\begin{itemize}
		\item The cost function given by \cref{eq:svr-error-function} must be minimized subject to the constraints:
		\begin{align}
			\xi_i 			&\ge 0 							\\
			\widehat{\xi}_i 	&\ge 0 							\\
			y^{(i)} 		&\le h(\bm{x}^{(i)}) + \varepsilon + \xi_i 	\\
			y^{(i)} 		&\ge h(\bm{x}^{(i)}) - \varepsilon - \widehat{\xi}_i
		\end{align}
		\item This can be achieved by introducing Lagrange multipliers $\alpha_i \ge 0, \widehat{\alpha}_i \ge 0, \mu_i \ge 0$ and $\widehat{\mu}_i \ge 0$:
		\begin{align}
			\nonumber
			\mathcal{L}
				&= C \sum_{i=1}^n (\xi_i + \widehat{\xi}_i) + \frac{1}{2} \Vert \bm{w} \Vert^2 - \sum_{i=1}^n (\mu_i \xi_i + \widehat{\mu}_i \widehat{\xi}_i) \\
				&\phantom{=} - \sum_{i=1}^n \alpha_i (\varepsilon + \xi_i + \widehat{h}(\bm{x}^{(i)}) - y^{(i)}) -
					\sum_{i=1}^n \widehat{\alpha}_i (\varepsilon + \widehat{\xi}_i - \widehat{h}(\bm{x}^{(i)}) + y^{(i)})
		\end{align}
	\end{itemize}
\end{dwHeaderFrame}


\begin{frame}
	\dwHeader{Derivatives of $\mathcal{L}$}
	\begin{alignat}{2}
		\frac{\partial\mathcal{L}}{\partial \bm{w}} 		&\overset{!}{=} 0 \qquad &&\Rightarrow \qquad \bm{w} = \sum_{i=1}^n (\alpha_i - \widehat{\alpha}_i) \varphi(\bm{x}^{(i)})
		\label{eq:derivative-w}																													\\[3mm]
		\frac{\partial\mathcal{L}}{\partial b} 			&\overset{!}{=} 0 \qquad &&\Rightarrow \qquad \sum_{i=1}^n (\alpha_i - \widehat{\alpha}_i) = 0 						\\[3mm]
		\frac{\partial\mathcal{L}}{\partial \xi_i} 			&\overset{!}{=} 0 \qquad &&\Rightarrow \qquad \alpha_i + \mu_i = C
		\label{eq:derivative-xi}																													\\[3mm]
		\frac{\partial\mathcal{L}}{\partial \widehat{\xi}_i} 	&\overset{!}{=} 0 \qquad &&\Rightarrow \qquad \widehat{\alpha}_i + \widehat{\mu}_i = C
		\label{eq:derivative-xi-hat}
	\end{alignat}

	\dwInfoBox{We can use these results to obtain the dual formulation which has to be maximized.}
\end{frame}


\begin{frame}
	\dwHeader{Dual formulation}
	\begin{itemize}
		\item The dual formulation is given by:
		\begin{equation}
			\mathcal{L}(\bm{\alpha}, \bm{\widehat{\alpha}}) = -\frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n (\alpha_i - \widehat{\alpha}_i) (\alpha_j - \widehat{\alpha}_j)
				\mathcal{K}(\bm{x}^{(i)}, \bm{x}^{(j)}) - \varepsilon \sum_{i=1}^n (\alpha_i + \widehat{\alpha}_i) + \sum_{i=1}^n (\alpha_i - \widehat{\alpha}_i) y^{(i)}
		\end{equation}
		\item The dual is expressed in terms of a kernel function $\mathcal{K}(\bm{x}, \bm{x}')$.
		\item Maximize the dual function: $\max_{\bm{\alpha}, \bm{\widehat{\alpha}}} \mathcal{L}(\bm{\alpha}, \bm{\widehat{\alpha}})$
		\item Again, this is a constraint optimization problem which is optimized subject to:
		\begin{alignat}{2}
			0 	&\le \alpha_i 			&&\le C \\
			0 	&\le \widehat{\alpha}_i 	&&\le C
		\end{alignat}
		\item We again have the \textbf{box constraints} which directly follow from the fact that the Lagrange multipliers have to be $\ge 0$
			together with \cref{eq:derivative-xi} and \cref{eq:derivative-xi-hat}.
	\end{itemize}
\end{frame}


\begin{frame}
	\begin{itemize}
		\item Substituting \cref{eq:derivative-w} into $\widehat{h}(\bm{x})$, we see that predictions for new inputs can be made using:
		\begin{equation}
			\widehat{h}(\bm{x}) = \sum_{i=1}^n (\alpha_i - \widehat{\alpha}_i) \mathcal{K}(\bm{x}, \bm{x}^{(i)}) + b
			\label{eq:svr-prediction}
		\end{equation}
		\item The support vectors are those data points for which $\alpha_i \ne 0$ or $\widehat{\alpha}_i \ne 0$. Such points either lie on the boundary of the
			$\varepsilon$-tube or outside the tube. All points within the tube have $\alpha_i = \widehat{\alpha}_i = 0$.
		\item It is again a \textbf{sparse solution}, since we only need the support vectors for the prediction.
	\end{itemize}
\end{frame}


% Karush-Kuhn-Tucker Conditions
\begin{dwHeaderFrame}{Karush-Kuhn-Tucker Conditions}
	\begin{itemize}
		\item The \textbf{Karush-Kuhn-Tucker (KKT) conditions} state that at the solution, the product of dual variables and constraints must vanish.
		\item The KKT conditions for support vector regression are given by:
		\begin{align}
			\alpha_i (\varepsilon + \xi_i + \widehat{h}(\bm{x}^{(i)}) - y^{(i)}) 					&= 0
			\label{eq:kkt-1}														\\[2mm]
			\widehat{\alpha}_i (\varepsilon + \widehat{\xi}_i - \widehat{h}(\bm{x}^{(i)}) + y^{(i)}) 	&= 0
			\label{eq:kkt-2}														\\[2mm]
			\overbracket{(C - \alpha_i)}^{\mu_i} \xi_i									&= 0
			\label{eq:kkt-3}														\\[2mm]
			\underbracket{(C - \widehat{\alpha}_i)}_{\widehat{\mu}_i} \widehat{\xi}_i 			&= 0
		\end{align}
	\end{itemize}
	
	\dwInfoBox{We can derive useful results from the KKT conditions (cf. next slide).}
\end{dwHeaderFrame}


\begin{frame}
	\begin{itemize}
		\item First of all, we note that $\alpha_i$ can only be \textbf{non-zero}, if $\varepsilon + \xi_i + \widehat{h}(\bm{x}^{(i)}) - y^{(i)} = 0$. This implies that the data point either lies
			on the upper boundary of the $\varepsilon$-tube ($\xi_i = 0$) or above it ($\xi_i > 0$).
		\item Analogous: $\widehat{\alpha}_i$
		\item The two constraints $\varepsilon + \xi_i + \widehat{h}(\bm{x}^{(i)}) - y^{(i)}$ and $\varepsilon + \widehat{\xi}_i - \widehat{h}(\bm{x}^{(i)}) + y^{(i)}$ are incompatible.
			This can be seen by adding them together and noting that $\xi_i, \widehat{\xi}_i$ are non-negative and $\varepsilon$ is strictly positive. So for every data point
			$\bm{x}^{(i)}$, either $\alpha_i$ or $\widehat{\alpha}_i$ (or both) must be zero.
		\item Parameter $b$ in \cref{eq:svr-prediction} can be found by considering a data point for which $0 < \alpha_i < C$ ($\widehat{=}$ support vector).
			From \cref{eq:kkt-3} it must have $\xi_i = 0$. Therefore, according to \cref{eq:kkt-1} it must satisfy $\varepsilon + \widehat{h}(\bm{x}^{(i)}) - y^{(i)} = 0$.
		\item For $b$ we obtain:
		\begin{align}
			b 
				&= y^{(i)} - \varepsilon - \bm{w}^{\intercal} \varphi(\bm{x}^{(i)}) \\[1mm]
				&= y^{(i)} - \varepsilon - \sum_{j=1}^n (\alpha_j - \widehat{\alpha}_j) \mathcal{K}(\bm{x}^{(i)}, \bm{x}^{(j)})
		\end{align}
		\item In practice, it is better to consider all support vectors to find $b$ (average).
	\end{itemize}
\end{frame}


% Example
\begin{dwHeaderFrame}{Example}
	\dwFigure{\includegraphics[scale=0.275]{18_advanced_regression/02_img/svr_example}}
		{Example of support vector regression using \texttt{scikit-learn}}{fig:svr-example}
\end{dwHeaderFrame}


% Thank you
%______________________________________________________________________
\makethanks

\end{document}
