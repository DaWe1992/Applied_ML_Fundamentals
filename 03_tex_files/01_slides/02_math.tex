% rubber: clean 02_math.aux
% rubber: clean 02_math.fls
% rubber: clean 02_math.out
% rubber: clean 02_math.log
\input{preamble_theme}
\usepackage{hyperref}
\usepackage{movie15}

% ====================================================
% ====================================================
% PRESENTATION DATA
% ====================================================
% ====================================================

\title[Mathematical Foundations]{*** Applied Machine Learning Fundamentals *** Mathematical Foundations}
\institute{SAP\,SE}
\author{Daniel Wehner}
\date{\today}
\prefix{MATH}

% ====================================================
% ====================================================
% BEGIN OF DOCUMENT
% ====================================================
% ====================================================

\begin{document}

% Title frame
%______________________________________________________________________
\maketitlepage


% Lecture Overview
%______________________________________________________________________
\begin{frame}{Lecture Overview}{}
	\makeoverview{2}
\end{frame}


% Agenda
%______________________________________________________________________
\begin{frame}{Agenda \today}
	\begin{multicols}{2}
		\tableofcontents
	\end{multicols}
\end{frame}


% Section: Introduction
%______________________________________________________________________
\section{Introduction}
\makedivider{Introduction}

% Introduction
\begin{frame}{Introduction}{}

\end{frame}


% Section: Linear Algebra
%______________________________________________________________________
\section{Linear Algebra}
\makedivider{Linear Algebra}

% Subsection: Vectors
% --------------------------------------------------------------------------------------------------------
\subsection{Vectors}

% What is a Vector?
\begin{frame}{What is a Vector?}{}
	\divideTwo{0.49}{
		\begin{align*}
			\bm{x} 	&= \left[\begin{array}{C{1em}} $x_1$ \\ $x_2$ \end{array}\right] 	\\[4mm]
			\bm{v} 	&= \left[\begin{array}{C{1em}} $3$ \\ $1$ \end{array}\right] 		\\[4mm]
			\bm{w} 	&= \left[\begin{array}{C{1em}} $2$ \\ $4$ \end{array}\right]
		\end{align*}
	}{0.49}{
		\input{02_math/01_tikz/vector}
	}
\end{frame}


% Multiplication by a Scalar
\begin{frame}{Multiplication by a Scalar}{}
	\divideTwo{0.49}{
		\begin{align*}
			c \bm{x}
				&= c \left[\begin{array}{C{1em}} $x_1$ \\ $x_2$ \end{array}\right]
					= \left[\begin{array}{C{1em}} $c x_1$ \\ $c x_2$ \end{array}\right]	\\[4mm]
			2 \bm{v} 
				&= 2 \left[\begin{array}{C{1em}} $3$ \\ $1$ \end{array}\right]
					= \left[\begin{array}{C{1em}} $6$ \\ $2$ \end{array}\right]
		\end{align*}
	}{0.49}{
		\input{02_math/01_tikz/vector_mult_scalar}
	}
\end{frame}


% Addition of Vectors
\begin{frame}{Addition of Vectors}{}
	\divideTwo{0.59}{
		\begin{align*}
			\bm{x} + \bm{y} = \left[\begin{array}{C{1em}} $x_1$ \\ $x_2$ \end{array}\right] +
					\left[\begin{array}{C{1em}} $y_1$ \\ $y_2$ \end{array}\right]
				&= \left[\begin{array}{C{3em}} $x_1 + y_1$ \\ $x_2 + y_2$ \end{array}\right] \\[4mm]
			\bm{v} + \bm{w} = \left[\begin{array}{C{1em}} $3$ \\ $1$ \end{array}\right] +
					\left[\begin{array}{C{1em}} $2$ \\ $4$ \end{array}\right]
				&= \left[\begin{array}{C{1em}} $5$ \\ $5$ \end{array}\right]
		\end{align*}
	}{0.39}{
		\input{02_math/01_tikz/vector_addition}
	}
\end{frame}


% Linear Combination of Vectors
\begin{frame}{Linear Combination of Vectors}{}
	\begin{align*}
		\bm{u} = c_1 \bm{v}^{(1)} + c_2 \bm{v}^{(2)} + \dots + c_n \bm{v}^{(n)}
	\end{align*}
\end{frame}


% Vector Transpose and inner and outer Product
\begin{frame}{Vector Transpose and inner and outer Product}{}
	\begin{itemize}
		\item Vector transpose:
		\begin{equation*}
			\bm{v} = \left[\begin{array}{C{1em}} $3$ \\ $1$ \end{array}\right] \qquad\qquad
			\bm{v}^{\intercal} = \left[\begin{array}{C{1em} C{1em}} $3$ & $1$ \end{array}\right]
		\end{equation*}
		\item Inner product / dot product / scalar product:
		\begin{align*}
			\bm{v} \cdot \bm{w}
				&\equiv \bm{v}^{\intercal} \bm{w} \equiv \langle \bm{v}, \bm{w} \rangle \\[1mm]
				&= \left[\begin{array}{C{1em} C{1em}} $3$ & $1$ \end{array}\right]
					\left[\begin{array}{C{1em}} $2$ \\ $4$ \end{array}\right]
					= (3 \cdot 2) + (1 \cdot 4) = 10
		\end{align*}
	\end{itemize}
\end{frame}


% Vector Transpose and inner and outer Product (Ctd.)
\begin{frame}{Vector Transpose and inner and outer Product (Ctd.)}{}
	\begin{itemize}
		\item Outer product:
		\begin{equation*}
			\bm{v}\bm{w}^{\intercal}
				= 	\left[\begin{array}{C{1em}} $3$ \\ $1$ \end{array}\right]
						\left[\begin{array}{C{1em} C{1em}} $2$ & $4$ \end{array}\right]
				= 	\left[\begin{array}{C{1em} C{1em}} $6$ & $12$ \\ $2$ & $4$ \end{array}\right]
		\end{equation*}
	\end{itemize}

	\begin{boxBlueNoFrame}
		\highlight{The inner product yields a scalar value, the results of an outer product is a matrix!}
	\end{boxBlueNoFrame}
\end{frame}


% Length of a Vector
\begin{frame}{Length of a Vector}{}
	\begin{itemize}
		\item Length of a vector (Frobenius norm):
		\begin{align}
			\Vert \bm{x} \Vert
				&= \sqrt{\bm{x}^{\intercal} \bm{x}}		\\[1mm]
			\Vert c \bm{x} \Vert
				&= \vert c \vert \cdot \Vert \bm{x} \Vert	\\[1mm]
			\Vert \bm{x} + \bm{y} \Vert
				&\le \Vert \bm{x} \Vert + \Vert \bm{y} \Vert
		\end{align}
		\item Example:
		\begin{equation*}
			\Vert \bm{v} \Vert = \sqrt{3^2 + 1^2} = 10
		\end{equation*}
	\end{itemize}
\end{frame}


% Angle between Vectors
\begin{frame}{Angle between Vectors}{}
	\begin{itemize}
		\item The angle between two vectors is given by:
		\begin{align}
			\cos \measuredangle (\bm{x}, \bm{y}) &= \frac{\bm{x} \cdot \bm{y}}{\Vert \bm{x} \Vert \cdot \Vert \bm{y} \Vert}
				= \frac{\sum_{j=1}^m x_j \cdot y_j}{\sqrt{\sum_{j=1}^m (x_j)^2} \cdot \sqrt{\sum_{j=1}^m (y_j)^2}} \\[3mm]
			\nonumber
			\cos \measuredangle (\bm{v}, \bm{w}) &= \frac{\bm{v} \cdot \bm{w}}{\Vert \bm{v} \Vert \cdot \Vert \bm{w} \Vert}
				= \frac{10}{\sqrt{10} \cdot \sqrt{20}} \approx 0.71
		\end{align}
		\vspace*{1mm}
		\item Inner product: $\bm{x} \cdot \bm{y}
			= \Vert \bm{x} \Vert \cdot \Vert \bm{y} \Vert \cdot \cos \measuredangle (\bm{x}, \bm{y})$
	\end{itemize}
\end{frame}


% Projection of Vectors
\begin{frame}{Projection of Vectors}{}
	\floattext{12}{5}{
		\input{02_math/01_tikz/vector_projection}
	}
	\begin{itemize}
		\item How is the projection of $\bm{x}$ onto $\bm{y}$ defined?
		\item Formally, we have:
		\begin{align}
			\nonumber
			p 	&= \Vert \bm{v} \Vert \cos \measuredangle (\bm{v}, \bm{w}) 							\\[1mm]
			\nonumber
				&= \Vert \bm{v} \Vert \frac{\bm{v} \cdot \bm{w}}{\Vert \bm{v} \Vert \cdot \Vert \bm{w} \Vert} 	\\[1mm]
				&= \frac{\bm{v} \cdot \bm{w}}{\Vert \bm{w} \Vert}
		\end{align}
		\item Note that $p$ is \textbf{not a vector!}
	\end{itemize}
\end{frame}


% Subsection: Matrices
% --------------------------------------------------------------------------------------------------------
\subsection{Matrices}

% What is a Matrix?
\begin{frame}{What is a Matrix?}{}
	\vspace*{-3mm}
	\divideTwo{0.49}{
		General case ($\mathbb{R}^{n \times m}$):
		\begin{equation*}
			\bm{X}
				= 	\left[\begin{array}{C{1.5em} C{1.5em} C{1.5em} C{1.5em}}
						$X_{11}$ 	& 	$X_{12}$ 	& 	$\hdots$ & 	$X_{1m}$ 	\\
						$X_{21}$ 	& 	$X_{22}$ 	& 	$\hdots$ &	$X_{2m}$ 	\\
						$\vdots$  	& 	$\vdots$		& 	$\ddots$ & 	$\vdots$		\\
						$X_{n1}$ 	& 	$X_{n2}$ 	& 	$\hdots$ & 	$X_{nm}$
					\end{array}\right]
		\end{equation*}
	}{0.49}{
		\begin{alignat*}{2}
			\bm{M}
				&=	\left[\begin{array}{C{1em} C{1em} C{1em}}
						$3$ & $4$ & $5$ \\
						$1$ & $0$ & $1$
					\end{array}\right]
			\qquad &\mathbb{R}^{2 \times 3}
			\\[3mm]
			\bm{N}
				&=	\left[\begin{array}{C{1em} C{1em} C{1em}}
						$3$ & $0$ & $0$ \\
						$0$ & $7$ & $0$ \\
						$0$ & $0$ & $1$
					\end{array}\right]
			\qquad &\mathbb{R}^{3 \times 3}
			\\[3mm]
			\bm{P}
				&=	\left[\begin{array}{C{1em} C{1em}}
						$10$ & $1$ \\
						$11$ & $2$
					\end{array}\right]
			\qquad &\mathbb{R}^{2 \times 2}
		\end{alignat*}
	}
\end{frame}


% Matrix Transpose and Addition
\begin{frame}{Matrix Transpose and Addition}{}
	\begin{itemize}
		\item Transpose of a matrix:
		\begin{equation}
			\bm{M}^{\intercal}
				=	\left[\begin{array}{C{1em} C{1em} C{1em}}
						$3$ & $4$ & $5$ \\
						$1$ & $0$ & $1$
					\end{array}\right]^{\intercal}
				=	\left[\begin{array}{C{1em} C{1em}}
						$3$ & $1$ \\
						$4$ & $0$ \\
						$5$ & $1$
					\end{array}\right]
		\end{equation}
		\item Addition of matrices:
		\begin{equation}
			\bm{X} + \bm{Y}
				=	\left[\begin{array}{C{1.5em} C{1.5em}}
						$X_{11}$ & $X_{12}$ \\
						$X_{21}$ & $X_{22}$
					\end{array}\right] +
					\left[\begin{array}{C{1.5em} C{1.5em}}
						$Y_{11}$ & $Y_{12}$ \\
						$Y_{21}$ & $Y_{22}$
					\end{array}\right]
				=	\left[\begin{array}{C{4em} C{4em}}
						$X_{11} + Y_{11}$ & $X_{12} + Y_{12}$ \\
						$X_{21} + Y_{21}$ & $X_{22} + Y_{22}$
					\end{array}\right]
		\end{equation}
	\end{itemize}
\end{frame}


% Matrix Multiplication
\begin{frame}{Matrix Multiplication}{}
	\begin{itemize}
		\item Multiplication by scalars:
		\begin{equation}
			c \bm{X}
				=	c \left[\begin{array}{C{1.5em} C{1.5em} C{1.5em}}
						$X_{11}$ & $X_{12}$ & $X_{13}$ \\
						$X_{21}$ & $X_{22}$ & $X_{23}$
					\end{array}\right]
				= 	\left[\begin{array}{C{3em} C{3em} C{3em}}
						$c \cdot X_{11}$ & $c \cdot X_{12}$ & $c \cdot X_{13}$ \\
						$c \cdot X_{21}$ & $c \cdot X_{22}$ & $c \cdot X_{23}$
					\end{array}\right]
		\end{equation}
		\item Matrix-vector multiplication:
		\begin{equation}
			\bm{z} = \bm{X}\bm{y}
				= 	\left[\begin{array}{C{1em} C{1em}}
						$X_{11}$ & $X_{12}$ \\
						$X_{21}$ & $X_{22}$
					\end{array}\right]
					\left[\begin{array}{C{1em}} $y_1$ \\ $y_2$ \end{array}\right]
				=	\left[\begin{array}{C{6em}}
						$X_{11} \cdot y_1 + X_{12} \cdot y_2$ \\
						$X_{21} \cdot y_1 + X_{22} \cdot y_2$
					\end{array}\right]
		\end{equation}
	\end{itemize}
\end{frame}


% Matrix Multiplication (Ctd.)
\begin{frame}{Matrix Multiplication (Ctd.)}{}
	\begin{itemize}
		\item Matrix-matrix multiplication:
		\begin{align}
			\nonumber
			\bm{Z}
				&= \bm{X}\bm{Y} \\
			\nonumber
				&= 	\left[\begin{array}{C{1.5em} C{1.5em} C{1.5em}}
						$X_{11}$ & $X_{12}$ & $X_{13}$ \\
						$X_{21}$ & $X_{22}$ & $X_{23}$
					\end{array}\right]
					\left[\begin{array}{C{1.5em} C{1.5em}}
						$Y_{11}$ & $Y_{12}$ \\
						$Y_{21}$ & $Y_{22}$ \\
						$Y_{31}$ & $Y_{32}$
					\end{array}\right] \\
				&= 	\left[\begin{array}{C{12em} C{12em}}
						$X_{11}Y_{11} + X_{12}Y_{21} + X_{13}Y_{31}$ & $X_{11}Y_{12} + X_{12}Y_{22} + X_{13}Y_{32}$ \\
						$X_{21}Y_{11} + X_{22}Y_{21} + X_{23}Y_{31}$ & $X_{21}Y_{12} + X_{22}Y_{22} + X_{23}Y_{32}$ \\
					\end{array}\right]
		\end{align}
	\end{itemize}
\end{frame}


% Matrix Inversion
\begin{frame}{Matrix Inversion}{}
	\begin{itemize}
		\item Matrix inversion is defined for \textbf{square matrices} $\bm{X} \in \mathbb{R}^{n \times n}$
		\item A matrix $\bm{X}$ multiplied by its inverse $\bm{X}^{-1}$ gives the \highlight{identity matrix}:
		\begin{align}
			\bm{X}^{-1} \bm{X}
				&= \bm{X} \bm{X}^{-1} = \bm{I} \\
			\bm{I}
				&= \left[\begin{array}{C{1em} C{1em} C{1em} C{1em}}
					$1$ 		& 	$0$ 		& 	$\hdots$ & 	$0$ 		\\
					$0$ 		& 	$1$ 		& 	$\hdots$ & 	$0$ 		\\
					$\vdots$ & 	$\vdots$	& 	$\ddots$ & 	$\vdots$	\\
					$0$ 		& 	$0$ 		& 	$\hdots$ & 	$1$
				\end{array}\right]
		\end{align}
		\item If $\bm{X}^{-1}$ exists, we say that $\bm{X}$ is \highlight{non-singular}
	\end{itemize}
\end{frame}


% Matrix Inversion (Ctd.)
\begin{frame}{Matrix Inversion (Ctd.)}{}
	\begin{itemize}
		\item It holds that ($\bm{C}$ is the \highlight{cofactor matrix}):
		\begin{equation}
			\bm{X}^{-1} = \frac{1}{\text{det}(\bm{X})} \bm{C}^{\intercal}
		\end{equation}
		\item A condition for invertability is that \textbf{the determinant has to be different than zero}
		\item \textbf{Example:}
		\begin{equation*}
			\bm{X}
				=	\left[\begin{array}{C{1em} C{1em}}
						$1$ & $0$ \\
						$0$ & $0$
					\end{array}\right]
			\qquad\qquad
			\text{det}(\bm{X}) = 0
			\qquad\qquad
			\bm{X}^{-1} =\ ?
		\end{equation*}
	\end{itemize}
\end{frame}


% Matrix Inversion Example
\begin{frame}{Matrix Inversion Example}{}
	\begin{equation*}
		\bm{X}
			=	\left[\begin{array}{C{2em} C{2em}}
					$1$ 	& $\nicefrac{1}{2}$ \\
					$-1$	& $1$
				\end{array}\right]
		\qquad
		\bm{X}^{-1}
			=	\left[\begin{array}{C{2em} C{2em}}
					$\nicefrac{2}{3}$ 	& 	$-\nicefrac{1}{3}$ \\
					$\nicefrac{2}{3}$	& 	$\nicefrac{2}{3}$
				\end{array}\right]
	\end{equation*}

	Please verify!
	\begin{equation*}
		\bm{X} \bm{X}^{-1} = \bm{I}
			= 	\left[\begin{array}{C{1em} C{1em}}
					$1$ & $0$ \\
					$0$ & $1$
				\end{array}\right]
			= \bm{X}^{-1} \bm{X}
	\end{equation*}

	\begin{boxBlueNoFrame}
		\highlight{Use for example the Gauss-Jordan algorithm to find the inverse!}
	\end{boxBlueNoFrame}
\end{frame}


% Matrix Pseudoinverse
\begin{frame}{Matrix Pseudoinverse}{}
	\begin{itemize}
		\item \textbf{Question:} How can we invert a matrix $\bm{X} \in \mathbb{R}^{n \times m}$ which is not squared?
		\item \textbf{Left pseudoinverse} $\bm{X}^{\#} \bm{X}$:
		\begin{equation}
			\bm{X}^{\#} \bm{X}
				= \textcolor{myblue1}{\underbracket{
					(\bm{X}^{\intercal} \bm{X})^{-1} \bm{X}^{\intercal}
				}_{\text{left-multiplied}}} \bm{X} = \bm{I}_m
		\end{equation}
		\item \textbf{Right pseudoinverse} $\bm{X} \bm{X}^{\#}$:
		\begin{equation}
			\bm{X} \bm{X}^{\#}
				= \bm{X} \textcolor{myblue1}{\underbracket{
					\bm{X}^{\intercal} (\bm{X} \bm{X}^{\intercal})^{-1}
				}_{\text{right-multiplied}}} = \bm{I}_n
		\end{equation}
	\end{itemize}
\end{frame}


% Subsection: Eigenvectors and Eigenvalues
% --------------------------------------------------------------------------------------------------------
\subsection{Eigenvectors and Eigenvalues}

% Eigenvectors and Eigenvalues
\begin{frame}{Eigenvectors and Eigenvalues}{}
	\begin{itemize}
		\item Some vectors $\bm{v}$ only change their length when multiplied by a matrix $\bm{X}$
	\end{itemize}
\end{frame}


% Subsection: Miscellaneous
% --------------------------------------------------------------------------------------------------------
\subsection{Miscellaneous}

% Symmetric Matrices
\begin{frame}{Symmetric Matrices}{}
	\begin{itemize}
		\item A squared $n \times n$ matrix $\bm{X}$ is \highlight{symmetric}, iff
		\begin{align}
			\forall i, j: \qquad X_{ij}
				&= X_{ji} \\
			\bm{X}
				&= \bm{X}^{\intercal}
		\end{align}
		\item Some properties:
		\begin{itemize}
			\item The inverse $\bm{X}^{-1}$ is also symmetric
			\item \highlight{Eigen-decomposition:} $\bm{X}$ can be decomposed into $\bm{X} = \bm{Q} \bm{D} \bm{Q}^{\intercal}$,
				where the columns of $\bm{Q}$ are the eigenvectors of $\bm{X}$, and $\bm{D}$ is a diagonal matrix whose
				entries are the corresponding eigenvalues
		\end{itemize}
	\end{itemize}
\end{frame}


% Positive (semi-)definite Matrices
\begin{frame}{Positive (semi-)definite Matrices}{}
	\begin{itemize}
		\item A \textbf{squared symmetric} matrix $\bm{X}^{n \times n}$ is \highlight{positive definite},
			iff for any vector $\bm{y} \in \mathbb{R}^n$:
		\begin{equation}
			\bm{y}^{\intercal} \bm{X} \bm{y} > 0
		\end{equation}
		\item Or \highlight{positive semi-definite}, iff $\bm{y}^{\intercal} \bm{X} \bm{y} \ge 0$
	\end{itemize}
	
	\vspace*{3mm}
	\begin{boxBlueNoFrame}
		Such matrices are important in machine learning. For instance, the covariance matrix is always positive semi-definite.
	\end{boxBlueNoFrame}
\end{frame}


% Section: Statistics
%______________________________________________________________________
\section{Statistics}
\makedivider{Statistics}


% Subsection: Random Variables and Common Distributions
% --------------------------------------------------------------------------------------------------------
\subsection{Random Variables and Common Distributions}

% Random Variables
\begin{frame}{Random Variables}{}
	\begin{itemize}
		\item What is a \highlight{random variable}? \pause
		\begin{itemize}
			\item It's a random number determined by chance (according to a distribution)
			\item Random variables in machine learning: input data, output data, noise
		\end{itemize}
		\item What is a \highlight{probability distribution}? \pause
		\begin{itemize}
			\item Describes the probability that a random variable is equal to a certain value
			\item It can be given by the physics of an experiment (e.\,g. throwing dice)
			\item \highlight{Discrete} vs. \highlight{continuous} distributions
		\end{itemize}
	\end{itemize}
\end{frame}


% Uniform Distribution
\begin{frame}{Uniform Distribution}{}
	\input{02_math/01_tikz/uniform_distribution}
	
	\begin{center}
		Every outcome is equally probable within a bounded region $\mathcal{R}$
		\begin{equation}
			p(x) = \nicefrac{1}{\mathcal{R}}
		\end{equation}
	\end{center}
\end{frame}


% Discrete Distributions
\begin{frame}{Discrete Distributions}{}
	\begin{boxBlueNoFrame}
		The random variables take on \textbf{discrete values}
	\end{boxBlueNoFrame}

	\textbf{Examples:}
	\begin{itemize}
		\item When throwing a die, the possible values are given by a countably finite set:
		\begin{equation*}
			x_i \in \{ 1, 2, 3, 4, 5, 6 \}
		\end{equation*}
		\item The number of sand grains at the beach (countably infinite set):
		\begin{equation*}
			x_i \in \mathbb{N}
		\end{equation*}
	\end{itemize}
\end{frame}


% Discrete Distributions (Ctd.)
\begin{frame}{Discrete Distributions (Ctd.)}{}
	\begin{itemize}
		\item All probabilities sum up to 1:
		\begin{equation*}
			\sum_i p(x_i) = 1
		\end{equation*}
		\item Discrete distributions are particularly important in classification
		\item A discrete distribution is described by a \highlight{probability mass function} \\
			(also called frequency function)
	\end{itemize}
\end{frame}


% Bernoulli Distribution
\begin{frame}{Bernoulli Distribution}{}
	\begin{itemize}
		\item A \highlight{Bernoulli random variable} only takes on two values (e.\,g. 0 and 1):
		\begin{align}
			x
				&\in \{ 0, 1 \} 				\\
			p(x = 1 \vert \mu)
				&= \mu 					\\
			\text{Bern}(x \vert \mu)
				&= \mu^x (1 - \mu)^{1 - x} 	\\
			\mathbb{E}\{ x \}
				&= \mu 					\\
			\text{var}\{ x \}
				&= \mu (1 - \mu)
		\end{align}
		\item The only parameter is $\mu$, i.\,e. the distribution is completely defined by this parameter
	\end{itemize}
\end{frame}


% Binomial Distribution
\begin{frame}{Binomial Distribution}{}
	\begin{itemize}
		\item \highlight{Binomial variables} are a sequence of $n$ repeated Bernoulli variables
		\item \textbf{Example:} What is the probability of getting $m \in \mathbb{N}$ heads in $N$ trials?
		\begin{align}
			\text{Bin}(m \vert N, \mu)
				&= \binom{N}{m} \mu^m (1 - \mu)^{N - m}		\\[2mm]
			\mathbb{E}\{ m \}
				&= N \mu 								\\[2mm]
			\text{var}\{ m \}
				&= N \mu (1 - \mu)
		\end{align}
	\end{itemize}
\end{frame}


% Binomial Distribution (Ctd.)
\begin{frame}{Binomial Distribution (Ctd.)}{}
	\input{02_math/01_tikz/binomial_distribution}
\end{frame}


% Continuous Distributions
\begin{frame}{Continuous Distributions}{}
	\begin{boxBlueNoFrame}
		The random variables take on \textbf{continuous values}
	\end{boxBlueNoFrame}

	\begin{itemize}
		\item Continuous distributions are discrete distributions where the number of discrete values goes to infinity
		\item It's described by a \highlight{probability density function} which integrates to 1:
		\begin{equation*}
			\int_{-\infty}^{+\infty} p(x) \diff x = 1
		\end{equation*}
	\end{itemize}
\end{frame}


% Multivariate Gaussian
\begin{frame}{Multivariate Gaussian}{}
	\begin{figure}
		\centering
		\includegraphics[scale=0.35]{02_math/02_img/multivariate_gaussian}
	\end{figure}
\end{frame}


% Section: Optimization
%______________________________________________________________________
\section{Optimization}
\makedivider{Optimization}

% 
\begin{frame}{Motivation}{Every machine learning problem is an optimization problem!}
	\begin{itemize}
		\item In every machine learning problem, you will have:
		\begin{itemize}
			\item an objective function you want to optimize
			\item data you want to learn from
			\item parameters which need to be learned
			\item assumptions on your problem, your data and how the world works
		\end{itemize}
		\item Thus, we would like to have general solutions to the problem of learning
		\item Machine learning provides suitable objective functions for optimization based on the data,
			different models embody different objective functions and assumptions
	\end{itemize}
\end{frame}

\begin{frame}{Constrained Optimization}{How to formalize an optimization problem}
	\begin{align*}
		\underset{\theta}{\text{min }} J(\theta, D) &= \dots &\leftarrow\text{cost function / objective}\\
		\text{s.\,t. } f(\theta, D) &= 0 &\leftarrow\text{equality constraints}\\
		g(\theta, D) &\ge 0 &\leftarrow\text{inequality constraints}\\
	\end{align*}
	What should an ideal optimization problem, i.\,e. the cost function and constraints look like?
\end{frame}

\begin{frame}{Constrained Optimization}{How to formalize an optimization problem}
	\begin{align*}
		\underset{\theta}{\text{min }} J(\theta, D) &= \dots &\leftarrow\text{convex function}\\
		\text{s.\,t. } f(\theta, D) &= 0 &\leftarrow\text{linear function}\\
		g(\theta, D) &\ge 0 &\leftarrow\text{convex set}\\
	\end{align*}
\end{frame}

\begin{frame}{Cost Functions}{Which cost functions are there? Ideally, the cost function is convex}
	\divideTwo{0.49}{\centering \href{https://upload.wikimedia.org/wikipedia/commons/a/a3/Gradient_descent.gif}{non-convex}}{0.49}{\hspace{1cm}convex}\\
	\vspace{0.4cm}
	\includegraphics[width=0.9\textwidth]{02_math/02_img/costfunctions.png}
\end{frame}

\begin{frame}{Convexity}{Convex Sets}
	\begin{itemize}
		\item A set $C \subseteq \mathbb{R}^n$ is convex if for each $x, y \in C$ and any $\alpha \in [0,1]$, $\alpha x + (1 - \alpha)y \in C$. Examples are $\mathbb{R}^n$ and norm balls.\\
	\end{itemize}
	\begin{figure}
		\center
		\includegraphics[width=0.6\textwidth]{02_math/02_img/convexsets.png}
	\end{figure}
\end{frame}

\begin{frame}{Convexity}{Convex Functions}
	\begin{itemize}
		\item A function $f : \mathbb{R}^n \rightarrow \mathbb{R}$ is convex if for $x, y \in dom(f)$ and any $\alpha \in [0,1]$, $f(\alpha x + (1 - \alpha)y) \le \alpha f(x) + (1 - \alpha)f(y)$. Examples are linear functions $f(x) = w^Tx + b$ and quadratic functions $f(x) = x^TAx + b^Tx + c$.\\
	\end{itemize}
	\begin{figure}
		\center
		\includegraphics[width=0.6\textwidth]{02_math/02_img/convexfunction.png}
	\end{figure}
\end{frame}

\begin{frame}{Convexity}{Why are convex cost functions so nice?}
	\begin{itemize}
		\item Local solutions are global optima
		\item Efficient implementations of optimizers are available
	\end{itemize}
\end{frame}

\begin{frame}{Convexity}{How to recognize a convex function? Convexity conditions}
	\begin{itemize}
		\item First-order convexity condition:\\ Suppose $f : \mathbb{R}^n \rightarrow \mathbb{R}$ is differentiable. The function $f$ is convex iff $f(y) \ge f(x) + \nabla f(x)^T(y - x) \hspace{0.2cm}\forall x, y \in dom(f)$.
		\item Second-order convexity condition:\\ Suppose $f : \mathbb{R}^n \rightarrow \mathbb{R}$ is twice differentiable. The function $f$ is convex iff $\nabla^2 f(x) \ge 0 \vspace{0.2cm}\forall x \in dom(f)$.
	\end{itemize}
\end{frame}

\begin{frame}{Constrained Optimization}{How to solve an optimization problem with constraints}
	\begin{align*}
		\text{min } f(x,y) &= 2\cdot y + x\\
		\text{s.\,t. } 0 = g(x,y) &= y^2 +xy -1\\
	\end{align*}
	\begin{itemize}
		\item Convert the problem to an unconstrained one\\
		\item Introduce Lagrange multipliers
	\end{itemize}
\end{frame}

\begin{frame}{Constrained Optimization}{Lagrange multipliers}
	\divideTwo{0.49}{
		\begin{align*}
			\text{min } f(x,y) &= 2\cdot y + x\\
			\text{s.\,t. } 0 = g(x,y) &= y^2 +xy -1\\
		\end{align*}
	}{0.49}{
		\begin{align*}
			L(x,y,\lambda) &= f(x,y) + \lambda g(x,y)\\
			\frac{\partial L}{\partial x} &= 1 + \lambda y\\
			\frac{\partial L}{\partial y} &= 2 + 2\lambda y + \lambda x\\
			\frac{\partial L}{\partial \lambda} &= y^2 + xy - 1\\
		\end{align*}
	}
\end{frame}

\begin{frame}{Constrained Optimization}{Lagrange multipliers}
	\divideTwo{0.49}{
		\begin{align*}
			\text{I.    }0 &= 1 + \lambda y\\
			\text{II.   }0 &= 2 + 2\lambda y + \lambda x\\
			\text{III.  }0 &= y^2 + xy - 1\\
		\end{align*}
	}{0.49}{
		\begin{align*}
			\text{I.  }\lambda &= -\frac{1}{y}\\
			\text{I.} \rightarrow \text{II.  }x &= 0\\
			\text{III. }y &= \pm 1
		\end{align*}
	}
\end{frame}

\begin{frame}{Numerical Optimization}{What to do if we cannot solve it analytically?}
	\begin{itemize}
		\item Different numerical optimization algorithms exist for optimizing a function numerically on a computer if we can't solve it analytically
		\item Many approaches incrementally update an estimate $\theta_{new} := \theta_{old} + \alpha \delta\theta$ of the optimal parameters, so that after each update $J(\theta_{new}) < J(\theta_{old})$
		\item The challenge is to find the right step size $\alpha$ and direction $\delta\theta$
		\item Different algorithms differ in the number of required iterations, the computational cost per iteration, the convergence guarantees, the robustness with noisy cost functions and their memory usage
	\end{itemize}
\end{frame}

\begin{frame}{Numerical Optimization}{Optimization algorithms}
	\begin{itemize}
		\item There are various approaches to numerical optimization
		\item Gradient-based methods require differentiable functions and not too many iterations, but only guarantee to find a local optimum, examples are:
		\begin{itemize}
			\item Gradient Descent (with constant, variable or line-search-optimized step size)
			\item (L-)BFGS
			\item Conjugate Gradient Descent
		\end{itemize}
		\item Non-gradient based methods may find a global optimum, but require a large number of steps, examples are:
		\begin{itemize}
			\item Genetic Algorithms
			\item Non-Linear Simplex
			\item Nelder-Mead
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Numerical Optimization}{There are many other things you have to consider}
Initialization also matters...
\includegraphics[width=0.75\textwidth]{02_math/02_img/gradientascent.png}
\end{frame}

\begin{frame}{Want to learn more about optimization?}{Every machine learning problem is an optimization problem!}
\begin{itemize}
\item Deep Learning book chapters 4.3, 4.4 and 8 (\href{https://www.deeplearningbook.org/contents/numerical.html}{Link chapters 4.3, 4.4}, \href{https://www.deeplearningbook.org/contents/optimization.html}{Link chapter 8}) are highly recommended
\item Boyd \& Vandenberghe, Convex Optimization (\href{http://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf}{Link})
\item Stanford convex optimization course (\href{https://web.stanford.edu/class/ee364a/lectures.html}{Link})
\item MOOC on constrained optimization (\href{https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/lagrange-multipliers-and-constrained-optimization/v/constrained-optimization-introduction}{Link})
\end{itemize}
\end{frame}


% Section: Wrap-Up
%______________________________________________________________________
\section{Wrap-Up}
\makedivider{Wrap-Up}

% Subsection: Summary
% --------------------------------------------------------------------------------------------------------
\subsection{Summary}

% Summary
\begin{frame}{Summary}{}
	\begin{itemize}
		\item
	\end{itemize}
\end{frame}


% Subsection: Self-Test Questions
% --------------------------------------------------------------------------------------------------------
\subsection{Self-Test Questions}

% Self-Test Questions
\begin{frame}{Self-Test Questions}{}\important
	\begin{enumerate}
		\item 
	\end{enumerate}
\end{frame}


% Subsection: Lecture Outlook
% --------------------------------------------------------------------------------------------------------
\subsection{Lecture Outlook}

\begin{frame}{What's next...?}{}
	\makeoverview{3}
\end{frame}


% Subsection: Recommended Literature and further Reading
% --------------------------------------------------------------------------------------------------------
\subsection{Recommended Literature and further Reading}

% Literature
%______________________________________________________________________
\begin{frame}{Recommended Literature and further Reading}{}
	\footnotesize
	\begin{thebibliography}{2}

	\end{thebibliography}
\end{frame}


% Thank you
%______________________________________________________________________
\makethanks

\end{document}