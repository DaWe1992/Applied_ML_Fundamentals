\input{preamble_theme}

% ====================================================
% ====================================================
% PRESENTATION DATA
% ====================================================
% ====================================================

\title[Probability Density Estimation]{*** Applied Machine Learning Fundamentals *** Probability Density Estimation (PDE)}
\institute{SAP\,SE}
\author{Daniel Wehner}
\date{\today}
\prefix{PDE}

% ====================================================
% ====================================================
% BEGIN OF DOCUMENT
% ====================================================
% ====================================================

\begin{document}

% Title frame
%______________________________________________________________________
\maketitlepage


% Lecture Overview
%______________________________________________________________________
\begin{frame}{Lecture Overview}{}
	\makeoverview{4}
\end{frame}


% Agenda
%______________________________________________________________________
\begin{frame}{Agenda \today}
	\begin{multicols}{2}
		\tableofcontents
	\end{multicols}
\end{frame}


% Section: Introduction
%______________________________________________________________________
\section{Introduction}
\makedivider{Introduction}

% Subsection: 
% --------------------------------------------------------------------------------------------------------
\subsection{What about continuous Data?}

% Probability Density Estimation (PDE)
\begin{frame}{Probability Density Estimation (PDE)}{}
	\begin{itemize}
		\item We have learned about Bayes' optimal classifiers which classify data based on the probability distribution
			$p(\bm{x} \vert \mathcal{C}_k) \cdot p(\mathcal{C}_k)$
		\item Na\"{i}ve Bayes is an instance of PDE for \textbf{discrete data}
		\item \highlight{How to get these probabilities in the continuous case?}
		\begin{itemize}
			\item The prior $p({\mathcal{C}_k})$ is still easy to compute
			\item The estimation of class conditional probabilities $p(\bm{x} \vert \mathcal{C}_k)$ is more complicated
			\item Assume labeled data; estimate the density separately for each class $\mathcal{C}_k$
		\end{itemize}
		\item NB: For ease of notation: $p(x) \equiv p(x \vert \mathcal{C}_k)$
	\end{itemize}
\end{frame}


% Training Data Example
\begin{frame}{Training Data Example}{}
	\begin{figure}
		\centering
		\includegraphics[scale=0.55]{04_density_estimation/02_img/pde_data_raw}
	\end{figure}
\end{frame}


% Subsection: 
% --------------------------------------------------------------------------------------------------------
\subsection{Methods for PDE}

% Overview of the Methods for PDE
\begin{frame}{Overview of the Methods for PDE}{}
	\begin{enumerate}
		\item \highlight{Parametric models} (maximum likelihood estimation)
		\begin{itemize}
			\item Assume a fixed parametric form (e.\,g. a Gaussian distribution)
			\item Estimate the parameters such that the model fits the data best
		\end{itemize}
		\item \highlight{Non-parametric models}
		\begin{itemize}
			\item Often we do not know the functional form of the density
			\item Estimate probability directly from the data without an explicit model
		\end{itemize}
		\item \highlight{Mixture models}
		\begin{itemize}
			\item Combination of \ding{182} and \ding{183}
			\item EM algorithm
		\end{itemize}
	\end{enumerate}
\end{frame}


% Section: Parametric Models
%______________________________________________________________________
\section{Parametric Models}
\makedivider{Parametric Models}

% Subsection: General Idea
% --------------------------------------------------------------------------------------------------------
\subsection{General Idea}

% General Approach
\begin{frame}{General Approach}{}
	\begin{itemize}
		\item Given some (continuous) training data $\bm{X} = \{ x^{(i)} \}_{i=1}^n$ \\
			(where all $x^{(i)}$ belong to the same class):
		\begin{figure}
			\centering
			\includegraphics[scale=0.3]{04_density_estimation/02_img/pde_data_1d}
		\end{figure}
		\item Estimate $p(x)$ using a fixed parametric form:
		\begin{figure}
			\centering
			\includegraphics[scale=0.3]{04_density_estimation/02_img/pde_result_1d}
		\end{figure}
	\end{itemize}
\end{frame}


% Example: Gaussian Distribution
\begin{frame}{Example: Gaussian Distribution}{}
	\bubble{11}{10}{
		\footnotesize $\mu\ \widehat{=}$ mean \\
		\footnotesize $\sigma^2\ \widehat{=}$ variance
	}
	\begin{itemize}
		\item One common case is the \highlight{Gaussian distribution}:
		\begin{equation}
			p(x \vert \mu, \sigma^2) = \mathcal{N}(x \vert \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}}
				\exp\left\{ -\frac{(x - \mu)^2}{2 \sigma^2} \right\}
		\end{equation}
		\item Notation for parametric models:
		\begin{itemize}
			\item $p(x \vert \bm{\theta})$
			\item In the case of a Gaussian: $\bm{\theta} = \{ \mu, \sigma^2 \}$
		\end{itemize}
	\end{itemize}
\end{frame}


% Subsection: Parameter Learning and Assumptions
% --------------------------------------------------------------------------------------------------------
\subsection{Parameter Learning and Assumptions}

% Learning the Parameters
\begin{frame}{Learning the Parameters}{}
	\begin{itemize}
		\item Learning = Estimation of the parameters $\bm{\theta}$ given the data $\bm{X}$
		\item \highlight{Likelihood} of the parameters $\bm{\theta}$:
		\begin{itemize}
			\item Is defined as the probability that $\bm{X}$ was generated by a
				probability density function (pdf) with parameters $\bm{\theta}$
			\begin{equation}
				\mathcal{L}(\bm{\theta}) = p(\bm{X} \vert \bm{\theta})
			\end{equation}
			\item We want to \textbf{maximize} the likelihood
		\end{itemize}
	\end{itemize}
	
	\begin{boxBlueNoFrame}
		\highlight{$\Rightarrow$ Maximum likelihood estimation (MLE)}
	\end{boxBlueNoFrame}
\end{frame}


% A fundamental Assumption
\begin{frame}{A fundamental Assumption}{}
	\begin{itemize}
		\item How to compute $\mathcal{L}(\bm{\theta})$?
		\item The data is assumed to be \highlight{i.\,i.\,d.} (independent and identically distributed):
		\begin{itemize}
			\item Two random variables $x_1$ and $x_2$ are independent if
			\begin{equation}
				P(x_1 \le \alpha, x_2 \le \beta) = P(x_1 \le \alpha) \cdot P(x_2 \le \beta) \qquad \forall \alpha, \beta \in \mathbb{R}
			\end{equation}
			\item Two random variables $x_1$ and $x_2$ are identically distributed if
			\begin{equation}
				P(x_1 \le \alpha) = P(x_2 \le \alpha) \qquad \forall \alpha \in \mathbb{R}
			\end{equation}
		\end{itemize}
	\end{itemize}
\end{frame}


% Subsection: Maximum Likelihood Estimation (MLE)
% --------------------------------------------------------------------------------------------------------
\subsection{Maximum Likelihood Estimation (MLE)}

% Computation of the Likelihood
\begin{frame}{Computation of the Likelihood}{}\important
	\bubble{9}{12.75}{
		\footnotesize What is the problem here?
	}
	\vspace*{-2mm}
	\begin{align}
		\mathcal{L}(\bm{\theta})
			&= p(\bm{X} \vert \bm{\theta}) \nonumber \\[2mm]
			&= p(x^{(1)}, x^{(2)}, \dots, x^{(n)} \vert \bm{\theta}) \nonumber
			\intertext{\footnotesize data is independent:}
			&= p(x^{(1)} \vert \bm{\theta}) \cdot p(x^{(2)} \vert \bm{\theta}) \cdot \ldots \cdot p(x^{(n)} \vert \bm{\theta}) \nonumber
			\intertext{\footnotesize data is identically distributed:} 
			&= \prod_{i=1}^n p(x^{(i)} \vert \bm{\theta})
	\end{align}
\end{frame}


% Computation of the Likelihood (Ctd.)
\begin{frame}{Computation of the Likelihood (Ctd.)}{}
	\bubble{11.5}{7.5}{
		\footnotesize Why is this an \\[-1mm]
		\footnotesize allowed transformation?
	}
	\bubble{4}{11}{
		$\log \Pi = \Sigma \log$
	}
	\begin{itemize}
		\item \Highlight{Problem:} Large $n$ might cause arithmetic underflows! \textbf{(why?)}
		\item Transform the likelihood using the logarithm \highlight{$\Rightarrow$ log-likelihood}
		\begin{align}
			\mathcal{L}\mathcal{L}(\bm{\theta})
				&= \log \mathcal{L}(\bm{\theta}) \nonumber \\[2mm]
				&= \log \prod_{i=1}^n p(x^{(i)} \vert \bm{\theta}) \nonumber \\[2mm]
				&= \sum_{i=1}^n \log p(x^{(i)} \vert \bm{\theta})
		\end{align}
	\end{itemize}
\end{frame}


% Maximum Likelihood of a Gaussian
\begin{frame}{Maximum Likelihood of a Gaussian}{}
	\begin{itemize}
		\item $\bm{\theta} = \{ \mu, \sigma^2 \}$
		\begin{align*}
			\mathcal{L}\mathcal{L}(\{ \mu, \sigma^2 \})
				&= \sum_{i=1}^n \log \mathcal{N}(x^{(i)} \vert \mu, \sigma^2) \\
				&= \sum_{i=1}^n \log \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left\{ -\frac{(x^{(i)} - \mu)^2}{2 \sigma^2} \right\}
		\end{align*}
		\item Find $\mu_{ml}$ and $\sigma_{ml}^2$ which maximize the log-likelihood:
		\begin{equation*}
			\mu_{ml}, \sigma_{ml}^2 = \argmax_{\mu, \sigma^2} \mathcal{L}\mathcal{L}(\bm{\theta})
		\end{equation*}
	\end{itemize}
\end{frame}


% Maximum Likelihood of a Gaussian (Ctd.)
\begin{frame}{Maximum Likelihood of a Gaussian (Ctd.)}{}
	\begin{itemize}
		\item Compute the partial derivatives with respect to the parameters $\bm{\theta}$
		\item Derivative w.\,r.\,t. $\mu$:
		\begin{equation*}
			\nabla_{\mu}\mathcal{L}\mathcal{L}(\bm{\theta})
				= \nabla_{\mu} \sum_{i=1}^n \log \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left\{ -\frac{(x^{(i)} - \mu)^2}{2 \sigma^2} \right\}
				= \sum_{i=1}^n \frac{x^{(i)} - \mu}{\sigma^2}
		\end{equation*}
		\item Set derivative to zero and solve:
		\begin{equation*}
			\sum_{i=1}^n x^{(i)} - \mu \overset{!}{=} 0
				\Leftrightarrow n \cdot \mu = \sum_{i=1}^n x^{(i)}
				\Leftrightarrow \mu = \frac{1}{n} \sum_{i=1}^n x^{(i)}
 		\end{equation*}
	\end{itemize}
\end{frame}


% Maximization of the Likelihood
\begin{frame}{Maximization of the Likelihood}{}
	\begin{figure}
		\centering
		\includegraphics[scale=0.55]{04_density_estimation/02_img/maximum_likelihood}
	\end{figure}
\end{frame}


% We can classify!
\begin{frame}{We can classify!}{}
	\bubble{11}{4.5}{
		\footnotesize Looks familiar?
	}
	\begin{itemize}
		\item Maximum likelihood parameters:
		\begin{equation*}
			\mu_{ml} =  \frac{1}{n} \sum_{i=1}^n x^{(i)}
			\qquad\qquad
			\sigma_{ml}^2 = \frac{1}{n} \sum_{i=1}^n (x^{(i)} - \mu_{ml})^2
		\end{equation*}
		\item Now we can use Bayes' rule to predict class labels
		\begin{itemize}
			\item We have the priors...
			\item ...and the class conditionals
		\end{itemize}
		\item Also, the \highlight{decision boundary} can be computed
	\end{itemize}
\end{frame}


% Multivariate Case
\begin{frame}{Multivariate Case}{}
	\begin{itemize}
		\item The solution above is for 1-D data, what if we have more dimensions?
		\item \highlight{Multivariate Gaussian distribution}:
		\begin{equation}
			\mathcal{N}_D(\bm{x} \vert \bm{\mu}, \bm{\Sigma})
				= \frac{1}{\sqrt{(2 \pi)^D \vert \bm{\Sigma} \vert}}
					\exp\left\{ -\frac{1}{2} (\bm{x} - \bm{\mu})^{\intercal} \bm{\Sigma}^{-1} (\bm{x} - \bm{\mu}) \right\}
		\end{equation}
		\item Luckily, the derivations don't change:
		\begin{equation}
			\bm{\mu}_{ml} = \frac{1}{n} \sum_{i=1}^n \bm{x}^{(i)}
			\qquad
			\bm{\Sigma}_{ml} = \frac{1}{n} \sum_{i=1}^n (\bm{x}^{(i)} - \bm{\mu}_{ml}) (\bm{x}^{(i)} - \bm{\mu}_{ml})^{\intercal}
		\end{equation}
	\end{itemize}
\end{frame}


% MLE for the Example Data Set
\begin{frame}{MLE for the Example Data Set}{}
	\begin{figure}
		\centering
		\includegraphics[scale=0.55]{04_density_estimation/02_img/pde_boundary}
	\end{figure}
\end{frame}


% Section: Non-parametric Models
%______________________________________________________________________
\section{Non-parametric Models}
\makedivider{Non-parametric Models}

% Subsection: 
% --------------------------------------------------------------------------------------------------------
\subsection{}

% Disadvantages of parametric Models
\begin{frame}{Disadvantages of parametric Models}{}
	\begin{itemize}
		\item Until now we used a fixed parametric form (e.\,g. a Gaussian) which is governed by a small amount of parameters
		\item \Highlight{This assumption may be wrong:}
		\begin{itemize}
			\item Another distribution (exponential, gamma, ...) may fit better
			\item A suitable 'text-book distribution' may not exist
		\end{itemize}
	\end{itemize}
	
	\begin{boxBlueNoFrame}
		\highlight{We don't want to make any assumptions about the underlying distribution!}
	\end{boxBlueNoFrame}
\end{frame}


% Non-parametric Approaches
\begin{frame}{Non-parametric Approaches}{}
	\begin{enumerate}
		\item \highlight{Histograms} (Binning)
		\item \highlight{Kernel density estimation} (KDE)
		\item \highlight{Nearest neighbors} (kNN)
	\end{enumerate}
\end{frame}


% Histograms
\begin{frame}{Histograms}{}
	\begin{itemize}
		\item Histograms partition the data $\bm{X} = \{ \bm{x}^{(i)} \}_{i=1}^n$ into distinct \textbf{bins} of volume $v_j$...
		\item ...and subsequently count the number of instances $k_j$ falling into the $j$-th bin
		\item Approximate the probability $p(\bm{x})$ by:
		\begin{equation}
			p(\bm{x}) \approx \frac{k_j}{n \cdot v_j}\qquad \text{for $\bm{x}$ in bin $j$}
		\end{equation}
		\item The sum of all probabilities equals 1: $\sum_{j=1}^m \frac{k_j}{n \cdot v_j} = 1$
		\item $v_j$ is a \textbf{hyper-parameter} (usually, all bins have equal size) 
	\end{itemize}
\end{frame}


% Histograms (Ctd.)
\begin{frame}{Histograms (Ctd.)}{}
	\begin{minipage}{0.32\framewidth}
		\begin{center}\highlight{Too narrow}\end{center}
		\includegraphics[scale=0.3]{04_density_estimation/02_img/histo_small}
	\end{minipage}
	\hfill
	\begin{minipage}{0.32\framewidth}
		\begin{center}\highlight{About right}\end{center}
		\includegraphics[scale=0.3]{04_density_estimation/02_img/histo_medium}
	\end{minipage}
	\hfill
	\begin{minipage}{0.32\framewidth}
		\begin{center}\highlight{Too wide}\end{center}
		\includegraphics[scale=0.3]{04_density_estimation/02_img/histo_large}
	\end{minipage}
\end{frame}


% Drawbacks of Histograms
\begin{frame}{Drawbacks of Histograms}{}
	\divideTwo{0.55}{
		\begin{itemize}
			\item Histograms are mostly unsuited for many applications
			\item \Highlight{Drawbacks:}
			\begin{enumerate}
				\item \textbf{Discontinuities} due to bin edges
				\item Number of bins \textbf{explodes} with growing number of dimensions $D$
			\end{enumerate}
		\end{itemize}
	}{0.44}{
		\includegraphics[scale=0.3]{04_density_estimation/02_img/curse_of_dimensionality}
	}
	
	\vspace*{4mm}
	\begin{boxBlueNoFrame}
		\highlight{The latter issue is known as the curse of dimensionality}
	\end{boxBlueNoFrame}
\end{frame}


% An alternative Approach
\begin{frame}{An alternative Approach}{}
	\begin{itemize}
		\item Don't use a fixed number of pre-determined bins
		\item Instead, employ a \textbf{sliding window} approach by centering a region $\mathcal{R}$ (bin)
			around the data point of interest $\bm{x}$
		\begin{equation}
			p(\bm{x}) \approx \frac{k}{n \cdot v}
		\end{equation}
		\item This gives rise to two different techniques:
		\begin{enumerate}
			\item \highlight{Kernel density estimation} (Fix $v$ and determine $k$)
			\item \highlight{k-nearest neighbors} (Fix $k$ and determine $v$)
		\end{enumerate}
	\end{itemize}
\end{frame}


% Kernel Density Estimation: Parzen Window
\begin{frame}{Kernel Density Estimation: Parzen Window}{}
	\begin{itemize}
		\item $\mathcal{R}$ is a $D$-dimensional \highlight{hyper-cube} of edge length $h$ centered on $\bm{x}$
		\item Determine if a data point falls into region $\mathcal{R}$:
		\begin{equation}
			H(\bm{u}) = \begin{cases} 
				1\quad \text{if}\ \vert u_d \vert \le \nicefrac{h}{2}, d = 1, 2, \dots, D \\
				0\quad \text{otherwise}
			\end{cases}
		\end{equation}
		\item The total number of data points falling into region $\mathcal{R}$ is given by:
		\begin{equation}
			k(\bm{x}) = \sum_{i=1}^n H(\bm{x} - \bm{x}^{(i)})
		\end{equation}
	\end{itemize}
\end{frame}


% Kernel Density Estimation: Parzen Window (Ctd.)
\begin{frame}{Kernel Density Estimation: Parzen Window (Ctd.)}{}
	\begin{itemize}
		\item The volume $v$ is simple to compute:
		\begin{equation}
			v = \int H(\bm{u}) \diff \bm{u} = h^D
		\end{equation}
		\item Putting it all together we get:
		\begin{equation}
			p(\bm{x}) \approx \frac{k(\bm{x})}{n \cdot v} = \frac{1}{n \cdot h^D} \sum_{i=1}^n H(\bm{x} - \bm{x}^{(i)})
		\end{equation}
	\end{itemize}
\end{frame}


% Kernel Density Estimation: Parzen Window (Ctd.)
\begin{frame}{Kernel Density Estimation: Parzen Window (Ctd.)}
	\begin{figure}
		\centering
		\includegraphics[scale=0.55]{04_density_estimation/02_img/kde}
	\end{figure}
\end{frame}


% k-Nearest Neighbors
\begin{frame}{k-Nearest Neighbors}
	\begin{itemize}
		\item 
	\end{itemize}
\end{frame}


% k-Nearest Neighbors (Ctd.)
\begin{frame}{k-Nearest Neighbors (Ctd.)}
	\begin{figure}
		\centering
		\includegraphics[scale=0.55]{04_density_estimation/02_img/knn}
	\end{figure}
\end{frame}


% Section: Mixture Models
%______________________________________________________________________
\section{Mixture Models}
\makedivider{Mixture Models}

% Subsection: 
% --------------------------------------------------------------------------------------------------------
\subsection{}

% 
\begin{frame}{}{}

\end{frame}



% Section: Wrap-Up
%______________________________________________________________________
\section{Wrap-Up}
\makedivider{Wrap-Up}

% Subsection: Summary
% --------------------------------------------------------------------------------------------------------
\subsection{Summary}

% Summary
\begin{frame}{Summary}{}

\end{frame}


% Subsection: Self-Test Questions
% --------------------------------------------------------------------------------------------------------
\subsection{Self-Test Questions}

% Self-Test Questions
\begin{frame}{Self-Test Questions}{}\important

\end{frame}


% Subsection: Lecture Outlook
% --------------------------------------------------------------------------------------------------------
\subsection{Lecture Outlook}

\begin{frame}{What's next...?}{}
	\makeoverview{5}
\end{frame}


% Subsection: Recommended Literature and further Reading
% --------------------------------------------------------------------------------------------------------
\subsection{Recommended Literature and further Reading}

% Literature
%______________________________________________________________________
\begin{frame}{Recommended Literature and further Reading}{}
	\footnotesize
	\begin{thebibliography}{2}

	\end{thebibliography}
\end{frame}


% Thank you
%______________________________________________________________________
\makethanks

\end{document}