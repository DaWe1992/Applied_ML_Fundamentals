\input{preamble_theme}

% ====================================================
% ====================================================
% PRESENTATION DATA
% ====================================================
% ====================================================

\title[Probability Density Estimation]{*** Applied Machine Learning Fundamentals *** Probability Density Estimation (PDE)}
\institute{SAP\,SE}
\author{Clemens Biehl, Daniel Wehner}
\date{\today}
\prefix{PDE}

% ====================================================
% ====================================================
% BEGIN OF DOCUMENT
% ====================================================
% ====================================================

\begin{document}

% Title frame
%______________________________________________________________________
\maketitlepage


% Lecture Overview
%______________________________________________________________________
\begin{frame}{Lecture Overview}{}
	\makeoverview{4}
\end{frame}


% Agenda
%______________________________________________________________________
\begin{frame}{Agenda \today}
	\begin{multicols}{2}
		\tableofcontents
	\end{multicols}
\end{frame}


% Section: Introduction
%______________________________________________________________________
\section{Introduction}
\makedivider{Introduction}

% Subsection: 
% --------------------------------------------------------------------------------------------------------
\subsection{What about continuous Data?}

% Probability Density Estimation (PDE)
\begin{frame}{Probability Density Estimation (PDE)}{}
	\begin{itemize}
		\item We have learned about Bayes' optimal classifiers which classify data based on the probability distribution
			$p(\bm{x} \vert \mathcal{C}_k) \cdot p(\mathcal{C}_k)$
		\item Na\"{i}ve Bayes is an instance of PDE for \textbf{discrete data}
		\item \highlight{How to get these probabilities in the continuous case?}
		\begin{itemize}
			\item The prior $p({\mathcal{C}_k})$ is still easy to compute
			\item The estimation of class conditional probabilities $p(\bm{x} \vert \mathcal{C}_k)$ is more complicated
			\item Assume labeled data; estimate the density separately for each class $\mathcal{C}_k$
		\end{itemize}
		\item NB: For ease of notation: $p(x) \equiv p(x \vert \mathcal{C}_k)$
	\end{itemize}
\end{frame}


% Training Data Example
\begin{frame}{Training Data Example}{}
	\begin{figure}
		\centering
		\includegraphics[scale=0.55]{04_density_estimation/02_img/pde_data_raw}
	\end{figure}
\end{frame}


% Subsection: 
% --------------------------------------------------------------------------------------------------------
\subsection{Methods for PDE}

% Overview of the Methods for PDE
\begin{frame}{Overview of the Methods for PDE}{}
	\begin{enumerate}
		\item \highlight{Parametric models} (maximum likelihood estimation)
		\begin{itemize}
			\item Assume a fixed parametric form (e.\,g. a Gaussian distribution)
			\item Estimate the parameters such that the model fits the data best
		\end{itemize}
		\item \highlight{Non-parametric models}
		\begin{itemize}
			\item Often we do not know the functional form of the density
			\item Estimate probability directly from the data without an explicit model
		\end{itemize}
		\item \highlight{Mixture models}
		\begin{itemize}
			\item Combination of \ding{182} and \ding{183}
			\item EM algorithm
		\end{itemize}
	\end{enumerate}
\end{frame}


% Section: Parametric Models
%______________________________________________________________________
\section{Parametric Models}
\makedivider{Parametric Models}

% Subsection: General Idea
% --------------------------------------------------------------------------------------------------------
\subsection{General Idea}

% General Approach
\begin{frame}{General Approach}{}
	\begin{itemize}
		\item Given some (continuous) training data $\bm{X} = \{ x^{(i)} \}_{i=1}^n$ \\
			(where all $x^{(i)}$ belong to the same class):
		\begin{figure}
			\centering
			\includegraphics[scale=0.3]{04_density_estimation/02_img/pde_data_1d}
		\end{figure}
		\item Estimate $p(x)$ using a fixed parametric form:
		\begin{figure}
			\centering
			\includegraphics[scale=0.3]{04_density_estimation/02_img/pde_result_1d}
		\end{figure}
	\end{itemize}
\end{frame}


% Example: Gaussian Distribution
\begin{frame}{Example: Gaussian Distribution}{}
	\bubble{11}{10}{
		\footnotesize $\mu\ \widehat{=}$ mean \\
		\footnotesize $\sigma^2\ \widehat{=}$ variance
	}
	\begin{itemize}
		\item One common case is the \highlight{Gaussian distribution}:
		\begin{equation}
			p(x \vert \mu, \sigma^2) = \mathcal{N}(x \vert \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}}
				\exp\left\{ -\frac{(x - \mu)^2}{2 \sigma^2} \right\}
		\end{equation}
		\item Notation for parametric models:
		\begin{itemize}
			\item $p(x \vert \bm{\theta})$
			\item In the case of a Gaussian: $\bm{\theta} = \{ \mu, \sigma^2 \}$
		\end{itemize}
	\end{itemize}
\end{frame}


% Subsection: Parameter Learning and Assumptions
% --------------------------------------------------------------------------------------------------------
\subsection{Parameter Learning and Assumptions}

% Learning the Parameters
\begin{frame}{Learning the Parameters}{}
	\begin{itemize}
		\item Learning = Estimation of the parameters $\bm{\theta}$ given the data $\bm{X}$
		\item \highlight{Likelihood} of the parameters $\bm{\theta}$:
		\begin{itemize}
			\item Is defined as the probability that $\bm{X}$ was generated by a
				probability density function (pdf) with parameters $\bm{\theta}$
			\begin{equation}
				\mathcal{L}(\bm{\theta}) = p(\bm{X} \vert \bm{\theta})
			\end{equation}
			\item We want to \textbf{maximize} the likelihood
		\end{itemize}
	\end{itemize}
	
	\begin{boxBlueNoFrame}
		\highlight{$\Rightarrow$ Maximum likelihood estimation (MLE)}
	\end{boxBlueNoFrame}
\end{frame}


% A fundamental Assumption
\begin{frame}{A fundamental Assumption}{}
	\begin{itemize}
		\item How to compute $\mathcal{L}(\bm{\theta})$?
		\item The data is assumed to be \highlight{i.\,i.\,d.} (independent and identically distributed):
		\begin{itemize}
			\item Two random variables $x_1$ and $x_2$ are independent if
			\begin{equation}
				P(x_1 \le \alpha, x_2 \le \beta) = P(x_1 \le \alpha) \cdot P(x_2 \le \beta) \qquad \forall \alpha, \beta \in \mathbb{R}
			\end{equation}
			\item Two random variables $x_1$ and $x_2$ are identically distributed if
			\begin{equation}
				P(x_1 \le \alpha) = P(x_2 \le \alpha) \qquad \forall \alpha \in \mathbb{R}
			\end{equation}
		\end{itemize}
	\end{itemize}
\end{frame}


% Subsection: Maximum Likelihood Estimation (MLE)
% --------------------------------------------------------------------------------------------------------
\subsection{Maximum Likelihood Estimation (MLE)}

% Computation of the Likelihood
\begin{frame}{Computation of the Likelihood}{}\important
	\bubble{9}{12.75}{
		\footnotesize What is the problem here?
	}
	\vspace*{-2mm}
	\begin{align}
		\mathcal{L}(\bm{\theta})
			&= p(\bm{X} \vert \bm{\theta}) \nonumber \\[2mm]
			&= p(x^{(1)}, x^{(2)}, \dots, x^{(n)} \vert \bm{\theta}) \nonumber
			\intertext{\footnotesize data is independent:}
			&= p(x^{(1)} \vert \bm{\theta}) \cdot p(x^{(2)} \vert \bm{\theta}) \cdot \ldots \cdot p(x^{(n)} \vert \bm{\theta}) \nonumber
			\intertext{\footnotesize data is identically distributed:} 
			&= \prod_{i=1}^n p(x^{(i)} \vert \bm{\theta})
	\end{align}
\end{frame}


% Computation of the Likelihood (Ctd.)
\begin{frame}{Computation of the Likelihood (Ctd.)}{}
	\bubble{11.5}{7.5}{
		\footnotesize Why is this an \\[-1mm]
		\footnotesize allowed transformation?
	}
	\bubble{4}{11}{
		$\log \Pi = \Sigma \log$
	}
	\begin{itemize}
		\item \Highlight{Problem:} Large $n$ might cause arithmetic underflows! \textbf{(why?)}
		\item Transform the likelihood using the logarithm \highlight{$\Rightarrow$ log-likelihood}
		\begin{align}
			\mathcal{L}\mathcal{L}(\bm{\theta})
				&= \log \mathcal{L}(\bm{\theta}) \nonumber \\[2mm]
				&= \log \prod_{i=1}^n p(x^{(i)} \vert \bm{\theta}) \nonumber \\[2mm]
				&= \sum_{i=1}^n \log p(x^{(i)} \vert \bm{\theta})
		\end{align}
	\end{itemize}
\end{frame}


% Maximum Likelihood of a Gaussian
\begin{frame}{Maximum Likelihood of a Gaussian}{}
	\begin{itemize}
		\item $\bm{\theta} = \{ \mu, \sigma^2 \}$
		\begin{align}
			\mathcal{L}\mathcal{L}(\{ \mu, \sigma^2 \})
				&= \sum_{i=1}^n \log \mathcal{N}(x^{(i)} \vert \mu, \sigma^2) \\
				&= \sum_{i=1}^n \log \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left\{ -\frac{(x^{(i)} - \mu)^2}{2 \sigma^2} \right\}
		\end{align}
		\item Find $\mu_{ml}$ and $\sigma_{ml}^2$ which maximize the log-likelihood:
		\begin{equation*}
			\mu_{ml}, \sigma_{ml}^2 = \argmax_{\mu, \sigma^2} \mathcal{L}\mathcal{L}(\bm{\theta})
		\end{equation*}
	\end{itemize}
\end{frame}


% Maximum Likelihood of a Gaussian (Ctd.)
\begin{frame}{Maximum Likelihood of a Gaussian (Ctd.)}{}
	\begin{itemize}
		\item Compute the partial derivatives with respect to the parameters $\bm{\theta}$
		\item Derivative w.\,r.\,t. $\mu$:
		\begin{equation*}
			\nabla_{\mu}\mathcal{L}\mathcal{L}(\bm{\theta})
				= \nabla_{\mu} \sum_{i=1}^n \log \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left\{ -\frac{(x^{(i)} - \mu)^2}{2 \sigma^2} \right\}
				= \sum_{i=1}^n \frac{x^{(i)} - \mu}{\sigma^2}
		\end{equation*}
		\item Set derivative to zero and solve:
		\begin{equation*}
			\sum_{i=1}^n x^{(i)} - \mu \overset{!}{=} 0
				\Leftrightarrow n \cdot \mu = \sum_{i=1}^n x^{(i)}
				\Leftrightarrow \mu = \frac{1}{n} \sum_{i=1}^n x^{(i)}
 		\end{equation*}
	\end{itemize}
\end{frame}


% Maximization of the Likelihood
\begin{frame}{Maximization of the Likelihood}{}
	\begin{figure}
		\centering
		\includegraphics[scale=0.55]{04_density_estimation/02_img/maximum_likelihood}
	\end{figure}
\end{frame}


% We can classify!
\begin{frame}{We can classify!}{}
	\bubble{11}{4.5}{
		\footnotesize Looks familiar?
	}
	\begin{itemize}
		\item Maximum likelihood parameters:
		\begin{equation*}
			\mu_{ml} =  \frac{1}{n} \sum_{i=1}^n x^{(i)}
			\qquad\qquad
			\sigma_{ml}^2 = \frac{1}{n} \sum_{i=1}^n (x^{(i)} - \mu_{ml})^2
		\end{equation*}
		\item Now we can use Bayes' rule to predict class labels
		\begin{itemize}
			\item We have the priors...
			\item ...and the class conditionals
		\end{itemize}
		\item Also, the \highlight{decision boundary} can be computed
	\end{itemize}
\end{frame}


% Multivariate Case
\begin{frame}{Multivariate Case}{}
	\begin{itemize}
		\item The solution above is for 1-D data, what if we have more dimensions?
		\item \highlight{Multivariate Gaussian distribution}:
		\begin{equation}
			\mathcal{N}_D(\bm{x} \vert \bm{\mu}, \bm{\Sigma})
				= \frac{1}{\sqrt{(2 \pi)^D \vert \bm{\Sigma} \vert}}
					\exp\left\{ -\frac{1}{2} (\bm{x} - \bm{\mu})^{\intercal} \bm{\Sigma}^{-1} (\bm{x} - \bm{\mu}) \right\}
		\end{equation}
		\item Luckily, the derivations don't change:
		\begin{equation}
			\bm{\mu}_{ml} = \frac{1}{n} \sum_{i=1}^n \bm{x}^{(i)}
			\qquad
			\bm{\Sigma}_{ml} = \frac{1}{n} \sum_{i=1}^n (\bm{x}^{(i)} - \bm{\mu}_{ml}) (\bm{x}^{(i)} - \bm{\mu}_{ml})^{\intercal}
		\end{equation}
	\end{itemize}
\end{frame}


% MLE for the Example Data Set
\begin{frame}{MLE for the Example Data Set}{}
	\begin{figure}
		\centering
		\includegraphics[scale=0.55]{04_density_estimation/02_img/pde_boundary}
	\end{figure}
\end{frame}


% Section: Non-parametric Models
%______________________________________________________________________
\section{Non-parametric Models}
\makedivider{Non-parametric Models}

% Subsection: 
% --------------------------------------------------------------------------------------------------------
\subsection{}

% Disadvantages of parametric Models
\begin{frame}{Disadvantages of parametric Models}{}
	\begin{itemize}
		\item Until now we used a fixed parametric form (e.\,g. a Gaussian) which is governed by a small amount of parameters
		\item \Highlight{This assumption may be wrong:}
		\begin{itemize}
			\item Another distribution (exponential, gamma, ...) may fit better
			\item A suitable 'text-book distribution' may not exist
		\end{itemize}
	\end{itemize}
	
	\begin{boxBlueNoFrame}
		\highlight{We don't want to make any assumptions about the underlying distribution!}
	\end{boxBlueNoFrame}
\end{frame}


% Non-parametric Approaches
\begin{frame}{Non-parametric Approaches}{}
	\begin{enumerate}
		\item \highlight{Histograms} (Binning)
		\item \highlight{Kernel density estimation} (KDE)
		\item \highlight{Nearest neighbors} (kNN)
	\end{enumerate}
\end{frame}


% Histograms
\begin{frame}{Histograms}{}
	\begin{itemize}
		\item Histograms partition the data $\bm{X} = \{ \bm{x}^{(i)} \}_{i=1}^n$ into distinct \textbf{bins} of volume $v_j$...
		\item ...and subsequently count the number of instances $k_j$ falling into the $j$-th bin
		\item Approximate the probability $p(\bm{x})$ by:
		\begin{equation}
			p(\bm{x}) \approx \frac{k_j}{n \cdot v_j}\qquad \text{for $\bm{x}$ in bin $j$}
		\end{equation}
		\item The sum of all probabilities equals 1: $\sum_{j=1}^m \frac{k_j}{n \cdot v_j} = 1$
		\item $v_j$ is a \textbf{hyper-parameter} (usually, all bins have equal size) 
	\end{itemize}
\end{frame}


% Histograms (Ctd.)
\begin{frame}{Histograms (Ctd.)}{}
	\begin{minipage}{0.32\framewidth}
		\begin{center}\highlight{Too narrow}\end{center}
		\includegraphics[scale=0.3]{04_density_estimation/02_img/histo_small}
	\end{minipage}
	\hfill
	\begin{minipage}{0.32\framewidth}
		\begin{center}\highlight{About right}\end{center}
		\includegraphics[scale=0.3]{04_density_estimation/02_img/histo_medium}
	\end{minipage}
	\hfill
	\begin{minipage}{0.32\framewidth}
		\begin{center}\highlight{Too wide}\end{center}
		\includegraphics[scale=0.3]{04_density_estimation/02_img/histo_large}
	\end{minipage}
\end{frame}


% Drawbacks of Histograms
\begin{frame}{Drawbacks of Histograms}{}
	\divideTwo{0.55}{
		\begin{itemize}
			\item Histograms are mostly unsuited for many applications
			\item \Highlight{Drawbacks:}
			\begin{enumerate}
				\item \textbf{Discontinuities} due to bin edges
				\item Number of bins \textbf{explodes} with growing number of dimensions $D$
			\end{enumerate}
		\end{itemize}
	}{0.44}{
		\includegraphics[scale=0.3]{04_density_estimation/02_img/curse_of_dimensionality}
	}
	
	\vspace*{4mm}
	\begin{boxBlueNoFrame}
		\highlight{The latter issue is known as the curse of dimensionality}
	\end{boxBlueNoFrame}
\end{frame}


% An alternative Approach
\begin{frame}{An alternative Approach}{}
	\begin{itemize}
		\item Don't use a fixed number of pre-determined bins
		\item Instead, employ a \textbf{sliding window} approach by centering a region $\mathcal{R}$ (bin)
			around the data point of interest $\bm{x}$
		\begin{equation}
			p(\bm{x}) \approx \frac{k}{n \cdot v}
		\end{equation}
		\item This gives rise to two different techniques:
		\begin{enumerate}
			\item \highlight{Kernel density estimation} (Fix $v$ and determine $k$)
			\item \highlight{k-nearest neighbors} (Fix $k$ and determine $v$)
		\end{enumerate}
	\end{itemize}
\end{frame}


% Kernel Density Estimation: Parzen Window
\begin{frame}{Kernel Density Estimation: Parzen Window}{}
	\begin{itemize}
		\item $\mathcal{R}$ is a $D$-dimensional \highlight{hyper-cube} of edge length $h$ centered on $\bm{x}$
		\item Determine if a data point falls into region $\mathcal{R}$:
		\begin{equation}
			H(\bm{u}) = \begin{cases} 
				1\quad \text{if}\ \vert u_d \vert \le \nicefrac{h}{2}, d = 1, 2, \dots, D \\
				0\quad \text{otherwise}
			\end{cases}
		\end{equation}
		\item The total number of data points falling into region $\mathcal{R}$ is given by:
		\begin{equation}
			k(\bm{x}) = \sum_{i=1}^n H(\bm{x} - \bm{x}^{(i)})
		\end{equation}
	\end{itemize}
\end{frame}


% Kernel Density Estimation: Parzen Window (Ctd.)
\begin{frame}{Kernel Density Estimation: Parzen Window (Ctd.)}{}
	\begin{itemize}
		\item The volume $v$ is simple to compute:
		\begin{equation}
			v = \int H(\bm{u}) \diff \bm{u} = h^D
		\end{equation}
		\item Putting it all together we get:
		\begin{equation}
			p(\bm{x}) \approx \frac{k(\bm{x})}{n \cdot v} = \frac{1}{n \cdot h^D} \sum_{i=1}^n H(\bm{x} - \bm{x}^{(i)})
		\end{equation}
	\end{itemize}
\end{frame}


% Kernel Density Estimation: Parzen Window (Ctd.)
\begin{frame}{Kernel Density Estimation: Parzen Window (Ctd.)}
	\begin{figure}
		\centering
		\includegraphics[scale=0.55]{04_density_estimation/02_img/kde}
	\end{figure}
\end{frame}


% k-Nearest Neighbors
\begin{frame}{k-Nearest Neighbors}
	\begin{itemize}
		\item Increase the size of a sphere until $k$ data points fall into this sphere, keep the number $K$ of data points fixed
		\begin{equation}p(\bm{x}) \approx \frac{k}{n \cdot v(\bm{x})}\end{equation}
		\item We will also look at k-Nearest Neighbors as a classification model later\newline $\rightarrow$ you can use a majority vote among the $k$ closest training data points to classify a new data point $\bm{x}$
	\end{itemize}
\end{frame}


% k-Nearest Neighbors (Ctd.)
\begin{frame}{k-Nearest Neighbors (Ctd.)}
	\begin{figure}
		\centering
		\includegraphics[scale=0.55]{04_density_estimation/02_img/knn}
	\end{figure}
\end{frame}


% Section: Mixture Models
%______________________________________________________________________
\section{Mixture Models}
\makedivider{Mixture Models}

% Subsection: 
% --------------------------------------------------------------------------------------------------------
\subsection{General Idea}

% General Idea / Motivation
\begin{frame}{Why do we need mixture models?}{}
	\begin{itemize}
		\item Parametric models have low memory footprint, are quick at runtime and often have nice analytic properties
		\item Non-parametric models make fewer assumptions about the data, but are slower and have a high memory footprint
		\item We can combine different models in a mixture model!
		\begin{equation}
			p(\bm{x}) = \sum_{j=1}^M p(\bm{x}|j) p(j)
		\end{equation}
	\end{itemize}
\end{frame}

% General Idea / Motivation
\begin{frame}{Why do we need mixture models?}{}
	\begin{itemize}
		\item A single parametric model might not be able to capture the structure of the dataset at hand
		\item Mixture distributions (e.\,g. a linear combination of Gaussians) can approximate almost any continuous density to arbitrary accuracy (given a sufficient number of Gaussians is used)
	\end{itemize}
	\begin{figure}
		\centering
		\includegraphics[scale=0.30]{04_density_estimation/02_img/mixturemodels_oldfaithful.png}
	\end{figure}
\end{frame}

\subsection{Mixture of Gaussians}
% Definition of Mixture of Gaussians
\begin{frame}{Mixture of Gaussians Definition}{}
	\begin{itemize}
		\item Sum up the $M$ individual Gaussian distributions
		\begin{align}
			p(x) &= \sum_{j=1}^M p(x|z_j)\\
			p(x|z_j) &= \mathcal{N}(x|\mu_j, \sigma_j) = \frac{1}{\sqrt{2\pi}\sigma_j} \exp\left(-\frac{(x - \mu_j)^2}{2\sigma_j^2}\right)
		\end{align}
		\begin{figure}
			\centering
			\includegraphics[scale=0.20]{04_density_estimation/02_img/gaussianmixturedistribution}
		\end{figure}
	\end{itemize}
\end{frame}

\begin{frame}{Mixture of Gaussians Definition}{}
	\begin{itemize}
		\item Sum up the $M$ individual Gaussian distributions
		\begin{align}
			p(x) &= \sum_{j=1}^M p(x|z_j)\\
			p(x|z_j) &= \mathcal{N}(x|\mu_j, \sigma_j) = \frac{1}{\sqrt{2\pi}\sigma_j} \exp\left(-\frac{(x - \mu_j)^2}{2\sigma_j^2}\right)
		\end{align}
		\item $z_j$ for $j \in \{1, \dots, M\}$ are the \textit{mixing coefficients}
		\item $z_j \sim \text{Multinomial}(\bm{\phi})$, where $\phi_j \ge 0$ and $\sum_{j=1}^M \phi_j = 1$
	\end{itemize}
\end{frame}

\begin{frame}{Mixture of Gaussians Definition}{}
	\begin{itemize}
		\item Sum up the $M$ individual Gaussian distributions
		\begin{align}
			p(x) &= \sum_{j=1}^M p(x|z_j)\\
			p(x|z_j) &= \mathcal{N}(x|\mu_j, \sigma_j) = \frac{1}{\sqrt{2\pi}\sigma_j} \exp\left(-\frac{(x - \mu_j)^2}{2\sigma_j^2}\right)
		\end{align}
		\item The overall mixture density $p(x)$ integrates to $1$
		\item The parameters: $\theta = \{\mu_1, \sigma_1, \pi_1, \dots, \mu_M, \sigma_M, \pi_M\}$
		\begin{boxBlueNoFrame}
			\highlight{Which function do we need to maximize to estimate the parameters?}
		\end{boxBlueNoFrame}
	\end{itemize}
\end{frame}

\begin{frame}{Maximum Likelihood Estimation for MoG}{}
	\begin{itemize}
		\item We have defined our Gaussian mixture model: $p(x) = \sum_{j=1}^M p(x|z_j)$
		\item Maximize the log-likelihood to estimate the parameters $\bm{\theta}$:
		\begin{align}
			\mathcal{L} &= \log L(\theta) = \sum_{i=1}^N \log p(x_i|\theta)\\
			\frac{\partial \mathcal{L}}{\partial \mu_j} &= 0\\
			\mu_j &= \frac{\sum_{i=1}^N p(z_j|x_i) x_i}{\sum_{i=1}^N p(z_j|x_i)}\hspace{0.5cm} \Leftarrow \text{Do you see the issue?}
		\end{align}
	\end{itemize}
\end{frame}

\begin{frame}{Maximum Likelihood Estimation for MoG}{}
	\begin{itemize}
		\item Maximize the log-likelihood to estimate the parameters $\bm{\theta}$:
		\begin{align}
			\mathcal{L} &= \log L(\theta) = \sum_{i=1}^N \log p(x_i|\theta)\\
			\frac{\partial \mathcal{L}}{\partial \mu_j} &= 0\\
			\mu_j &= \frac{\sum_{i=1}^N p(z_j|x_i) x_i}{\sum_{i=1}^N p(z_j|x_i)}\hspace{0.5cm} \Leftarrow \text{Do you see the issue?}
		\end{align}
		\item There is circular dependency, $\mu_j$ depends on $z_j$ $\Rightarrow$ there is no analytical solution!
	\end{itemize}
\end{frame}

\begin{frame}{Maximum Likelihood Estimation for MoG}{}
	\begin{itemize}
		\item There is circular dependency, $\mu_j$ depends on $z_j$ $\Rightarrow$ there is no analytical solution!
		\item How about using gradient descent?
		\begin{itemize}
			\item Complex gradient (circular dependency, non-linearity)
			\item Optimization of each Gaussian mixture component depends on all of the other mixture components!
		\end{itemize}
		\item We have to take a different approach: \textit{Expectation Maximization}
	\end{itemize}
\end{frame}

\subsection{Expectation Maximization for MoG}
\begin{frame}{Expectation Maximization}{}
	\begin{itemize}
		\item We have observed data (without labels) $x_i$ and unobserved / hidden / latent variables $z_j$
		\item If we knew which Gaussian has generated which data point $x_i$, then we could do a maximum likelihood estimation for each mixture component $j$
		\item If we knew the Gaussians already, i.\,e. their means and (co)variances, then we could assign each data point $x_i$ to mixture component $\eta$ if $p(z_\eta = 1|x_i) > p(z_j = 2|x_i) \quad \forall j$
	\end{itemize}
\end{frame}

\begin{frame}{Expectation Maximization}{}
	\begin{itemize}
		\item This is a chicken-and-egg problem:
		\begin{itemize}
			\item We don't know which mixture component generated which $x_i$\\
			\item We don't know the distributions
		\end{itemize}
		\item How can we estimate the parameters of the distribution and the mixture coefficients $z_j$?
	\end{itemize}
	\begin{figure}
		\centering
		\includegraphics[scale=0.25]{04_density_estimation/02_img/gaussianmixturedistribution}
	\end{figure}
\end{frame}

\begin{frame}{Expectation Maximization}{}
	\begin{itemize}
		\item How can we estimate the parameters of the distribution and the mixture coefficients $z_j$?
		\item Idea:
		\begin{itemize}
			\item Assign a \textit{mixture label} to each data point $x_i$, cluster the data points and assign them to the mixture components
			\item The mixture coefficients $z_j$ are \textit{soft assignments} of data points to mixture components
			\item Iterate between updating the estimate of the mixture coefficients $z_j$ and the distribution parameters $\mu_j, \sigma_j$
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Expectation Maximization}{}
EM for Gaussian Mixture Models:
	\begin{enumerate}
		\item Initialize $\mu_j, \sigma_j, z_j$ and evaluate the initial log-likelihood
		\item \textbf{E step:} Compute the posterior distribution (\textit{responsibilities} $\gamma_{ij}$) for each mixture coefficient $z_j$ and all data points $x_i$ using the current parameter values
		\begin{equation}
			\gamma_{ij} = p(z_j|x_i) = \frac{z_j \mathcal{N}(x_i|\mu_j, \sigma_j)}{\sum_{m=1}^M z_m \mathcal{N}(x_i|\mu_m, \sigma_m)}
		\end{equation}
	\end{enumerate}
\end{frame}

\begin{frame}{Expectation Maximization}{}
EM for Gaussian Mixture Models:
	\begin{enumerate}
		\setcounter{enumi}{2}
		\item \textbf{M step:} Compute the new parameters using weighted estimates\\
		\begin{gather}
			N_j = \sum_{i=1}^N \gamma_{ij} \qquad z_{j}^{\text{new}} = \frac{N_j}{N}\\
			\mu_j^{\text{new}} = \frac{1}{N_j} \sum_{i=1}^N \gamma_{ij} x_i \qquad (\sigma_j^{\text{new}})^2 = \frac{1}{N_j} \sum_{i=1}^N \gamma_{ij} (x_i - \mu_j^{\text{new}})^2
		\end{gather}
	\end{enumerate}
\end{frame}

\begin{frame}{Expectation Maximization}{}
EM for Gaussian Mixture Models:
	\begin{enumerate}
		\setcounter{enumi}{3}
		\item Iterate \textbf{E Step} and \textbf{M Step} until convergence, i.\,e. evaluate the log-likelihood after each run and check if the parameters converge:
		\begin{equation}
			\mathcal{L} = \sum_{i=1}^N \log p(x_i|\bm{\theta}) = \sum_{i=1}^N \log\left(\sum_{j=1}^M z_j \mathcal{N}(x_i|\mu_j, \sigma_j)\right)
		\end{equation}
	\end{enumerate}
\end{frame}

\begin{frame}{Expectation Maximization}{}
	\begin{itemize}
		\item We can use EM for performing maximum likelihood estimation even when the data is incomplete (missing features)
		\item The incomplete log-likelihood $\mathcal{L}$ is maximized locally
		\item The log-likelihood is guaranteed to improve or stay the same in every EM iteration
		\item There are great visualizations of EM for Gaussian mixture models:
		\begin{itemize}
			\item \href{https://www.youtube.com/watch?v=l1W3BvjJnmY}{\linkstyle{EM density estimation animation}}
			\item \href{https://youtu.be/eXdGCO-2n90?t=2}{\linkstyle{2-dimensional EM animation}}
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Expectation Maximization}{}
	Let's look at some iterations of expectation maximization on a simple dataset:
	\begin{figure}
		\includegraphics[scale=0.45]{04_density_estimation/02_img/em}
	\end{figure}
\end{frame}

\begin{frame}{Expectation Maximization}{}
	\begin{itemize}
		\item How do we initialize the parameters for EM?
		\begin{itemize}
			\item EM depends on a good initialization of the parameters, a poor initialization can lead to getting stuck in bad local optima
			\item We can use \textit{k-means} to get an initial clustering
		\end{itemize}
		\item How many mixture components do we need?
		\begin{itemize}
			\item There are different criteria, we can for instance maximize the \textit{Bayesian Information Criterion} with respect to $K$:
			\begin{equation}
				\log p(X|\theta_{ML}) - \frac{1}{2} K \log N
			\end{equation}
			\begin{itemize}
				\item $K$: Number of parameters
				\item $N$: Number of data points
			\end{itemize}
		\end{itemize}
	\end{itemize}
\end{frame}



% Section: Wrap-Up
%______________________________________________________________________
\section{Wrap-Up}
\makedivider{Wrap-Up}

% Subsection: Summary
% --------------------------------------------------------------------------------------------------------
\subsection{Summary}

% Summary
\begin{frame}{Summary}{}
	\begin{itemize}
		\item We can use parametric, non-parametric and mixture models for density estimation
		\item This allows us to estimate the probabilities needed for e.\,g. a Na\"ive Bayes model to work with continuous features
		\item Parametric models assume a certain form of the density, governed by parameters like mean and variance for a Gaussian
		\item Maximum likelihood estimation allows us to determine the parameters based on our dataset
		\item Non-parametric models directly use the (training) data points themselves
		\item We can use expectation maximization to optimize the parameters of mixture models
	\end{itemize}
\end{frame}


% Subsection: Self-Test Questions
% --------------------------------------------------------------------------------------------------------
\subsection{Self-Test Questions}

% Self-Test Questions
\begin{frame}{Self-Test Questions}{}\important
	\begin{itemize}
		\item What is maximum likelihood estimation? How can you get the maximum likelihood estimate for a Gaussian distribution?
		\item What does \textit{non-parametric} mean in \textit{non-parametric models}?
		\item What is different between kernel density estimation and k-nearest neighbors?
		\item Why can't we use a simple maximum likelihood estimate for a mixture model as with a single Gaussian distribution?
		\item What happens in the E and M steps in expectation maximization?
	\end{itemize}
\end{frame}


% Subsection: Lecture Outlook
% --------------------------------------------------------------------------------------------------------
\subsection{Lecture Outlook}

\begin{frame}{What's next...?}{}
	\makeoverview{5}
\end{frame}


% Subsection: Recommended Literature and further Reading
% --------------------------------------------------------------------------------------------------------
\subsection{Recommended Literature and further Reading}

% Literature
%______________________________________________________________________
\begin{frame}{Recommended Literature and further Reading}{}
	\footnotesize
	\begin{thebibliography}{2}
		\literature{book}{Bishop (2006)}{[1] Pattern Recognition and Machine Learning - Chapter 1.2.4 \textit{The Gaussian Distribution}}
			{Bishop. 2006.}{$\rightarrow$ \href{http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop\%20-\%20Pattern\%20Recognition\%20And\%20Machine\%20Learning\%20-\%20Springer\%20\%202006.pdf}{\linkstyle{Link}}}
		\literature{book}{Bishop (2006)}{[1] Pattern Recognition and Machine Learning - Chapter 2.5 \textit{Non-Parametric Methods}}
			{Bishop. 2006.}{$\rightarrow$ \href{http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop\%20-\%20Pattern\%20Recognition\%20And\%20Machine\%20Learning\%20-\%20Springer\%20\%202006.pdf}{\linkstyle{Link}}}
		\literature{book}{Bishop (2006)}{[1] Pattern Recognition and Machine Learning - Chapter 9.2 \textit{Mixtures of Gaussians}}
			{Bishop. 2006.}{$\rightarrow$ \href{http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop\%20-\%20Pattern\%20Recognition\%20And\%20Machine\%20Learning\%20-\%20Springer\%20\%202006.pdf}{\linkstyle{Link}}}
	\end{thebibliography}
\end{frame}


% Subsection: Meme of the Day
% --------------------------------------------------------------------------------------------------------
\subsection{Meme of the Day}

% Meme of the Day
\begin{frame}{Meme of the Day}{}
	\begin{figure}
		\includegraphics[scale=0.90]{04_density_estimation/02_img/gaussianseverywhere}
	\end{figure}
\end{frame}


% Thank you
%______________________________________________________________________
\makethanks

\end{document}