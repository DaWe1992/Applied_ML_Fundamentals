\input{../preamble_theme}

% ====================================================
% ====================================================
% PRESENTATION DATA
% ====================================================
% ====================================================

\title[Clustering]{*** Applied Machine Learning Fundamentals *** Clustering}
\institute[SAP\,SE]{SAP\,SE / DHBW Mannheim}
\author{Daniel Wehner, M.Sc.}
\date{Winter term 2020/2021}
\prefix{CL}

% ====================================================
% ====================================================
% BEGIN OF DOCUMENT
% ====================================================
% ====================================================

\begin{document}

% Title frame
%______________________________________________________________________
\maketitlepage


% Lecture Overview
%______________________________________________________________________
\begin{frame}{Lecture Overview}{}
	\makeoverview{9}
\end{frame}


% Agenda
%______________________________________________________________________
\begin{frame}{Agenda for this Unit}
	\begin{multicols}{2}
		\tableofcontents
	\end{multicols}
\end{frame}


% Section: Introduction
%______________________________________________________________________
\section{Introduction}
\makedivider{Introduction}

% Subsection: What is Clustering?
% --------------------------------------------------------------------------------------------------------
\subsection{What is Clustering?}

% Clustering Introduction
\begin{frame}{Clustering Introduction}{}
	\begin{itemize}
		\item \highlight{Clustering} belongs to the category of \textbf{unsupervised learning}
		\item A clustering algorithm tries to \textbf{find structure} in the data
		\item Once the clusters are found, they first have to be interpreted...
		\item ...and can then be used for prediction purposes
	\end{itemize}
	
	\vspace*{3mm}
	\begin{boxBlueNoFrame}
		\footnotesize
		A cluster should be \highlight{internally homogeneous}, but simultaneously \highlight{externally heterogeneous}.
		(Elements of one cluster should be similar to each other, but should differ significantly from elements belonging to other clusters.)
	\end{boxBlueNoFrame}
\end{frame}


% Example Use Cases for Clustering
\begin{frame}{Example Use Cases for Clustering}{}
	\begin{itemize}
		\item \textbf{Behavioral segmentation}
		\begin{itemize}
			\item Customer segmentation (e.\,g. \highlight{sinus milieus})
			\item Creating profiles based on activity monitoring
		\end{itemize}
		\item \textbf{Sorting sensor measurements}
		\begin{itemize}
			\item Image grouping
			\item Detection of activity types in motion sensors
		\end{itemize}
		\item \textbf{Inventory categorization}
		\begin{itemize}
			\item Grouping inventory by sales activity
			\item Grouping inventory by manufacturing metrics
		\end{itemize}
		\item Many, many more, ...
	\end{itemize}
\end{frame}


% Subsection: Clustering Strategies Overview
% --------------------------------------------------------------------------------------------------------
\subsection{Clustering Strategies Overview}

% Clustering Strategies
\begin{frame}{Clustering Strategies}{}
	\begin{enumerate}
		\item \highlight{EM-based clustering}, e.\,g.: $k$-Means
		\item \highlight{Hierarchical clustering}, e.\,g.:
		\begin{itemize}
			\item Agglomerative clustering
			\item Divisive clustering
		\end{itemize}
		\item \highlight{Affinity-based clustering}, e.\,g.:
		\begin{itemize}
			\item Spectral clustering
			\item DBSCAN
		\end{itemize}
	\end{enumerate}
\end{frame}


% Section: $k$-Means
%______________________________________________________________________
\section{$k$-Means}
\makedivider{$k$-Means}

% Subsection: Introduction
% --------------------------------------------------------------------------------------------------------
\subsection{Introduction}

% $k$-Means: Procedure
\begin{frame}{$k$-Means: Procedure}{}\important
	\begin{itemize}
		\item The algorithm is an instance of \highlight{vector quantization}
		\begin{itemize}
			\item It represents data points by a single vector (\highlight{centroid}) which is close to them
			\item This is useful for \textbf{data compression}!
		\end{itemize}
		\item \textbf{How to}: Create $k$ partitions ($\widehat{=}$ clusters) of the data set $\mathcal{D}$, such that
			the sum of squared deviations from the cluster centroids is \textbf{minimal}:
		\begin{equation}
			\min_{\bm{\mu}_j} \sum_{j=1}^k \sum_{\bm{x}^{(i)} \in \mathcal{D}_j} \Vert \bm{x}^{(i)} - \bm{\mu}_j \Vert^2
		\end{equation}
		\item Where $\mathcal{D}_j \equiv j$-th cluster, $\bm{\mu}_j \equiv$ centroid of $j$-th cluster
	\end{itemize}
\end{frame}


% Result: Voronoi Diagram
\begin{frame}{Result: Voronoi Diagram}{}\important
	\divideTwo{0.49}{
		\begin{itemize}
			\item The dots represent cluster centroids
			\item The lines visualize the \textbf{cluster boundaries}
			\item For a new point we can easily determine to which cluster it has to be assigned
		\end{itemize}
	}{0.49}{
		\begin{figure}
			\includegraphics[scale=0.2]{12_clustering/02_img/voronoi}
		\end{figure}
	}
\end{frame}


% Subsection: $k$-Means Algorithm
% --------------------------------------------------------------------------------------------------------
\subsection{$k$-Means Algorithm}

% $k$-Means Algorithm 
\begin{frame}{$k$-Means Algorithm}{}\important
	\vspace*{2mm}
	\begin{itemize}
		\item Input: $\mathcal{D} = \{ \bm{x}^{(1)}, \bm{x}^{(2)}, \dots, \bm{x}^{(n)} \} \in \mathbb{R}^{n \times m}$,
			number of clusters $k$	
		\item Algorithm:
		\begin{enumerate}
			\item $t \longleftarrow 1$
			\item Randomly choose $k$ means $\bm{\mu}_1^{\langle 1 \rangle}, \bm{\mu}_2^{\langle 1 \rangle}, \dots, \bm{\mu}_k^{\langle 1 \rangle}$ 
			\item While not converged:
			\begin{itemize}
				\item[\textbf{3a}] Assign each $\bm{x}^{(i)} \in \mathcal{D}$ to the closest cluster:
				{\footnotesize
				\begin{equation*}
					\mathcal{D}_j^{\langle t \rangle}
						= \left\{
							\bm{x}^{(i)} : \Vert \bm{x}^{(i)} - \bm{\mu}_j^{\langle t \rangle} \Vert^2 \le
							\Vert \bm{x}^{(i)} - \bm{\mu}_{j^*}^{\langle t \rangle} \Vert^2;\
							\forall j^* = 1, 2, \dots, k; \bm{x}^{(i)} \in \mathcal{D}
						\right\}
				\end{equation*}}
				\item[\textbf{3b}] Update cluster centroids $\bm{\mu}_j$:
				{\footnotesize
				\begin{equation*}
					\bm{\mu}_{j}^{\langle t+1 \rangle} =
						\frac{1}{\vert \mathcal{D}_j^{\langle t \rangle} \vert} \sum_{\bm{x}^{(i)} \in \mathcal{D}_j^{\langle t \rangle}} \bm{x}^{(i)}
					\qquad\text{then update $t$:}\quad t \longleftarrow t + 1
				\end{equation*}}
			\end{itemize}
		\end{enumerate}
	\end{itemize}
\end{frame}


% $k$-Means Algorithm (Ctd.)
\begin{frame}{$k$-Means Algorithm (Ctd.)}{}\important
	\divideTwo{0.49}{
		\begin{figure}
			\centering
			\animategraphics[autoplay,loop,height=5cm]{2.5}
				{12_clustering/02_img/k_means_animation/}{0}{14}
		\end{figure}
	}{0.49}{
		\begin{itemize}
			\item The algorithm might need some iterations until the result is satisfactory
			\item \Highlight{Caveat:} The algorithm might get stuck in local optima \\
				$\Rightarrow$ several restarts
		\end{itemize}
	}
\end{frame}


% Subsection: Use Case: Image Compression
% --------------------------------------------------------------------------------------------------------
\subsection{Use Case: Image Compression}

% Image Compression
\begin{frame}{Image Compression}{}
	\begin{figure}
		\centering
		\includegraphics[scale=0.3]{12_clustering/02_img/image_compression}
	\end{figure}
\end{frame}


% Subsection: Problems and Issues
% --------------------------------------------------------------------------------------------------------
\subsection{Problems and Issues}

% $k$-Means Issues
\begin{frame}{$k$-Means Issues}{}
	\begin{itemize}
		\item The algorithm assumes that all clusters are \textbf{spherical} \\
			($\ne$ \highlight{affinity-based clustering})
		\item It does not have a notion of \textbf{outliers} (unlike DBSCAN)
		\item What is the correct value for $k$? $\Rightarrow$ \highlight{Elbow-method:}
		\begin{itemize}
			\item Measure sum of squared distances from data points to cluster centers (inertia)
			\item Record results for different values for $k$ and plot them
			\item Search for the `elbow point'
		\end{itemize}
	\end{itemize}
\end{frame}


% Elbow Method
\begin{frame}{Elbow Method}{}
	\begin{figure}
		\centering
		\includegraphics[scale=0.6]{12_clustering/02_img/elbow}
	\end{figure}
\end{frame}


% Section: Hierarchical Clustering
%______________________________________________________________________
\section{Hierarchical Clustering}
\makedivider{Hierarchical Clustering}

% Subsection: Agglomerative Clustering Algorithm
% --------------------------------------------------------------------------------------------------------
\subsection{Agglomerative Clustering Algorithm}

% Agglomerative Clustering Algorithm
\begin{frame}{Agglomerative Clustering Algorithm}{}\important
	\begin{enumerate}
		\item Start with one cluster for each instance: $C = \{ \{ \bm{x}^{(i)}\} : \bm{x}^{(i)} \in \mathcal{D} \}$
		\item Compute distance $d(C_i, C_j)$ between all pairs of clusters $C_i$, $C_j$
		\item Join clusters $C_i$ and $C_j$ with minimum distance into a new cluster $C_p$:
		\begin{align*}
			C_p
				&= \{ C_i, C_j \} \\
			C
				&= (C \backslash \{ C_i, C_j \}) \cup \{ C_p \}
		\end{align*}
		\item Compute distances between $C_p$ and all other clusters in $C$
		\item If $\vert C \vert > 1$, goto 3
	\end{enumerate}
\end{frame}


% Subsection: Agglomerative Clustering: Example
% --------------------------------------------------------------------------------------------------------
\subsection{Agglomerative Clustering: Example}

% Agglomerative Clustering: Example
\begin{frame}{Agglomerative Clustering: Example}{}
	\begin{figure}
		\centering
		\includegraphics[scale=0.4]{12_clustering/02_img/aggl_clust_1}
	\end{figure}
\end{frame}


% Agglomerative Clustering: Example (Ctd.)
\begin{frame}{Agglomerative Clustering: Example (Ctd.)}{}
	\begin{figure}
		\centering
		\includegraphics[scale=0.4]{12_clustering/02_img/aggl_clust_2}
	\end{figure}
\end{frame}


% Agglomerative Clustering: Example (Ctd.)
\begin{frame}{Agglomerative Clustering: Example (Ctd.)}{}
	\begin{figure}
		\centering
		\includegraphics[scale=0.4]{12_clustering/02_img/aggl_clust_3}
	\end{figure}
\end{frame}


% Agglomerative Clustering: Example (Ctd.)
\begin{frame}{Agglomerative Clustering: Example (Ctd.)}{}
	\begin{figure}
		\centering
		\includegraphics[scale=0.4]{12_clustering/02_img/aggl_clust_4}
	\end{figure}
\end{frame}


% Agglomerative Clustering: Example (Ctd.)
\begin{frame}{Agglomerative Clustering: Example (Ctd.)}{}
	\begin{figure}
		\centering
		\includegraphics[scale=0.4]{12_clustering/02_img/aggl_clust_5}
	\end{figure}
\end{frame}


% Agglomerative Clustering: Example (Ctd.)
\begin{frame}{Agglomerative Clustering: Example (Ctd.)}{}
	\bubble{13}{5}{
		\footnotesize This is a \\[-2mm]
		\footnotesize \highlight{dendrogram}
	}
	\begin{figure}
		\centering
		\includegraphics[scale=0.4]{12_clustering/02_img/aggl_clust_6}
	\end{figure}
\end{frame}


% Subsection: Distance Metrics between Clusters
% --------------------------------------------------------------------------------------------------------
\subsection{Distance Metrics between Clusters}

% Single Linkage
\begin{frame}{Single Linkage}{}
	\begin{itemize}
		\item How to compute the distance between two clusters $C_1$ and $C_2$?
		\item \highlight{Single linkage}:
		\begin{equation*}
			d(C_1, C_2) = \min\{ d(\bm{x}^{(i)}, \bm{x}^{(j)}) : \bm{x}^{(i)} \in C_1, \bm{x}^{(j)} \in C_2 \}
		\end{equation*}
	\end{itemize}

	\begin{figure}
		\centering
		\includegraphics[scale=0.4]{12_clustering/02_img/single_linkage}
	\end{figure}
\end{frame}


% Complete Linkage
\begin{frame}{Complete Linkage}{}
	\begin{itemize}
		\item How to compute the distance between two clusters $C_1$ and $C_2$?
		\item \highlight{Complete linkage}:
		\begin{equation*}
			d(C_1, C_2) = \max\{ d(\bm{x}^{(i)}, \bm{x}^{(j)}) : \bm{x}^{(i)} \in C_1, \bm{x}^{(j)} \in C_2 \}
		\end{equation*}
	\end{itemize}

	\begin{figure}
		\centering
		\includegraphics[scale=0.4]{12_clustering/02_img/complete_linkage}
	\end{figure}
\end{frame}


% Section: Spectral Clustering
%______________________________________________________________________
\section{Spectral Clustering}
\makedivider{Spectral Clustering}

% Subsection: Motivation
% --------------------------------------------------------------------------------------------------------
\subsection{Motivation}

% Spectral Clustering
\begin{frame}{Spectral Clustering}{}
	\begin{itemize}
		\item Remember the disadvantage of $k$-Means? (spherical clusters)
		\item How can we cluster data without this assumption?
		\item[$\bm{\Rightarrow}$] \highlight{Affinity-based clustering}
		
		\vspace*{3mm}
		\begin{boxBlueNoFrame}
			\textbf{Affinity-based clustering} assumes \textbf{no shape} of the resulting clusters.
			It is based on the \textbf{connectedness of the data points}.
		\end{boxBlueNoFrame}

		\item Spectral clustering is affinity-based
		\item Whenever you hear \textit{`spectral'}: It has something to do with eigen-vectors
	\end{itemize}
\end{frame}


% Example Data Set
\begin{frame}{Example Data Set}{}
	\bubble{1}{5}{
		\footnotesize What would be \\[-2mm]
		\footnotesize the result of $k$-Means?}
	\input{12_clustering/01_tikz/unsupervised_learning}
\end{frame}


% Subsection: A Bit of Graph Theory
% --------------------------------------------------------------------------------------------------------
\subsection{A Bit of Graph Theory}

% A short Introduction to Graphs
\begin{frame}{A short Introduction to Graphs}{}
	\begin{itemize}
		\item A graph $\mathcal{G}$ is a tuple $\mathcal{G} = (\mathcal{V}, \mathcal{E})$
		\item $\mathcal{V} = \{ v_1, v_2, \dots, v_n \}$ is the set of $n$ vertices (nodes)
		\item $\mathcal{E} \in \mathcal{V} \times \mathcal{V}$ the set of edges (connections between nodes)
		\item \highlight{Adjacency matrix} $\bm{A}$
		\begin{itemize}
			\item $A_{ij} = 1$, iff $(v_i, v_j) \in \mathcal{E}$ ($v_i$ is a neighbor of $v_j$)
			\item $\bm{A}$ is symmetric for undirected graphs, i.\,e. $A_{ij} = A_{ji}$
		\end{itemize}
		\item The \highlight{degree matrix} $\bm{D} = diag(d_1, d_2, \dots, d_n)$ is a matrix of node degrees
		\begin{equation*}
			d_i = \sum_{j=1}^n A_{ij}
		\end{equation*}
	\end{itemize}
\end{frame}


% A short Introduction to Graphs (Ctd.)
\begin{frame}{A short Introduction to Graphs (Ctd.)}{}
	\begin{itemize}
		\item For graph analysis it is often useful to compute the \highlight{graph Laplacian} matrix:
		\begin{equation*}
			\bm{L} = \bm{D} - \bm{A}
		\end{equation*}
		\item Example:
		\begin{figure}
			\centering
			\includegraphics[scale=0.08]{12_clustering/02_img/graph}
		\end{figure}
	\end{itemize}
\end{frame}


% Example: Computation of $\bm{A}$, $\bm{D}$ and $\bm{L}$
\begin{frame}{Example: Computation of $\bm{A}$, $\bm{D}$ and $\bm{L}$}{}
	\scriptsize
	\begin{equation*}
		\bm{A} = \begin{bmatrix}
			0 & 1 & 0 & 0 & 1 & 0 \\
			1 & 0 & 1 & 0 & 1 & 0 \\
			0 & 1 & 0 & 1 & 0 & 0 \\
			0 & 0 & 1 & 0 & 1 & 1 \\
			1 & 1 & 0 & 1 & 0 & 0 \\
			0 & 0 & 0 & 1 & 0 & 0 \\
		\end{bmatrix}\qquad
		\bm{D} = \begin{bmatrix}
			2 & 0 & 0 & 0 & 0 & 0 \\
			0 & 3 & 0 & 0 & 0 & 0 \\
			0 & 0 & 2 & 0 & 0 & 0 \\
			0 & 0 & 0 & 3 & 0 & 0 \\
			0 & 0 & 0 & 0 & 3 & 0 \\
			0 & 0 & 0 & 0 & 0 & 1 \\
		\end{bmatrix}\qquad
		\bm{L} = \begin{bmatrix}
			 \phantom{-}2 	& -1 			& \phantom{-}0 	& \phantom{-}0 	& -1 			& \phantom{-}0 	\\
			-1 			& \phantom{-}3 	& -1 			& \phantom{-}0 	& -1 			& \phantom{-}0 	\\
			 \phantom{-}0 	& -1 			& \phantom{-}2 	& -1 			& \phantom{-}0 	& \phantom{-}0 	\\
			 \phantom{-}0 	& \phantom{-}0 	& -1 			& \phantom{-}3 	& -1 			& -1 			\\
			-1 			& -1 			& \phantom{-}0 	& -1 			& \phantom{-}3 	& \phantom{-}0 	\\
			 \phantom{-}0 	& \phantom{-}0 	& \phantom{-}0 	& -1 			& \phantom{-}0 	& \phantom{-}1 	\\
		\end{bmatrix}
	\end{equation*}
\end{frame}


% Subsection: Spectral Clustering Algorithm
% --------------------------------------------------------------------------------------------------------
\subsection{Spectral Clustering Algorithm}

% How to get the Graph for the Data Set?
\begin{frame}{How to get the Graph for the Data Set?}{}\important
	\begin{itemize}
		\item There are at least two possibilities:
		\begin{enumerate}
			\item \highlight{$\varepsilon$-neighborhood graph}
			\begin{itemize}
				\item Connect all instances whose pairwise distances are smaller than $\varepsilon$
				\item \Highlight{Problem:} How to choose $\varepsilon$?
			\end{itemize}
			\item \highlight{$k$-nearest neighbor graph}
			\begin{itemize}
				\item Connect instance $\bm{x}^{(i)}$ with instance $\bm{x}^{(j)}$, if $\bm{x}^{(j)}$ is among the $k$ nearest neighbors of $\bm{x}^{(i)}$
				\item Attention: This definition leads to a directed graph \textbf{(Why?)} \\
					$\Rightarrow$ Can be ignored
				\item \Highlight{Problem:} How to choose $k$?
			\end{itemize}
		\end{enumerate}
		\item Both approaches are used in practice
	\end{itemize}
\end{frame}


% Spectral Clustering Algorithm
\begin{frame}{Spectral Clustering Algorithm}{}\important
	\begin{itemize}
		\item Input: $\mathcal{D} = \{ \bm{x}^{(1)}, \bm{x}^{(2)}, \dots, \bm{x}^{(n)} \} \in \mathbb{R}^{n \times m}$,
			number of clusters $k$
		\item Algorithm:
		\begin{enumerate}
			\item Construct a similarity graph (adjacency matrix $\bm{A}$ and degree matrix $\bm{D}$)
			\item Compute the graph Laplacian matrix $\bm{L} = \bm{D} - \bm{A}$
			\item Perform \textbf{eigen-decomposition} on $\bm{L}$ (to obtain the eigen-vectors $\bm{Q}$)
			\begin{equation*}
				\bm{L} = \bm{Q}\bm{\Lambda}\bm{Q}^{-1}
			\end{equation*}
			\item Apply $k$-Means to the rows of matrix $\bm{Q}$ to obtain the clusters $\{ C_1, C_2, \dots, C_k \}$
		\end{enumerate} 
	\end{itemize}
\end{frame}


% Section: Wrap-Up
%______________________________________________________________________
\section{Wrap-Up}
\makedivider{Wrap-Up}

% Subsection: Summary
% --------------------------------------------------------------------------------------------------------
\subsection{Summary}

% Summary
\begin{frame}{Summary}{}
	\begin{itemize}
		\item Clustering belongs to the category of \textbf{unsupervised learning}
		\item With clustering we try to find \textbf{structure in the data}
		\item Different algorithms make \textbf{different assumptions} about the resulting clusters
		\item \textbf{Clustering Strategies:}
		\begin{itemize}
			\item EM-based clustering (e.\,g. $k$-Means)
			\item Hierarchical clustering
			\item Affinity-based clustering (e.\,g. spectral clustering, DBSCAN)
		\end{itemize}
	\end{itemize}
\end{frame}


% Subsection: Self-Test Questions
% --------------------------------------------------------------------------------------------------------
\subsection{Self-Test Questions}

% Self-Test Questions
\begin{frame}{Self-Test Questions}{}\important
	\begin{enumerate}
		\item What is clustering?
		\item What is the definition of a cluster. Which properties should it have?
		\item Describe the general procedure of $k$-Means. What are disadvantages?
		\item What is a dendrogram?
		\item How do we obtain the graphs for spectral clustering?
		\item What is affinity-based clustering? How does it differ from $k$-Means?
		\item How to calculate the graph Laplacian matrix?
	\end{enumerate}
\end{frame}


% Subsection: Lecture Outlook
% --------------------------------------------------------------------------------------------------------
\subsection{Lecture Outlook}

\begin{frame}{What's next...?}{}
	\makeoverview{10}
\end{frame}


% Subsection: Recommended Literature and further Reading
% --------------------------------------------------------------------------------------------------------
\subsection{Recommended Literature and further Reading}

% Literature
%______________________________________________________________________
\begin{frame}[allowframebreaks]{Recommended Literature and further Reading}{}
	\footnotesize
	\begin{thebibliography}{2}
		\literature{book}{Bishop.2006}{[1] Pattern Recognition and Machine Learning}
			{Christopher Bishop. Springer. 2006.}{$\rightarrow$ \href{
				http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop\%20-\%20Pattern\%20Recognition\%20And\%20Machine\%20Learning\%20-\%20Springer\%20\%202006.pdf
			}{\linkstyle{Link}}, cf. chapter 9}
		\literature{book}{Murphy.2012}{[2] Machine Learning: A Probabilistic Perspective}
			{Kevin Murphy. MIT Press. 2012.}{$\rightarrow$ \href{
				https://doc.lagout.org/science/Artificial\%20Intelligence/Machine\%20learning/Machine\%20Learning_\%20A\%20Probabilistic\%20Perspective\%20\%5BMurphy\%202012-08-24\%5D.pdf
			}{\linkstyle{Link}}, cf. chapter 25}	
	\end{thebibliography}
\end{frame}


% Subsection: Meme of the Day
% --------------------------------------------------------------------------------------------------------
\subsection{Meme of the Day}

% Meme of the Day
\begin{frame}{Meme of the Day}{}
	\begin{figure}
		\includegraphics[scale=0.5]{12_clustering/02_img/meme_of_the_day}
	\end{figure}
\end{frame}


% Thank you
%______________________________________________________________________
\makethanks

\end{document}