\input{../preamble_theme}

% ====================================================
% ====================================================
% PRESENTATION DATA
% ====================================================
% ====================================================

\title[Decision Trees and Ensembles]{*** Applied Machine Learning Fundamentals *** Decision Trees and Ensembles}
\institute[SAP\,SE]{SAP\,SE / DHBW Mannheim}
\author{Daniel Wehner, M.Sc.}
\date{Winter term 2020/2021}
\prefix{DT}

% ====================================================
% ====================================================
% BEGIN OF DOCUMENT
% ====================================================
% ====================================================

\begin{document}

% Title frame
%______________________________________________________________________
\maketitlepage


% Lecture Overview
%______________________________________________________________________
\begin{frame}{Lecture Overview}{}
	\makeoverview{6}
\end{frame}


% Agenda
%______________________________________________________________________
\begin{frame}{Agenda for this Unit}
	\begin{multicols}{2}
		\tableofcontents
	\end{multicols}
\end{frame}


% Section: Introduction
%______________________________________________________________________
\section{Introduction}
\makedivider{Introduction}

% What we want...
\begin{frame}{What we want...}{}
	\divideTwo{0.49}{
		\vspace*{4mm}
		\input{08_decision_trees/03_tbl/example_data_set}
	}{0.49}{
		\input{08_decision_trees/01_tikz/tree}
	}
\end{frame}


% What are Decision Trees?
\begin{frame}{What are Decision Trees?}{}
	\begin{itemize}
		\item Decision trees are induced in a \textbf{supervised} fashion
		\item Originally invented by \textit{Ross Quinlan} (1986)
		\item Decision trees are grown \textbf{recursively} $\rightarrow$ \textit{'divide-and-conquer'}
		\item A decision tree consists of:

		\begin{tabbing}
			\hspace*{2.5cm}\= \kill
			\textbf{Nodes}	\>	Each node corresponds to an attribute test 	\\
			\textbf{Edges}	\>	One edge per possible test outcome			\\
			\textbf{Leaves}	\>	Class label to predict
		\end{tabbing}
	\end{itemize}
\end{frame}


% Classifying new Instances
\begin{frame}{Classifying new Instances}{}
	\divideTwo{0.49}{
		\begin{itemize}
			\item Suppose we get a new instance:
			
			\footnotesize
			\begin{tabbing}
				\hspace*{2.5cm}\= \kill
				\texttt{Outlook}			\>	rainy		\\
				\texttt{Temperature} 		\>	mild	 	\\
				\texttt{Humidity}			\>	normal	\\
				\texttt{Wind}			\>	strong
			\end{tabbing}
			\normalsize

			\item \textbf{What is its class?}
			\item Answer: \textbf{No}
		\end{itemize}
	}{0.49}{
		\input{08_decision_trees/01_tikz/tree}
	}
\end{frame}


% Another Decision Tree...
\begin{frame}{Another Decision Tree...}{}
	\bubble{1}{11}{\footnotesize Is this one better?}
	\vspace*{-2mm}
	\input{08_decision_trees/01_tikz/tree_alternative}
\end{frame}


% Section: Iterative Dichotomizer (ID3)
%______________________________________________________________________
\section{Iterative Dichotomizer (ID3)}
\makedivider{Iterative Dichotomizer (ID3)}

% Subsection: Inductive Bias
% --------------------------------------------------------------------------------------------------------
\subsection{Inductive Bias}

% Inductive Bias of Decision Trees
\begin{frame}{Inductive Bias of Decision Trees}{}\important
	\divideTwo{0.75}{
		\begin{itemize}
			\item Complex models tend to \textbf{overfit} the data and \textbf{do not generalize well}
			\item Small decision trees are preferred
			\vspace*{4mm}
			\begin{boxBlueNoFrame}
				\textbf{Occam's razor}: \\
				\footnotesize \textbf{`More things should not be used than are necessary.'}
			\end{boxBlueNoFrame}
			\vspace*{2mm}
			\item \highlight{Prefer the simplest hypothesis that fits the data!}
		\end{itemize}
	}{0.20}{
		\begin{figure}
			\centering
			\fbox{\includegraphics[scale=0.2]{08_decision_trees/02_img/william_of_ockham}}
		\end{figure}
	}
\end{frame}


% The Root of all Evil... Which Attribute to choose?
\begin{frame}{The Root of all Evil... Which Attribute to choose?}{}
	\divideTwo{0.49}{
		\input{08_decision_trees/01_tikz/attribute_split_outlook}
		\vspace*{0.25mm}	
	}{0.49}{
		\input{08_decision_trees/01_tikz/attribute_split_temperature}
		\vspace*{0.25mm}	
	}

	\divideTwo{0.49}{
		\input{08_decision_trees/01_tikz/attribute_split_wind}
	}{0.49}{
		\input{08_decision_trees/01_tikz/attribute_split_humidity}
	}
\end{frame}


% Subsection: Entropy and Information Gain
% --------------------------------------------------------------------------------------------------------
\subsection{Entropy and Information Gain}

% Finding a proper Attribute
\begin{frame}{Finding a proper Attribute}{}
	\divideTwo{0.79}{
		\begin{itemize}
			\item Simple and small trees are preferred
			\begin{itemize}
				\item Data in successor node should be \textbf{as pure as possible}
				\item I.\,e. nodes containing one class only are preferable
			\end{itemize}
			\item \textbf{Question:} How can we express this thought as a mathematical formula?
			\item \textbf{Answer:}
			\begin{itemize}
				\item \highlight{Entropy} (\textit{Claude E. Shannon})
				\item Originates in the field of \textbf{information theory}
			\end{itemize}
		\end{itemize}
	}{0.19}{
		\fbox{\includegraphics[scale=0.3]{08_decision_trees/02_img/claude_shannon}}
	}
\end{frame}


% Measure of Impurity: Entropy
\begin{frame}{Measure of Impurity: Entropy}{}
	\begin{itemize}
		\item Entropy is a measure of chaos in the data (measured in bits)
		\item \textbf{Example:} Consider two classes $A$ and $B$ ($\mathcal{C} = \{ A, B \}$)
	
		\footnotesize
		\begin{tabbing}
			\hspace*{5cm}\=\hspace*{1.5cm}\= \kill
			$E(\{ \bm{A}, \bm{A}, \bm{A}, \bm{A}, \bm{A}, \bm{A}, \bm{A}, \bm{A} \})$
				\>	$\rightarrow$ 0		\>	$Bits$	\\
			$E(\{ \bm{A}, \bm{A}, \bm{A}, \bm{A}, \bm{A}, \bm{A}, B, B \})$
				\>	$\rightarrow$ 0.81 	\>	$Bits$	\\
			$E(\{ \bm{A}, \bm{A}, \bm{A}, \bm{A}, B, B, B, B \})$
				\>	$\rightarrow$ 1		\>	$Bit$		\\
			$E(\{ \bm{A}, \bm{A}, B, B, B, B, B, B \})$
				\>	$\rightarrow$ 0.81 	\>	$Bits$	\\
			$E(\{ B, B, B, B, B, B, B, B \})$	
				\>	$\rightarrow$ 0		\>	$Bits$
		\end{tabbing}
		\normalsize
	\end{itemize}
	
	\begin{boxBlueNoFrame}
		\footnotesize
		\highlight{If both classes are equally distributed, the entropy function $E$ reaches its maximum.
		Pure data sets have minimal entropy}.
	\end{boxBlueNoFrame}
\end{frame}


% Measure of Impurity: Entropy (Ctd.)
\begin{frame}{Measure of Impurity: Entropy (Ctd.)}{}
	\input{08_decision_trees/01_tikz/entropy}
\end{frame}


% Measure of Impurity: Entropy (Ctd.)
\begin{frame}{Measure of Impurity: Entropy (Ctd.)}{}\important
	\vspace*{2mm}
	\begin{boxBlueNoFrame}
		\highlight{Entropy formula:}
		\begin{equation}
			E(\mathcal{D}) = -\sum_{c \in \mathcal{C}} p_c \cdot \log_2 p_c
		\end{equation}
	\end{boxBlueNoFrame}

	\begin{itemize}
		\item $p_c$ denotes the relative frequency of class $c \in \mathcal{C}$	
		\item \textbf{Weather data:}
		\begin{align*}
			\mathcal{C} &= \{ yes, no \} \qquad \text{i.\,e.} \qquad
				p_{yes} = \nicefrac{9}{14} \quad \text{and} \quad p_{no} = \nicefrac{5}{14} \\[3mm]
			E(\mathcal{D})
				&= -\sum_{c \in \mathcal{C}} p_c \cdot \log_2 p_c
	       		   	= - (\nicefrac{9}{14} \cdot \log_2 \nicefrac{9}{14} + \nicefrac{5}{14} \cdot \log_2 \nicefrac{5}{14})
				= \boldsymbol{0.9403}
		\end{align*}
	\end{itemize}
\end{frame}


% Quality of the Split: Average Entropy
\begin{frame}{Quality of the Split: Average Entropy}{}
	\begin{itemize}
		\item We still don't know which attribute to use for the split
		\item Calculate the entropy after each potential split
		\item \highlight{Average Entropy} after splitting by attribute \texttt{A}:
		\begin{equation}
			E(\mathcal{D}, \texttt{A})
				= \sum_{v \in \text{dom}(\texttt{A})}
					\frac{\vert \mathcal{D}_{\texttt{A}=v} \vert}{\vert \mathcal{D} \vert} \cdot E(\mathcal{D}_{\texttt{A}=v})
		\end{equation}
		\item Legend:
		\footnotesize
		\begin{tabbing}
			\hspace*{2.5cm}\= \kill
			\texttt{A}							\>	Attribute												\\
			$\text{dom}(\texttt{A})$				\>	Possible values attribute \texttt{A} can take (domain of \texttt{A})	\\
			$\vert \mathcal{D}_{\texttt{A}=v} \vert$	\>	Number of examples satisfying $\texttt{A} = v$
		\end{tabbing}
		\normalsize
	\end{itemize}
\end{frame}


% Quality of the Split: Average Entropy (Ctd.)
\begin{frame}{Quality of the Split: Average Entropy (Ctd.)}{}
	\textbf{Example:} Attribute \texttt{Outlook}
	\begin{alignat*}{2}
		E(\mathcal{D}, \texttt{Outlook})
			&= \sum_{v \in \text{dom}(\texttt{Outlook})}
				\frac{\vert \mathcal{D}_{\texttt{Outlook} = v} \vert}{\vert \mathcal{D} \vert} \cdot E(\mathcal{D}_{\texttt{Outlook} = v}) 	\\[2mm]
			&= \nicefrac{5}{14} \cdot 0.9710 + \nicefrac{5}{14} \cdot 0.9710 + \nicefrac{4}{14} \cdot 0
			&&= \bm{0.6936}																				\\[2mm]
																										\hline
																										\\[-3mm]
		E(\mathcal{D}_{\texttt{Outlook}=sunny})
			&= -(\nicefrac{2}{5} \cdot \log_2 (\nicefrac{2}{5}) + \nicefrac{3}{5} \cdot \log_2 (\nicefrac{3}{5}))
			&&= 0.9710																					\\[2mm]
		E(\mathcal{D}_{\texttt{Outlook}=rainy})
			&= -(\nicefrac{3}{5} \cdot \log_2 (\nicefrac{3}{5}) + \nicefrac{2}{5} \cdot \log_2 (\nicefrac{2}{5}))
			&&= 0.9710																					\\[2mm]
		E(\mathcal{D}_{\texttt{Outlook}=overcast})
			&= -(\nicefrac{4}{4} \cdot \log_2 (\nicefrac{4}{4}) + \nicefrac{0}{4} \cdot \log_2 (\nicefrac{0}{4}))
			&&= 0
	\end{alignat*}
\end{frame}


% Information Gain
\begin{frame}{Information Gain}{}
	\begin{itemize}
		\item We have calculated the entropy before and after the split
		\item The difference of both is called the \highlight{information gain (IG)}
		\item Select the attribute with the highest IG
		\vspace*{2mm}
		\input{08_decision_trees/03_tbl/ig}
		\vspace*{2mm}
		\item Attribute \texttt{Outlook} maximizes IG
		\item After the split: Remove attribute \texttt{Outlook}
	\end{itemize}
\end{frame}


% Training Data after the Split by Attribute \texttt{Outlook}
\begin{frame}{Training Data after the Split by Attribute \texttt{Outlook}}{}
	\divideTwo{0.49}{
		\vspace*{5mm}
		\input{08_decision_trees/03_tbl/training_data_after_split}
	}{0.49}{
		\begin{itemize}
			\item Data set $\mathcal{D}$ after the split
			\item We obtain three subsets (one per attribute value)
			\item Attribute \texttt{Outlook} is removed
		\end{itemize}
	}
\end{frame}


% Subsection: ID3 Algorithm
% --------------------------------------------------------------------------------------------------------
\subsection{ID3 Algorithm}

% How to proceed?
\begin{frame}{How to proceed?}{}\important
	\begin{itemize}
		\item The algorithm is recursively applied to the resulting subsets
		\begin{enumerate}
			\item Calculate entropy (before and after the split)
			\item Calculate information gain for each attribute 
			\item Choose the attribute with max. information gain for the split
			\item In the current branch: Do not consider the attribute any more
			\item \textbf{Recursion} $\bm{\circlearrowleft}$ (\texttt{Go to 1})
		\end{enumerate}
		\item Recursion stops as soon as the subset is pure
		\item In the example above the subset $\mathcal{D}_{\texttt{Outlook}=overcast}$ is already pure
		\item This algorithm is referred to as \highlight{ID3 (Iterative Dichotomizer)}
	\end{itemize}
\end{frame}


% Step by Step: Construction of the Tree
\begin{frame}{Step by Step: Construction of the Tree}{}
	\input{08_decision_trees/01_tikz/tree_construction_1_result}
\end{frame}


% Step by Step: Construction of the Tree (Ctd.)
\begin{frame}{Step by Step: Construction of the Tree (Ctd.)}{}
	\divideTwo{0.49}{
		\input{08_decision_trees/01_tikz/tree_construction_2_alternative_1}
		\vspace*{0.25mm}
	}{0.49}{
		\vspace*{1mm}
		\input{08_decision_trees/01_tikz/tree_construction_2_alternative_2}
		\vspace*{0.25mm}
	}
	
	\divideTwo{0.49}{
		\input{08_decision_trees/01_tikz/tree_construction_2_alternative_3}
	}{0.49}{
		\begin{itemize}
			\item $IG(\texttt{Temperature}) = 0.571$
			\item $IG(\texttt{Humidity}) = \textbf{0.971}$
			\item $IG(\texttt{Wind}) = 0.020$
		\end{itemize}
	}
\end{frame}


% Step by Step: Construction of the Tree (Ctd.)
\begin{frame}{Step by Step: Construction of the Tree (Ctd.)}{}
	\input{08_decision_trees/01_tikz/tree_construction_2_result}
\end{frame}


% Step by Step: Construction of the Tree (Ctd.)
\begin{frame}{Step by Step: Construction of the Tree (Ctd.)}{}
	\input{08_decision_trees/01_tikz/tree_construction_3_result}
\end{frame}


% Step by Step: Construction of the Tree (Ctd.)
\begin{frame}{Step by Step: Construction of the Tree (Ctd.)}{}
	\input{08_decision_trees/01_tikz/tree_broader}
\end{frame}


% ID3 Algorithm
\begin{frame}[plain]{}{}
	\input{08_decision_trees/04_alg/id3}
\end{frame}


% Section: Extensions and Variants
%______________________________________________________________________
\section{Extensions and Variants}
\makedivider{Extensions and Variants}

% Subsection: Other Measures of Impurity
% --------------------------------------------------------------------------------------------------------
\subsection{Other Measures of Impurity}

% An Alternative to Information Gain: Gini Index
\begin{frame}{An Alternative to Information Gain: Gini Index}{}
	\begin{boxBlueNoFrame}
		\highlight{Gini index:}
		\begin{equation}
			Gini(\mathcal{D})
				= \sum_{c \in \mathcal{C}} p_c \cdot (1 - p_c)
				= 1 -  \sum_{c \in \mathcal{C}} p_c^2
		\end{equation}
	\end{boxBlueNoFrame}
	\begin{itemize}
		\item Used e.\,g. in \highlight{CART (Classification and Regression Trees)}
		\item \textbf{Gini gain} could be defined analogously to IG \\
			{\footnotesize \textit{(usually not done)}}
	\end{itemize}
\end{frame}


% Why not use the Error as a splitting Criterion?
\begin{frame}{Why not use the Error as a splitting Criterion?}{}\optional
	\begin{itemize}
		\item The bias towards pure leaves is \textbf{not strong enough}
		\item \textbf{Example:}
	\end{itemize}
	
	\divideTwo{0.49}{
		\vspace*{2mm}
		\input{08_decision_trees/01_tikz/split_error}
	}{0.49}{
		Error without splitting: \\
		\textbf{20\,\%} \\ \vspace*{-3mm}
		
		Error after splitting: \\
		\textbf{20\,\%} \\ \vspace*{-3mm}
		
		\footnotesize
		\highlight{Both splits don't improve the error. \\
		But together they give a perfect split!}
	}
\end{frame}


% Summary: Impurity Measures
\begin{frame}{Summary: Impurity Measures}{}
	\input{08_decision_trees/01_tikz/impurity_measures_all}
\end{frame}


% Subsection: Highly-branching Attributes
% --------------------------------------------------------------------------------------------------------
\subsection{Highly-branching Attributes}

% Highly-branching Attributes
\begin{frame}{Highly-branching Attributes}{}
	\begin{boxBlueNoFrame}
		\highlight{Attributes with a large number of values are problematic, since the leaves are not
			`backed' with sufficient data examples.}

		\vspace*{4mm}
		\textbf{In extreme cases only one example per node (e.\,g. IDs)}
	\end{boxBlueNoFrame}

	\begin{boxBlueNoFrame}
		This may lead to:
		\begin{itemize}
			\item \textbf{Overfitting} \footnotesize (Selection of attributes which are not optimal for prediction) \normalsize
			\item \textbf{Fragmentation} \footnotesize (Data is fragmented into (too) many small sets) \normalsize
		\end{itemize}
	\end{boxBlueNoFrame}
\end{frame}


% Highly-branching Attributes (Ctd.)
\begin{frame}{Highly-branching Attributes (Ctd.)}{}
	\vspace*{-2mm}
	\input{08_decision_trees/01_tikz/highly_branching_attributes}
	\vspace*{-5mm}
	\begin{itemize}
		\item Entropy before was $0.9403$, Entropy after split is $0$
		\item IG($\mathcal{D}$, \texttt{Day}) = 0.9403
		\item Attribute \texttt{Day} would be chosen for the split \Highlight{$\Rightarrow$ Bad for prediction $\skull$}
	\end{itemize}
\end{frame}


% Highly-branching Attributes (Ctd.)
\begin{frame}{Highly-branching Attributes (Ctd.)}{}
	\begin{itemize}
		\item Calculate the \highlight{intrinsic information (IntI)}:
		\begin{equation}
			IntI(\mathcal{D}, \texttt{A})
				= -\sum_{v \in \text{dom}(\texttt{A})} \frac{\vert \mathcal{D}_{\texttt{A}=v} \vert}{\vert \mathcal{D} \vert} \cdot
					\log_2 \frac{\vert \mathcal{D}_{\texttt{A}=v} \vert}{\vert \mathcal{D} \vert}
		\end{equation}
		\item Attributes with high $IntI$ are \textbf{less useful} (high fragmentation)
		\item New splitting heuristic \highlight{Gain ratio (GR)}:
		\begin{equation}
			GR(\mathcal{D}, \texttt{A}) = \frac{IG(\mathcal{D}, \texttt{A})}{IntI(\mathcal{D}, \texttt{A})}
		\end{equation}
	\end{itemize}
\end{frame}


% Highly-branching Attributes (Ctd.)
\begin{frame}{Highly-branching Attributes (Ctd.)}{}
	\begin{itemize}
		\item Intrinsic information for attribute \texttt{Day}:
		\begin{equation}
			IntI(\mathcal{D}, \texttt{Day}) = 14 \cdot (-\nicefrac{1}{14} \cdot \log_2 (\nicefrac{1}{14})) = \bm{3.807}
		\end{equation}
		\item Gain ratio for attribute \texttt{Day}:
		\begin{equation}
			GR(\mathcal{D}, \texttt{Day}) = \frac{0.9403}{3.807} = \bm{0.246}
		\end{equation}
	\end{itemize}

	\vspace*{-3mm}
	\begin{boxBlueNoFrame}
		\footnotesize
		In this case the attribute \texttt{Day} would still be chosen.
		Be careful what features to include into the training data set! \highlight{(feature engineering is important!)}
	\end{boxBlueNoFrame}
\end{frame}


% Subsection: Numeric Attributes
% --------------------------------------------------------------------------------------------------------
\subsection{Numeric Attributes}

% Handling numeric Attributes
\begin{frame}{Handling numeric Attributes}{}
	\begin{itemize}
		\item Usually, only \textbf{binary splits} are considered, e.\,g.:
		\begin{itemize}
			\item $\texttt{Temperature} < 48$
			\item $\texttt{CPU} > 24$
			\item \textbf{Not:} $24 \le \texttt{Temperature} \le 31$
		\end{itemize}
		\item To support multiple splits, the attribute is \textbf{not removed} \\
			{\footnotesize \textit{(the same attribute can be used again for another split)}}
		\item \Highlight{Problem:} There is an \textbf{infinite number} of possible splits!
		\item \textbf{Solution:} Discretize range (fixed step size, ...)
		\item \highlight{Splitting on numeric attributes is computationally demanding!}
	\end{itemize}
\end{frame}


% Handling numeric Attributes (Ctd.)
\begin{frame}{Handling numeric Attributes (Ctd.)}{}\important
	\begin{itemize}
		\item Consider the attribute \texttt{Temperature}: \\
			Use \textbf{numerical values} instead of discrete values like \textit{cool}, \textit{mild}, \textit{hot}:
	\end{itemize}
	\vspace*{-3mm}
	\input{08_decision_trees/01_tikz/numeric_attributes}
	\divideTwo{0.49}{
		\begin{itemize}
			\item $\texttt{Temperature} < 71.5$ \\
				{\footnotesize yes: 4 | no: 2}
		\end{itemize}
	}{0.49}{
		\begin{itemize}
			\item $\texttt{Temperature} \ge 71.5$ \\
				{\footnotesize yes: 5 | no: 3}
		\end{itemize}
	}
	\vspace*{2mm}
	\begin{equation*}
			E(\mathcal{D}, \texttt{Temp.})
				= \nicefrac{6}{14} \cdot E(\texttt{Temp.} < 71.5) + \nicefrac{8}{14} \cdot E(\texttt{Temp.} \ge 71.5)
				= \bm{0.939}
		\end{equation*}
\end{frame}


% Handling numeric Attributes (Ctd.)
\begin{frame}{Handling numeric Attributes (Ctd.)}{}
	\input{08_decision_trees/03_tbl/numeric_attributes}
\end{frame}


% Subsection: Regression Trees
% --------------------------------------------------------------------------------------------------------
\subsection{Regression Trees}

% Regression Trees
\begin{frame}{Regression Trees}{}\optional
	\begin{itemize}
		\item Prediction of continuous variables
		\item Predict average value of all examples in the leaf
		\item Split the data such that variance in the leaves is minimized
		\item \textbf{Termination criterion is important, otherwise single point per leaf!}
	\end{itemize}
	\begin{boxBlueNoFrame}
		\highlight{Standard deviation reduction (SDR):}
		\vspace*{-1mm}
		\begin{equation}
			SDR(\mathcal{D}, \texttt{A}) = SD(\mathcal{D})
				- \sum_{v \in dom(\texttt{A})} \frac{\vert \mathcal{D}_{\texttt{A}=v} \vert}{\vert \mathcal{D} \vert}
				\cdot SD(\mathcal{D}_{\texttt{A}=v})
		\end{equation}
	\end{boxBlueNoFrame}
\end{frame}


% Iris Demo
%{\setbeamertemplate{navigation symbols}{}
%\begin{frame}[plain]{}{}
%	\begin{tikzpicture}[remember picture,overlay]
%            	\node[at=(current page.center)] {
%                	\includegraphics[width=\paperwidth,height=\paperheight]{08_decision_trees/02_img/iris}
%          	};
%        \end{tikzpicture}
%\end{frame}}


% Section: Ensemble Methods
%______________________________________________________________________
\section{Ensemble Methods}
\makedivider{Ensemble Methods}

% Subsection: Introduction
% --------------------------------------------------------------------------------------------------------
\subsection{Introduction to Ensembles}

% Introduction Ensemble Methods
\begin{frame}{Introduction Ensemble Methods}{}
	\begin{itemize}
		\item \textbf{Key Idea}: Don't learn a single classifier, but a \textbf{set of classifiers}
		\item Combine the predictions of the single classifiers to obtain the final prediction
	\end{itemize}
		
	\begin{boxBlueNoFrame}
		\footnotesize
		\Highlight{Problem:} How can we induce multiple classifiers from a single data set without getting
		the same classifier over and over again? \textbf{We want to have diverse classifiers,
		otherwise the ensemble is useless!}	
	\end{boxBlueNoFrame}
		
	\begin{itemize}
		\item Basic techniques:
		\begin{itemize}
			\item \highlight{Bagging}
			\item \highlight{Boosting}
			\item \highlight{Stacking}
		\end{itemize}
	\end{itemize}
\end{frame}


% What is the Advantage?
\begin{frame}{What is the Advantage?}{}
	\begin{itemize}
		\item Consider the following:
		\begin{itemize}
			\item There are 25 \textbf{independent} base classifiers
			\item \highlight{Independence assumption:}
				Probability of misclassification \textbf{does not} depend on other classifiers in the ensemble
			\item Usually, this assumption does not fully hold in practice
			\item Each classifier has an error rate of $\varepsilon = 0.35$
		\end{itemize}
		\item The ensemble makes a wrong prediction \textbf{if the majority is wrong} \\
			($\Rightarrow$ i.\,e. at least 13)
		\begin{equation}
			\varepsilon_{ensemble}
				= \sum_{u=13}^{25} \binom{25}{u} \cdot \varepsilon^u \cdot (1 - \varepsilon)^{25 - u}
				\approx \bm{0.06 \ll \varepsilon}
		\end{equation}
	\end{itemize}
\end{frame}


% Subsection: Bagging
% --------------------------------------------------------------------------------------------------------
\subsection{Bagging and Randomization}

% Bagging
\begin{frame}{Bagging: General Approach}{}\important
	\vspace*{1.5mm}
	Bagging $\widehat{=}$ \highlight{B}ootstrap \highlight{Agg}regat\highlight{ing}
	\vspace*{-2mm}
	\input{08_decision_trees/01_tikz/bagging}
\end{frame}


% Creating the Bootstrap Samples
\begin{frame}{Creating the Bootstrap Samples}{}
	\begin{itemize}
		\item How to generate multiple data sets which are different?
		\item \textbf{Solution:} Use sampling with replacement
		\vspace*{2mm}
		\input{08_decision_trees/03_tbl/bagging_bootstrap_samples}
		\vspace*{2mm}
		\item Some examples may appear \textbf{in more than one set}
		\item Some examples may appear \textbf{more than once} in one set
		\item Some examples may \textbf{not appear at all}
	\end{itemize}
\end{frame}


% Bagging Algorithm
\begin{frame}[plain]{}{}
	\input{08_decision_trees/04_alg/bagging}
\end{frame}


% Bagging Variations
\begin{frame}{Bagging Variations}{}
	\begin{itemize}
		\item The bootstrap samples had equal size and were drawn with replacement
		\item Also conceivable:
		\begin{enumerate}
			\item \textbf{Varying the size} of the bootstrap samples
			\item Sampling \textbf{without replacement} $\Rightarrow$ \highlight{Pasting}
			\item \textbf{Sampling of features}, not instances
			\begin{itemize}
				\item Not all features are available in all bootstrap samples
				\item This is how \highlight{random forests} work
			\end{itemize}
			\item Creating \textbf{heterogeneous ensembles} \\
				(neural networks, decision trees, support vector machines, ...)
		\end{enumerate}
	\end{itemize}
\end{frame}


% Bagged Decision Trees
\begin{frame}{Bagged Decision Trees}{}
	\begin{figure}
		\includegraphics[scale=0.35]{08_decision_trees/02_img/bagged_decision_trees}
		\vspace*{-4mm}
		\caption{Bagged decision trees; cf. Hastie 2008, page 284}
	\end{figure}
\end{frame}


% Randomization
\begin{frame}{Randomization}{}
	\begin{itemize}
		\item Why not \textbf{randomizing the algorithm} instead of the data?
		\item Some algorithms already do that: E.\,g. neural networks (random initialization of weights)
		\item Especially greedy algorithms can be randomized:
		\begin{itemize}
			\item Pick from the options \textbf{randomly}, instead of picking the best one
			\item E.\,g. decision trees: Do not choose attribute with highest information gain
		\end{itemize}
	\end{itemize}
	
	\vspace*{2mm}
	\begin{boxBlueNoFrame}
		\highlight{A random forest combines randomization and bagging.}
	\end{boxBlueNoFrame}
\end{frame}


% Random Forest Algorithm
\begin{frame}{Random Forest Algorithm}{}
	\divideTwo{0.65}{
		\begin{itemize}
			\item Ensemble of decision trees
			\item Combines \textbf{bagging} and \textbf{random attribute subset selection}
			\item Build decision tree from a bootstrap sample
			\item Select best split attribute among a random subset of $f$ attributes
		\end{itemize}
	
		\footnotesize
		\begin{boxBlueNoFrame}
			A random forest selects the best splitting attributes from the set
			of features available, but the globally best features \textbf{may not} be available.
		\end{boxBlueNoFrame}
	}{0.24}{
		\vspace*{2mm}
		\fbox{\includegraphics[scale=0.7]{08_decision_trees/02_img/forest}}
	}
\end{frame}


% Random Forest Algorithm
\begin{frame}[plain]{}{}
	\input{08_decision_trees/04_alg/rf}
\end{frame}


% ExtraTrees (Randomization 2.0)
\begin{frame}{ExtraTrees (Randomization 2.0)}{}
	\begin{itemize}
		\item One more step of randomization
			$\Rightarrow$ \highlight{Ext}remely \highlight{Ra}ndomized \highlight{Trees}
		\item The general approach is the same as for random forests \\
		\textbf{But:}
		\begin{itemize}
			\item Instead of choosing the optimal split point...
			\item ...it is selected randomly
			\item The decision tree is grown without having to calculate entropy
		\end{itemize}
		\item It is \textbf{much faster} (due to less computation)
	\end{itemize}

	\begin{boxBlueNoFrame}
		\highlight{The large number of classifiers compensates for suboptimal splits.}
	\end{boxBlueNoFrame}
\end{frame}


% Subsection: Boosting
% --------------------------------------------------------------------------------------------------------
\subsection{Boosting}

% Boosting Overview
\begin{frame}{Boosting Overview}{}\important
	\divideTwo{0.19}{
		\vspace*{3mm}
		\fbox{\includegraphics[scale=0.19]{08_decision_trees/02_img/rocket_launch}}
	}{0.80}{
		\vspace*{-6mm}
		\begin{itemize}
			\item \textbf{Key Idea}:
			\begin{itemize}
				\item New classifiers focus on examples misclassified by others
				\item Assign a weight to each classifier (depending on their error)
			\end{itemize}
			\item \textbf{How to}:
				\footnotesize \textit{(unlike bagging, boosting \textbf{cannot} be parallelized!)} \normalsize
			\begin{enumerate}
				\item Initialize example weights with $\nicefrac{1}{n}$
				\item Train a model using the example weights, calculate error and model weight
				\item Update example weights according to model weight
				\item Train the next model using the updated example weights
				\item Combine predictions according to model weights
			\end{enumerate}
		\end{itemize}
	}
\end{frame}


% Boosting: AdaBoost Algorithm
\begin{frame}[plain]{}{}
	\input{08_decision_trees/04_alg/boosting}
\end{frame}


% AdaBoost: Illustration of Weights
\begin{frame}{AdaBoost: Illustration of Weights}{}\optional
	\vspace*{2mm}
	\divideTwo{0.49}{
		\begin{center}
			\footnotesize
			\hspace*{7mm}\textbf{Model weights} $\alpha_u$
		\end{center}
		\vspace*{-7mm}
		\input{08_decision_trees/01_tikz/model_weights}
	}{0.49}{
		\begin{center}
			\footnotesize
			\hspace*{7mm}\textbf{Example weights} $w_i$
		\end{center}
		\vspace*{-7mm}
		\input{08_decision_trees/01_tikz/example_weights}
	}
	
	\footnotesize
	\begin{itemize}
		\item Classifier weights $\alpha_u$ grow exponentially
		\item Classifier weight is 0 if error is $\nicefrac{1}{2}$ (model cannot be trusted, random guessing)
		\item For high errors \highlight{$\Rightarrow$ Do the opposite of what the classifiers predicts}
	\end{itemize}
\end{frame}


% AdaBoost: Two Classes
\begin{frame}{AdaBoost: Two Classes}{}
	\begin{itemize}
		\item With two classes, $\textbf{-1}$ and \textbf{+1}, the final classifier $C^*$ can be written as:
		\begin{equation}
			H(\bm{x}) = sign \left( \sum_{u=1}^k \alpha_u h_u(\bm{x}) \right)
		\end{equation}
		\item $\alpha_u$ is defined as above
		\item The weight update for the examples is given by:
		\begin{equation}
			w_i^{new} = \frac{w_i^{old} \cdot \exp \{ -\alpha_u y^{(i)} h_i(\bm{x}^{(i)}) \}}{\sum_{i=1}^n w_i}
		\end{equation}
	\end{itemize}
\end{frame}


% Boosting Example
\begin{frame}{Boosting Example}{}
	\begin{itemize}
		\item Ensemble of \highlight{decision stumps} ($\widehat{=}$ tree with a single split only)
		\item 10 examples (5 from class \textbf{+1} and 5 from class \textbf{-1})
		\item Each example is initially weighted with a factor of $\nicefrac{1}{10}$
	\end{itemize}

	\begin{figure}
		\includegraphics[scale=0.50]{08_decision_trees/02_img/boosting_example}
	\end{figure}
\end{frame}


% Boosting Example (Ctd.) -- 1st Iteration
\begin{frame}{Boosting Example (Ctd.)}{$1^{st}$ Iteration}
	\begin{figure}
		\includegraphics[scale=0.60]{08_decision_trees/02_img/boosting_first_iteration}
	\end{figure}
\end{frame}


% Boosting Example (Ctd.) -- 2nd Iteration
\begin{frame}{Boosting Example (Ctd.)}{$2^{nd}$ Iteration}
	\begin{figure}
		\includegraphics[scale=0.60]{08_decision_trees/02_img/boosting_second_iteration}
	\end{figure}
\end{frame}


% Boosting Example (Ctd.) -- 3rd Iteration
\begin{frame}{Boosting Example (Ctd.)}{$3^{rd}$ Iteration}
	\begin{figure}
		\includegraphics[scale=0.60]{08_decision_trees/02_img/boosting_third_iteration}
	\end{figure}
\end{frame}


% Boosting Example (Ctd.)
\begin{frame}{Boosting Example (Ctd.)}{Prediction}
	\vspace*{-2mm}
	\begin{figure}
		\includegraphics[scale=0.45]{08_decision_trees/02_img/boosting_prediction}
	\end{figure}
\end{frame}


% Subsection: Stacking
% --------------------------------------------------------------------------------------------------------
\subsection{Stacking}

% Combining Predictions
\begin{frame}{Combining Predictions}{}
	\begin{enumerate}
		\item \highlight{Voting} \checkmark
		\begin{itemize}
			\item Each classifier votes for one of the classes
			\item Each classifier has the same (model) weight
			\item This is \textbf{Bagging}
		\end{itemize}
		\item \highlight{Weighted Voting} \checkmark
		\begin{itemize}
			\item Weight the individual predictions with model weight
			\item This is \textbf{Boosting}
		\end{itemize}
		\item \highlight{Stacking}
		\begin{itemize}
			\item Use a \textbf{meta classifier} to combine the predictions
			\item See next slides...
		\end{itemize}
	\end{enumerate}
\end{frame}


% Stacking Algorithm
\begin{frame}[plain]{}{}
	\input{08_decision_trees/04_alg/stacking}
	
	\vspace*{5mm}
	\footnotesize
	(*) Meta data set:\;
	\begin{itemize}
		\item \textbf{Labels}: \\
			Use the same labels as in the original data set
		\item \textbf{Attributes}: \\
			One attribute for each base classifier ($k$ attributes) \\
			The attribute values are the predictions of the corresponding classifiers
	\end{itemize}
\end{frame}


% Stacking Example
\begin{frame}{Stacking Example}{}
	\divideTwo{0.49}{
		\vspace*{2mm}
		\input{08_decision_trees/03_tbl/stacking_original_dataset}
	}{0.49}{
		\vspace*{1mm}
		\input{08_decision_trees/03_tbl/stacking_predictions}
	}
	\vspace*{-6mm}
	\input{08_decision_trees/03_tbl/stacking_meta_dataset}
\end{frame}


% Subsection: Error-correcting Output Codes
% --------------------------------------------------------------------------------------------------------
\subsection{Error-correcting Output Codes}

% Multi-Class-Problems
\begin{frame}{Multi-Class-Problems}{}\optional
	\begin{itemize}
		\item Usually, a \textbf{class binarization} technique is needed
		\item One-vs-Rest (one classifier for each class)
		\item Each class is encoded in a \textbf{bit-string}
	\end{itemize}
	
	\vspace*{3mm}
	\divideTwo{0.49}{
		\vspace*{4mm}
		\input{08_decision_trees/03_tbl/class_binarization}
	}{0.49}{
		\footnotesize
		\begin{boxBlueNoFrame}
			What would you do if the result were \textbf{1100}?
			It is not clear if the new instance belongs to class \textbf{a}
			or class \textbf{b}. \\
			\highlight{Solution: Use error-correcting output codes instead!}
		\end{boxBlueNoFrame}
	}
\end{frame}


% Error-Correcting Output Codes (ECOCs)
\begin{frame}{Error-Correcting Output Codes (ECOCs)}{}\optional
	\begin{itemize}
		\item Use code words that have a high pairwise \highlight{Hamming distance} $d$
		\item Can correct up to $(d - 1) / 2$ single bit-errors
		\item In this case: Use seven classifiers
		\input{08_decision_trees/03_tbl/ecoc}
		\item Predicted code-word: \textbf{1011111} $\Rightarrow$ What is the true class??? \\
			\textit{Probably class \textbf{a}, the second classifier made a mistake...}
	\end{itemize}
\end{frame}


% ECOCs: How to encode the classes?
\begin{frame}{ECOCs: How to encode the classes?}{}\optional
	\divideTwo{0.15}{
		\vspace*{5mm}
		\fbox{\includegraphics[scale=0.60]{08_decision_trees/02_img/binary}}
	}{0.84}{
		\begin{itemize}
			\item Criteria for the code words
			\begin{itemize}
				\item \textbf{Row separation} guarantees that errors can be corrected
				\item \textbf{Column separation}
					(identical columns $\Rightarrow$ Classifier makes same error several times)
			\end{itemize}
			\item \highlight{Exhaustive Codes}
			\begin{itemize}
				\item \textbf{Class 1:} Code word consists of 1s only
				\item \textbf{Class 2:} $2^{k-2}$ 0s followed by $2^{k-2}-1$ 1s
					($k\ \widehat{=}$ \# classes)
				\item \textbf{Class i:} Alternating runs of $2^{k-i}$ 0s and 1s \\
					(Last run is one bit shorter than the others)
			\end{itemize}
		\end{itemize}
	}
\end{frame}


% Section: Wrap-Up
%______________________________________________________________________
\section{Wrap-Up}
\makedivider{Wrap-Up}

% Subsection: Summary
% --------------------------------------------------------------------------------------------------------
\subsection{Summary}

% Summary
\begin{frame}{Summary}{}
	\begin{itemize}
		\item \textbf{Decision trees:}
		\begin{itemize}
			\item The construction of decision trees is guided by an \textbf{impurity measure}, \\
				e.\,g. entropy or Gini
			\item Recursively select features which \textbf{maximize the information gain}
			\item Decision trees can handle \textbf{numeric attributes} and \textbf{continuous output}
		\end{itemize}
		\item \textbf{Ensembles:}
		\begin{itemize}
			\item \textbf{Bagging:} Sample diverse data sets from underlying data
			\item \textbf{Boosting:} Focus on instances misclassified by earlier classifiers
			\item \textbf{Stacking:} Learn a meta-classifier on top of base classifiers
			\item \textbf{Random forests} combine bagging with randomization
		\end{itemize}
	\end{itemize}
\end{frame}


% Subsection: Self-Test Questions
% --------------------------------------------------------------------------------------------------------
\subsection{Self-Test Questions}

% Self-Test Questions
\begin{frame}{Self-Test Questions}{}\important
	\begin{enumerate}
		\item What is an inductive bias? What is the inductive bias of decision trees?
		\item Explain what Occam's razor is.
		\item What does entropy measure? How do you compute the information gain?
		\item True or false: \textit{'Pure data sets have maximal entropy.'}
		\item What is the advantage of ensemble methods?
		\item What distinguishes bagging from boosting?
		\item Explain what a random forest does.
	\end{enumerate}
\end{frame}


% Subsection: Lecture Outlook
% --------------------------------------------------------------------------------------------------------
\subsection{Lecture Outlook}

\begin{frame}{What's next...?}{}
	\makeoverview{7}
\end{frame}


% Subsection: Recommended Literature and further Reading
% --------------------------------------------------------------------------------------------------------
\subsection{Recommended Literature and further Reading}

% Literature
%______________________________________________________________________
\begin{frame}[allowframebreaks]{Recommended Literature and further Reading}{}
	\footnotesize
	\begin{thebibliography}{2}
		\literature{book}{Hastie et al. 2008}{[1] The Elements of Statistical Learning}
			{Hastie et al. Springer. 2008.}{$\rightarrow$ \href{
				https://web.stanford.edu/~hastie/Papers/ESLII.pdf
			}{\linkstyle{Link}}, cf. chapters 8.7, 8.8, 9.2, 10.1}\\
		\literature{book}{Tom M. Mitchell 1997}{[2] Machine Learning}
			{Tom M. Mitchell. McGraw-Hill. 1997.}{$\rightarrow$ \href{
				https://www.cs.ubbcluj.ro/~gabis/ml/ml-books/McGrawHill\%20-\%20Machine\%20Learning\%20-Tom\%20Mitchell.pdf
			}{\linkstyle{Link}}, cf. chapter 3}\\
		\literature{online}{Dave Sotelo, Online Blog}{[3] Using Bagging and Boosting to Improve Classification Tree Accuracy}{Dave Sotelo, Online Blog}{$\rightarrow$ \href{https://towardsdatascience.com/using-bagging-and-boosting-to-improve-classification-tree-accuracy-6d3bb6c95e5b}{\linkstyle{Link}}}
		\literature{book}{Bishop.2006}{[4] Pattern Recognition and Machine Learning}
			{Christopher Bishop. Springer. 2006.}{$\rightarrow$ \href{
				http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop\%20-\%20Pattern\%20Recognition\%20And\%20Machine\%20Learning\%20-\%20Springer\%20\%202006.pdf
			}{\linkstyle{Link}}, cf. chapter 14.3}
	\end{thebibliography}
\end{frame}


% Subsection: Meme of the Day
% --------------------------------------------------------------------------------------------------------
\subsection{Meme of the Day}

% Meme of the Day
\begin{frame}{Meme of the Day}{}
	\begin{figure}
		\includegraphics[scale=0.65]{08_decision_trees/02_img/meme_of_the_day}
	\end{figure}
\end{frame}


% Thank you
%______________________________________________________________________
\makethanks

\end{document}