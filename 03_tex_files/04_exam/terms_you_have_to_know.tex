\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{multicol}
\usepackage{fullpage}
\usepackage{marginnote}

\pagestyle{fancy}
\fancyhf{}

\rhead{Page \thepage}
\lhead{150+ terms you have to know...}
\rfoot{}
\lfoot{}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0.5\baselineskip}
\addtolength{\topskip}{1.5cm}

\newcommand{\mn}[1]{\marginnote{\tiny #1}}

% begin of document
\begin{document}

\begin{multicols*}{2}

% ML Introduction
\subsection*{Introduction}

{
\reversemarginpar
\mn{B}
batch learning

\mn{C}
curse of dimensionality

\mn{D}
decision boundary

\mn{F}
feature / attribute

\mn{G}
generalization

\mn{L}
label

\mn{M}
machine learning

model / hypothesis

\mn{O}
Occam's razor

online learning

overfitting

\mn{P}
prediction

predictor

\mn{R}
reinforcement learning

\mn{S}
semi-supervised learning

supervised learning

\mn{U}
underfitting

unsupervised learning
}


% Decision Theory
\subsection*{Decision Theory}

{\reversemarginpar
\mn{B}
Bayes optimal

Bayes' theorem

\mn{C}
class conditional probability

class prior probability

conditionally independent
}

\mn{E}
error minimization

\mn{L}
Laplace smoothing

loss function

\mn{N}
na\"{i}ve Bayes

\mn{R}
risk minimization

\mn{S}
scaling


% Density Estimation
\subsection*{Density Estimation}

\mn{B}
Bayesian information criterion (BIC)

binning

\mn{E}
EM algorithm

\mn{G}
Gaussian distribution

Gaussian kernel

Gaussian mixture model

\mn{H}
histogram

\mn{I}
independent and identically distributed (iid)

\mn{K}
kernel density estimation

\mn{L}
latent variable

likelihood

log-likelihood

\mn{M}
maximum likelihood estimation

mixture model

multivariate Gaussian distribution

\mn{N}
nearest neighbors

non-parametric models

\mn{O}
observed variable

{\reversemarginpar
\mn{P}
parametric models

Parzen window
}


% Regression
\subsection*{Regression}

{\reversemarginpar
\mn{B}
basis function (polynomial, RBF)

batch gradient descent

\mn{D}
data input space

design matrix / regressor matrix

\mn{E}
elastic net

\mn{F}
feature mapping

\mn{G}
gradient descent

\mn{H}
hypothesis space

\mn{L}
Lasso regression

learning rate ($\alpha$)

\mn{M}
mini-batch gradient descent

\mn{N}
noise

normal equation

\mn{O}
ordinary least squares (OLS)

\mn{P}
precision

probabilistic regression

\mn{R}
regression

regularization

ridge regression

\mn{S}
squared error

stochastic gradient descent
}


% Classification
\subsection*{Classification}

\mn{A}
activation (function)

AdaBoost

average entropy

\mn{B}
backpropagation

bagging

batch-normalization

binary classification

boosting

\mn{C}
classification and regression trees (CART)

convolutional neural network (CNN)

cosine similarity

cross entropy

\mn{D}
decision tree

deep learning

distance metric

\mn{E}
ensemble methods

entropy

epoch

Euclidean distance

ExtraTrees

\mn{F}
fully connected neural network

\mn{G}
Gain ratio (GR)

Gini index

\mn{H}
hidden layer

hinge loss

{\reversemarginpar
\mn{I}
ID3

information gain (IG)

intrinsic information (IntI)

\mn{K}
$k$-nearest neighbors

\mn{L}
lazy learning

\mn{M}
Manhattan distance

Minkowski distance

multi-class classification

Multi-layer perceptron (MLP)

\mn{N}
negative log-likelihood

neural network

\mn{O}
one-hot encoding

one-vs-one (OvO)

one-vs-rest (OvR)

\mn{P}
Perceptron

Perceptron convergence theorem

pooling

pre-activation

\mn{R}
random forest

regression trees

rectified linear unit (ReLU)

recurrent neural network (RNN)

\mn{S}
sigmoid function ($\sigma$)

softmax

stacking

standard deviation reduction (SDR) \\
}

\mn{T}
tangent hyperbolic (tanh)

\mn{W}
word2vec


% Evaluation
\subsection*{Evaluation}

\mn{A}
accuracy

area-under-the-curve (AUC)

\mn{B}
bias

\mn{C}
confusion matrix

cross-validation (X-Val)

\mn{D}
dev split

\mn{E}
early stopping

\mn{F}
$F_1$-score

\mn{G}
grid search

\mn{L}
leave-one-out cross-validation (LOO X-Val)

\mn{M}
macro average

mean absolute error (MAE)

micro average

\mn{O}
out-of-sample testing

\mn{P}
precision

precision-recall curve

\mn{R}
random search

recall

receiver operating characteristic (ROC)

root mean square error (RMSE)

\mn{S}
stratified split

\mn{T}
test split

train split

{\reversemarginpar
\mn{V}
variance
}


% Unsupervised Learning
\subsection*{Unsupervised Learning}

{\reversemarginpar
\mn{A}
affinity-based clustering

agglomerative clustering

\mn{C}
centroid

cluster

clustering

complete linkage

\mn{D}
dendrogram

dimensionality reduction

\mn{E}
eigenvalue problem

elbow method

EM-based clustering

\mn{H}
hierarchical clustering

\mn{K}
$k$-means

\mn{M}
maximum variance formulation

\mn{O}
orthogonal projection

\mn{P}
principal component analysis (PCA)

\mn{S}
single linkage

\mn{V}
vector quantization

voronoi diagram
}

\end{multicols*}

\end{document}