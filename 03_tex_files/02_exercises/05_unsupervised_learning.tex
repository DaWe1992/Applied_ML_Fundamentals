\input{preamble_theme}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usepackage[bottom]{footmisc}

% title
\title{Exercise 5 - Backpropagation and Unsupervised Learning}

% ====================================================
% ====================================================
% BEGIN OF DOCUMENT
% ====================================================
% ====================================================
\begin{document}
\AddToShipoutPicture*{\BackgroundPic}

\maketitle
\medskip
\newpage
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{pystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}
 
\lstset{style=pystyle}

% Backprop
%______________________________________________________________________
\section{Backpropagation}

\begin{enumerate}[label=\alph*)]

% Task 1
\exercise{Backpropagation by Hand (5 points)}{You are given the following neural network. Compute one forward pass and backward pass based on the labeled training example $(\bf{x} = (0.05,\,0.10), \bf{y} = (0.01, 0.99))$. The weight matrices and bias values are initialized as follows (where for instance the weight $W_{12}$ connects the input $x_2$ with hidden layer neuron $1$ and the weight $V_{21}$ connects hidden layer neuron $2$ with output layer neuron $1$):
\begin{align*}
W &= \begin{pmatrix}
0.15 & 0.20\\
0.25 & 0.30
\end{pmatrix}\\
b_1 &= 0.35\\
V &= \begin{pmatrix}
0.40 & 0.45\\
0.50 & 0.55
\end{pmatrix}\\
b_2 &= 0.60
\end{align*}
Use the activation functions mentioned above the hidden layer (ReLU) and output layer (Sigmoid). Show your calculations, annotate the visual illustration of the network with the gradients at the single neurons (derivative of the activations) and state the gradients for the weights $w_{12}$, $v_{21}$.
}{1cm}

\def\layersep{2.5cm}
\begin{figure}[h]
\centering
\begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep]
    \tikzstyle{every pin edge}=[<-,shorten <=1pt]
    \tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
    \tikzstyle{input neuron}=[neuron, fill=green!50];
    \tikzstyle{output neuron}=[neuron, fill=red!50];
    \tikzstyle{hidden neuron}=[neuron, fill=blue!50];
    \tikzstyle{annot} = [text width=6em, text centered]

    % Draw the input layer nodes
    \foreach \name / \y in {1,...,2}
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
        \node[input neuron, pin=left:\hphantom{1em}] (I-\name) at (0,-\y) {$x_\y$};
    % Le bias
    \node[input neuron] (I-bias) at (0,-3cm) {$b_1$};

    % Draw the hidden layer nodes
    \foreach \name / \y in {1,...,2}
        \path[yshift=0cm]
            node[hidden neuron] (H-\name) at (\layersep,-\y cm) {$p_\y$};
    % Le bias
    \path[yshift=0cm]
        node[hidden neuron] (H-bias) at (\layersep,-3cm) {$b_2$};

    % Draw the output layer nodes
    \foreach \name / \y in {1,...,2}
        \path[yshift=0cm]
             node[output neuron, pin={[pin edge={->}]right:\hphantom{1em}}] (O-\name) at (\layersep+\layersep,-\y cm) {$y_\y$};

    % Connect every node in the input layer with every node in the
    % hidden layer.
    \foreach \source in {1,...,2}
        \foreach \dest in {1,...,2}
            \path (I-\source) edge (H-\dest);
            
    \foreach \dest in {1,...,2}
        \path (I-bias) edge (H-\dest);

    % Connect every node in the hidden layer with the output layer
    \foreach \source in {1,...,2}
        \foreach \dest in {1,...,2}
            \path (H-\source) edge (O-\dest);
            
    \foreach \dest in {1,...,2}
        \path (H-bias) edge (O-\dest);

    % Annotate the layers
    \node[annot,above of=H-1, node distance=2cm] (hl) {Hidden layer (ReLU)};
    \node[annot,left of=hl] {Input layer};
    \node[annot,right of=hl] (ol) {Output layer (Sigmoid)};
    \node[annot,below of=hl,left of=hl, node distance=1.3cm] (matw) {$W$};
    \node[annot,below of=hl,left of=ol, node distance=1.3cm] (matw) {$V$};
\end{tikzpicture}
\caption{Multi-layer perceptron to be used for one forward and backward pass}
\end{figure}


% Solution:


\end{enumerate}
\pagebreak

% Clustering
%______________________________________________________________________
\section{Unsupervised Learning}

\begin{enumerate}[label=\alph*)]

% Task 1
\exercise{k-Means clustering (4 points)}{Implement k-Means clustering for image compression. Compress the example image file \textit{data/dhbw.jpg}. You find an explanation of how image compression with k-Means works under the following \href{https://medium.com/@agarwalvibhor84/image-compression-using-k-means-clustering-8c0ec055103f}{link\footnote{\url{https://medium.com/@agarwalvibhor84/image-compression-using-k-means-clustering-8c0ec055103f}}}.}{5cm}

% Task 2
\exercise{PCA (1 point)}{Explain how to choose the number of eigenvectors to use for principal component analysis. Why does this work?}{5cm}

% Bonus
\exercise{Bonus Question: Spectral clustering (1 point)}{How can you automatically choose the number of clusters for spectral clustering?}{5cm}

% Solution:


\end{enumerate}

\end{document}