\input{preamble_theme}

% ====================================================
% ====================================================
% BEGIN OF DOCUMENT
% ====================================================
% ====================================================
\begin{document}

\mytitle{Exercise 4 - Classification}
\myauthor{name1, name2, name3}
\firstpage{\insertmytitle}{Winter term 2019/2020}{\insertmyauthor}

% Decision Trees
%______________________________________________________________________
\section{Decision Trees}

\begin{enumerate}[label=\alph*)]

% Task 1
\exercise{ID3 Decision Tree Construction (3 points)}{
You are given the following labeled data set. Construct a decision tree classifier using pen and paper.
Apply the information gain splitting heuristic. Constructing the first two levels of the tree is sufficient.
Draw the tree and indicate each splitting attribute. Show your calculations.
\vspace*{4mm}
\begin{table}[h]
	\centering
    	\begin{tabular}{| c  c  c  c | c |}
    		\hline
   	 	\texttt{Outlook} 	&
   	 	\texttt{Temperature} 	&
   	 	\texttt{Humidity} 	&
   	 	\texttt{Wind} 		&
   	 	\texttt{Sport} 		\\ \hline
    		sunny 	& cold 		& high 	& weak 	& soccer 		\\
    		cloudy 	& cold 		& low 	& strong 	& soccer 		\\
    		sunny 	& warm 		& low 	& weak 	& soccer 		\\ \hline
    		rainy 		& cold 		& high	& weak 	& squash 		\\
    		sunny 	& cold 		& high 	& weak 	& squash 		\\  
    		rainy 		& warm 		& high 	& strong 	& squash 		\\
    		cloudy 	& cold 		& high 	& weak 	& squash 		\\
    		rainy 		& warm 		& high 	& weak 	& squash 		\\ \hline
    		cloudy 	& warm 		& high 	& weak 	& tennis 		\\ 
    		cloudy 	& cold 		& low 	& strong 	& tennis 		\\
    		sunny 	& cold 		& low 	& strong 	& tennis 		\\
    		cloudy	& cold 		& high 	& weak 	& tennis 		\\ \hline
    	\end{tabular}
\end{table}
}{
% >>>> your answer here <<<<
\vspace*{7cm}
}

\end{enumerate}

\newpage


% Neural Networks
%______________________________________________________________________
\section{Neural Networks}

\begin{enumerate}[label=\alph*)]

% Task 2
\exercise{Hyperparameter exploration (2 points)}{
On the \textit{TensorFlow Playground} webpage\footnote{\url{https://playground.tensorflow.org}} try varying the hyper-parameters of an MLP (\# hidden layers, \# neurons per layer) using the \textit{`Circle'} classification data set.
Does it work better to \ding{182} use more neurons near the input layer, \ding{183} more neurons towards the last hidden layer or \ding{184} use the same number of neurons in each hidden layer?
Provide a justification for why \ding{182}, \ding{183} or \ding{184} might work better. Report the best configuration which you found. Can a perceptron separate the circular dataset? Why or why not?
\vspace*{4mm}
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.6]{01_img/tf_playground_settings}
	\caption{Mandatory settings on TensorFlow Playground for this exercise}
\end{figure}
}{
% >>>> your answer here <<<<
}

\newpage


% Task 3
\exercise{Multi-Layer Perceptron for Sentiment Analysis (5 points)}{
Implement an MLP to classify movie reviews into either positive or negative sentiment using the deep learning library \texttt{PyTorch}.\footnote{\url{https://pytorch.org/}}
The data is stored in the folder \cb{\texttt{/data}}. Each line $i$ in \cb{\texttt{labels.txt}} contains the label of the $i$-th movie review in \cb{\texttt{reviews.txt}}.
You have to map each of the movie review texts to a fixed-size embedding vector, which you can use as input to your MLP. You can do this by using the \texttt{flair} library.\footnote{\url{https://github.com/flairNLP/flair}}
Install it by running \cb{\texttt{pip install flair}}. Perform a 3-fold cross-validation / random subsample validation and report precision and recall. Also, report your hyper-parameter configuration (learning rate, batch size, network structure, etc.).
}{
% >>>> your answer here <<<<
\vspace*{7cm}
}




% Task 4
\exercise{Bonus Question: Contextualized Word Embeddings (1 point)}{
Read the paper \textit{`Deep contextualized word representations'}\footnote{Peters et al., \url{https://arxiv.org/pdf/1802.05365.pdf}} and answer the following questions:
What is the most important difference between word2vec embeddings and the ELMo model proposed in the paper? Why does this difference have a large effect on the quality of the resulting word embeddings?
}{
% >>>> your answer here <<<<
}

\end{enumerate}

\end{document}