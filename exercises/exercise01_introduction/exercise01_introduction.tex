\input{../exercise_sheets_theme}

% title and author
\title{Exercise 1 - Introduction}

% begin of document
\begin{document}
\AddToShipoutPicture*{\BackgroundPic}

\maketitle
\medskip
\newpage

\section{Linear Algebra Refresher}
\begin{enumerate}[label=\alph*)]
\exercise{Matrix Operations (1 point)}{A fellow student suggests that matrix addition and multiplication are very similar to scalar addition and multiplication, i.\,e. commutative, associative and distributive. Is this a correct statement? Prove it mathematically or disprove it by providing at least one counter example per property (commutativity, associativity, distributivity).}{14.5cm}

\exercise{Matrix Inverse (1 point)}{What is a matrix inverse? How can you build the inverse of a non-square matrix? You would like to invert a matrix $M \in \mathbb{R}^{2 \times 3}$ - write down the equation for computing it and specify the dimensionality of the matrices after each single operation (e.g. multiplication, inverse).}{7cm}

\exercise{Eigenvectors and Eigenvalues (1 point)}{Explain what eigenvectors and eigenvalues of a matrix $M$ are. Why are they relevant in machine learning?}{7cm}

\end{enumerate}

\section{Statistics Refresher}
\begin{enumerate}[label=\alph*)]
\exercise{Terminology (1 point)}{What is a random variable? What is a probability density function (PDF)? What is a probability mass function (PMF)? What do a PDF and a PMF tell us about a random variable?}{7cm}

\exercise{Expectation and Variance (1 point)}{State the general definition of expectation and variance for the probability density $f : \Omega \rightarrow \mathbb{R}$ of a continuous random variable. What do expectation and variance express?}{7cm}

\end{enumerate}

\section{Optimization}
\begin{enumerate}[label=\alph*)]
\exercise{Numerical Optimization - Gradient Descent (5 points)}{Implement a simple gradient descent algorithm for finding a minimum of the Rosenbrock function with $n = 2$ using Python and NumPy: $$f(x) = \sum_{i = 0}^{n - 1} \left[100 (x_{i+1} - x_i^2)^2 + (x_i - 1)^2\right]$$ Submit your code and a plot of the learning curve for the best run of your gradient descent implementation. Which learning rate worked best? (Hint: You need to find the first derivative(s) of $f(x)$ for $n = 2$ and iteratively evaluate them during gradient descent. Automatic differentiation tools are not allowed for this exercise.)}{7cm}

\end{enumerate}

\end{document}