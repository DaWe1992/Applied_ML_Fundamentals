\input{preamble_theme}

% ====================================================
% ====================================================
% PRESENTATION DATA
% ====================================================
% ====================================================

\title[Decision Theory]{*** Applied Machine Learning Fundamentals *** Decision Theory}
\institute{SAP\,SE}
\author{Daniel Wehner}
\date{\today}
\prefix{DT}

% ====================================================
% ====================================================
% BEGIN OF DOCUMENT
% ====================================================
% ====================================================

\begin{document}

% Title frame
%______________________________________________________________________
\maketitlepage


% Agenda
%______________________________________________________________________
\begin{frame}{Agenda \today}
	\begin{multicols}{2}
		\tableofcontents
	\end{multicols}
\end{frame}


% Section: Bayesian Decision Theory
%______________________________________________________________________
\section{Bayesian Decision Theory}
\makedivider{Bayesian Decision Theory}

% Subsection: Introduction
% --------------------------------------------------------------------------------------------------------
\subsection{Introduction}

% Statistical Methods
\begin{frame}{Statistical Methods}{}
	\begin{itemize}
		\item Statistical methods assume that the process that `generates' the data is governed by the
			\highlight{rules of probability}
		\item The data is understood to be a set of \highlight{random samples} from some
			underlying \highlight{probability distribution}
		\item The basic assumption about how the data is generated is always there, even if you don' t see
			a single probability distribution
		\item This is the reason for the name \highlight{statistical machine learning}
	\end{itemize}
\end{frame}


% Running Example: Optical Character Recognition (OCR)
\begin{frame}{Running Example: Optical Character Recognition (OCR)}{}
	\begin{figure}
		\centering
		\includegraphics[scale=0.6]{03_decision_theory/02_img/characters}
	\end{figure}
	\vspace*{-1mm}
	\footnotesize
	\Highlight{Goal: Classify a new letter so that the probability of a wrong classification is minimized}
\end{frame}


% Subsection: Class Conditional Probabilities
% --------------------------------------------------------------------------------------------------------
\subsection{Class Conditional Probabilities}

% Class Conditional Probabilities
\begin{frame}{Class Conditional Probabilities}{}
	\begin{itemize}
		\item First concept: \highlight{Class conditional probabilities}
		\item Probability of $\bm{x}$ given a specific class $\mathcal{C}_k$ is formally written as:
		\begin{equation}
			p(\bm{x} \vert \mathcal{C}_k) \in [0, 1]
		\end{equation}
		\item $\bm{x} \in \mathbb{R}^m$ is a feature vector, e.\,g. \#\,black pixels, height-width ratio, ...
		\begin{figure}
			\centering
			\includegraphics[scale=0.3]{03_decision_theory/02_img/conditional_probabilities}
		\end{figure}
	\end{itemize}
\end{frame}


% Class Conditional Probabilities (Ctd.)
\begin{frame}{Class Conditional Probabilities (Ctd.)}{}
	\divideTwo{0.49}{
		\begin{figure}
			\centering
			\includegraphics[scale=0.4]{03_decision_theory/02_img/conditional_probabilities_overlap1}
		\end{figure}
	}{0.49}{
		\begin{figure}
			\centering
			\includegraphics[scale=0.4]{03_decision_theory/02_img/conditional_probabilities_overlap2}
		\end{figure}
	}
	\vspace*{3mm}
	\begin{boxBlueNoFrame}
		If $\bm{x} = 15$ we would predict class $a$ since $p(15 \vert a) > p(15 \vert b)$. \\
		If $\bm{x} = 25$ we would output class $b$ since $p(25 \vert b) > p(25 \vert a)$.
	\end{boxBlueNoFrame}
\end{frame}


% Class Conditional Probabilities (Ctd.)
\begin{frame}{Class Conditional Probabilities (Ctd.)}{}
	\bubble{12}{5}{
		\scriptsize We have a problem!
	}
	\begin{figure}
		\centering
		\includegraphics[scale=0.5]{03_decision_theory/02_img/conditional_probabilities_overlap3}
	\end{figure}
	\vspace*{-3mm}
	\begin{itemize}
		\item \highlight{Which class should be chosen now?}
		\item The conditional probabilities are the same... $\skull$
	\end{itemize}
\end{frame}


% Subsection: Class Priors
% --------------------------------------------------------------------------------------------------------
\subsection{Class Priors}

% Class Prior Probabilities
\begin{frame}{Class Prior Probabilities}{}
	\bubble{10}{10}{
		\footnotesize How would you decide now?
	}
	\begin{itemize}
		\item Second concept: \highlight{Class priors}
		\item The prior probability of a data point belonging to a particular class $\mathcal{C}$
		\begin{alignat*}{2}
			\mathcal{C}_1 &\equiv a \qquad p(\mathcal{C}_1) &&= 0.75 \\
			\mathcal{C}_2 &\equiv b \qquad p(\mathcal{C}_2) &&= 0.25
		\end{alignat*}
		\item By definition:
		\begin{itemize}
			\item $0 \le p(\mathcal{C}_k) \le 1,\ \forall k$
			\item The sum of all probabilities equals one: $\sum_{k=1}^{\vert \mathcal{C} \vert} p(\mathcal{C}_k) = 1$
		\end{itemize}
		\item \highlight{The class prior is equivalent to a prior belief in the class label}
	\end{itemize}
\end{frame}


% How to get the Prior Probabilities?
\begin{frame}{How to get the Prior Probabilities?}{}
	\bubble{11}{5}{
		\footnotesize Don't count \textcolor{orange}{\textbf{oranges}}!
	}
	\vspace*{4mm}
	\begin{figure}
		\centering
		\includegraphics[scale=0.6]{03_decision_theory/02_img/count_count_bubble}
	\end{figure}
	\begin{textblock}{100}(4,4.5)
		\footnotesize \highlight{Count Count's advice:}
	\end{textblock}
	\begin{textblock}{100}(4,6)
		\textbf{Simply count the} \\
		\textbf{number of instances} \\
		\textbf{in each class!}
	\end{textblock}
\end{frame}


% Subsection: Bayes' Theorem
% --------------------------------------------------------------------------------------------------------
\subsection{Bayes' Theorem}

% Bayesian Decision Theory: Bayes' Theorem
\begin{frame}{Bayesian Decision Theory: Bayes' Theorem}{}
	\begin{itemize}
		\item What we actually want to compute: $P(\mathcal{C}_k \vert \bm{x}) \Rightarrow$
			\highlight{Posterior probability}
		\item We can compute it by applying \highlight{Bayes' theorem}
		\item This is one of the \Highlight{most important formulas (!!!)}
	\end{itemize}

	\begin{boxBlue}
		\begin{equation}
			\overbracket{p(\mathcal{C}_k \vert \bm{x})}^{\text{\highlight{Class posterior}}}
				= \frac{
					\overbracket{p(\bm{x} \vert \mathcal{C}_k)}^{\text{\highlight{Class cond.}}}
					\cdot
					\overbracket{p(\mathcal{C}_k)}^{\text{\highlight{Class prior}}}
				}{
					\underbracket{p(\bm{x})}_{\text{\highlight{Normalization term}}}
				}
				= \frac{
					p(\bm{x} \vert \mathcal{C}_k) \cdot p(\mathcal{C}_k)
				}{
					\sum_{j=1}^{\vert \mathcal{C} \vert} p(\bm{x} \vert \mathcal{C}_j) \cdot p(\mathcal{C}_j)
				}
		\end{equation}
	\end{boxBlue}
\end{frame}


% Section: Na\"{i}ve Bayes Classifier
%______________________________________________________________________
\section{Na\"{i}ve Bayes Classifier}
\makedivider{Na\"{i}ve Bayes Classifier}


%
\begin{frame}{}{}

\end{frame}


% Section: Wrap-Up
%______________________________________________________________________
\section{Wrap-Up}
\makedivider{Wrap-Up}

% Subsection: Summary
% --------------------------------------------------------------------------------------------------------
\subsection{Summary}

% Summary
\begin{frame}{Summary}{}

\end{frame}


% Subsection: Lecture Overview
% --------------------------------------------------------------------------------------------------------
\subsection{Lecture Overview}

\makeoverview{3}


% Subsection: Self-Test Questions
% --------------------------------------------------------------------------------------------------------
\subsection{Self-Test Questions}

% Self-Test Questions
\begin{frame}{Self-Test Questions}{}

\end{frame}


% Subsection: Recommended Literature and further Reading
% --------------------------------------------------------------------------------------------------------
\subsection{Recommended Literature and further Reading}

% Literature
%______________________________________________________________________
\begin{frame}{Recommended Literature and further Reading}{}
	\footnotesize
	\begin{thebibliography}{2}

	\end{thebibliography}
\end{frame}


% Thank you
%______________________________________________________________________
\makethanks

\end{document}